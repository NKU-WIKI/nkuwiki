#!/usr/bin/env python3
"""
ç»Ÿä¸€ç´¢å¼•æ„å»ºè„šæœ¬

æ­¤è„šæœ¬ä½¿ç”¨ETLæ¨¡å—çš„ç»Ÿä¸€è·¯å¾„é…ç½®æ¥æ„å»ºæ‰€æœ‰ç±»å‹çš„ç´¢å¼•ï¼š
- MySQLç´¢å¼•ï¼ˆæ•°æ®å¯¼å…¥å’Œè¡¨ä¼˜åŒ–ï¼‰
- BM25æ–‡æœ¬ç´¢å¼•ï¼ˆæ”¯æŒä¸­æ–‡åˆ†è¯å’Œåœç”¨è¯ï¼‰
- Qdrantå‘é‡ç´¢å¼•ï¼ˆè¯­ä¹‰åµŒå…¥å’Œæ–‡æœ¬åˆ†å—ï¼‰
- Elasticsearchå…¨æ–‡ç´¢å¼•ï¼ˆé€šé…ç¬¦æŸ¥è¯¢å’Œå¤æ‚æ–‡æœ¬åŒ¹é…ï¼‰

æ‰€æœ‰ç´¢å¼•å™¨ç°åœ¨æ”¯æŒå¤šç§æ•°æ®æºï¼š
- raw_files: æ··åˆæ¨¡å¼ï¼Œä»åŸå§‹æ–‡ä»¶åŠ è½½æ•°æ®å¹¶è¡¥å……PageRankåˆ†æ•°ï¼ˆæ¨èï¼‰
- mysql: ä»…ä»MySQLæ•°æ®åº“åŠ è½½æ•°æ®
- raw_only: ä»…ä»åŸå§‹JSONæ–‡ä»¶åŠ è½½æ•°æ®

ä½¿ç”¨æ–¹æ³•:
    python etl/build_all_indexes.py [--limit 1000] [--test] [--data-source raw_files]
    python etl/build_all_indexes.py --only bm25 --data-source mysql --limit 100
    python etl/build_all_indexes.py --validate
    python etl/build_all_indexes.py --batch-size 5000 --start-batch 0 --max-batches 10
    python etl/build_all_indexes.py --batch-size -1  # ä¸åˆ†æ‰¹ï¼Œä¸€æ¬¡æ€§å¤„ç†
    
åˆ†æ‰¹æ„å»ºç¤ºä¾‹:
    # æ¯æ‰¹5000æ¡ï¼Œå¤„ç†æ‰€æœ‰æ•°æ®
    python etl/build_all_indexes.py --batch-size 5000
    
    # ä»ç¬¬3æ‰¹å¼€å§‹ï¼Œåªå¤„ç†5ä¸ªæ‰¹æ¬¡ï¼ˆæ–­ç‚¹ç»­å»ºï¼‰
    python etl/build_all_indexes.py --start-batch 3 --max-batches 5
    
    # åªæ„å»ºBM25ç´¢å¼•ï¼Œæ¯æ‰¹1000æ¡
    python etl/build_all_indexes.py --only bm25 --batch-size 1000
"""

import sys
import argparse
import asyncio
from pathlib import Path
from typing import Dict, Any
import logging

# å…³é”®ä¿®æ”¹ï¼šé…ç½®Pythonå†…ç½®çš„loggingæ¨¡å—ä»¥æ•è·ç¬¬ä¸‰æ–¹åº“çš„DEBUGæ—¥å¿—
# è¿™å°†ä½¿æˆ‘ä»¬èƒ½å¤Ÿçœ‹åˆ° elasticsearch-py åº“è¯¦ç»†çš„ç½‘ç»œæ´»åŠ¨
logging.basicConfig(stream=sys.stderr, level=logging.DEBUG)

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).resolve().parent.parent))

from core.utils.logger import register_logger
logger = register_logger("etl.build_indexes")

# å¯¼å…¥é…ç½®å’ŒETLè·¯å¾„
from etl import BASE_PATH, INDEX_PATH, QDRANT_PATH, MYSQL_PATH, NLTK_PATH, MODELS_PATH
from etl.indexing.bm25_indexer import BM25Indexer
from etl.indexing.qdrant_indexer import QdrantIndexer
from etl.indexing.elasticsearch_indexer import ElasticsearchIndexer
from etl.indexing.mysql_indexer import MySQLIndexer

async def build_all_indexes(limit: int = None, test_mode: bool = False, only: str = None, data_source: str = "raw_files", 
                          batch_size: int = -1, start_batch: int = 0, max_batches: int = None, incremental: bool = False) -> Dict[str, Any]:
    """
    æ„å»ºæ‰€æœ‰ç´¢å¼•
    
    Args:
        limit: é™åˆ¶å¤„ç†çš„è®°å½•æ•°é‡
        test_mode: æµ‹è¯•æ¨¡å¼ï¼Œä¸å®é™…åˆ›å»ºç´¢å¼•æ–‡ä»¶
        only: åªæ„å»ºæŒ‡å®šç±»å‹çš„ç´¢å¼• ('mysql', 'bm25', 'qdrant', 'elasticsearch')
        data_source: æ•°æ®æºç±»å‹ ("raw_files"=æ··åˆæ¨¡å¼, "mysql"=ä»…MySQL, "raw_only"=ä»…åŸå§‹æ–‡ä»¶)
        batch_size: æ¯æ‰¹å¤„ç†çš„è®°å½•æ•°é‡ï¼Œ-1è¡¨ç¤ºä¸åˆ†æ‰¹ï¼ˆé»˜è®¤-1ï¼‰
        start_batch: ä»ç¬¬å‡ æ‰¹å¼€å§‹å¤„ç†ï¼ˆæ–­ç‚¹ç»­å»ºï¼‰
        max_batches: æœ€å¤§æ‰¹æ¬¡æ•°ï¼ŒNoneè¡¨ç¤ºå¤„ç†æ‰€æœ‰æ‰¹æ¬¡
        
    Returns:
        æ‰€æœ‰ç´¢å¼•çš„æ„å»ºç»“æœ
    """
    results = {}
    
    logger.info("=" * 60)
    logger.info("å¼€å§‹æ„å»ºæ‰€æœ‰ç´¢å¼•")
    logger.info(f"æ•°æ®åŸºç¡€è·¯å¾„: {BASE_PATH}")
    logger.info(f"ç´¢å¼•å­˜å‚¨è·¯å¾„: {INDEX_PATH}")
    logger.info(f"å‘é‡å­˜å‚¨è·¯å¾„: {QDRANT_PATH}")
    logger.info(f"MySQLæ•°æ®è·¯å¾„: {MYSQL_PATH}")
    logger.info(f"NLTKæ•°æ®è·¯å¾„: {NLTK_PATH}")
    logger.info(f"æ¨¡å‹ç¼“å­˜è·¯å¾„: {MODELS_PATH}")
    if limit:
        logger.info(f"å¤„ç†è®°å½•é™åˆ¶: {limit}")
    logger.info(f"æ•°æ®æºç±»å‹: {data_source}")
    if batch_size == -1:
        logger.info("åˆ†æ‰¹é…ç½®: ä¸åˆ†æ‰¹å¤„ç†ï¼ˆä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰æ•°æ®ï¼‰")
    else:
        logger.info(f"åˆ†æ‰¹é…ç½®: batch_size={batch_size}, start_batch={start_batch}, max_batches={max_batches}")
    if test_mode:
        logger.info("è¿è¡Œæ¨¡å¼: æµ‹è¯•æ¨¡å¼")
    logger.info("=" * 60)
    
    # 0. æ„å»ºMySQLç´¢å¼•ï¼ˆæ•°æ®å¯¼å…¥ï¼‰
    if only is None or only == 'mysql':
        logger.info("ğŸ—„ï¸ [0/4] å¼€å§‹æ„å»ºMySQLç´¢å¼•...")
        logger.info(f"   - æµ‹è¯•æ¨¡å¼: {'æ˜¯' if test_mode else 'å¦'}")
        
        try:
            mysql_indexer = MySQLIndexer(logger)
            logger.info("   - åˆå§‹åŒ–MySQLç´¢å¼•å™¨å®Œæˆ")
            
            # MySQLç´¢å¼•æ„å»ºï¼ˆæ•°æ®å¯¼å…¥å’Œè¡¨ä¼˜åŒ–ï¼‰
            mysql_result = await mysql_indexer.build_indexes(dry_run=test_mode)
            results['mysql'] = {
                "success": mysql_result,
                "message": "MySQLç´¢å¼•æ„å»ºæˆåŠŸ" if mysql_result else "MySQLç´¢å¼•æ„å»ºå¤±è´¥"
            }
            
            if mysql_result:
                # è·å–ç»Ÿè®¡ä¿¡æ¯
                stats = await mysql_indexer.get_statistics()
                total_records = stats.get('total_records', 0)
                logger.info(f"âœ… MySQLç´¢å¼•æ„å»ºæˆåŠŸï¼æ•°æ®åº“åŒ…å« {total_records} æ¡è®°å½•")
                logger.info(f"   - è¯¦æƒ…: {stats}")
            else:
                logger.error("âŒ MySQLç´¢å¼•æ„å»ºå¤±è´¥")
                
        except Exception as e:
            logger.error(f"âŒ MySQLç´¢å¼•æ„å»ºå¼‚å¸¸: {e}")
            results['mysql'] = {"success": False, "error": str(e)}
            
        logger.info("ğŸ—„ï¸ MySQLç´¢å¼•æ„å»ºé˜¶æ®µç»“æŸ\n")
    else:
        logger.info("â­ï¸ è·³è¿‡MySQLç´¢å¼•æ„å»ºï¼ˆæ ¹æ® --only å‚æ•°ï¼‰")
    
    # 1. æ„å»ºBM25ç´¢å¼•
    if only is None or only == 'bm25':
        logger.info("ğŸ” [1/4] å¼€å§‹æ„å»ºBM25ç´¢å¼•...")
        logger.info(f"   - è®°å½•é™åˆ¶: {limit if limit else 'æ— é™åˆ¶'}")
        logger.info(f"   - æ•°æ®æº: {data_source}")
        logger.info(f"   - æµ‹è¯•æ¨¡å¼: {'æ˜¯' if test_mode else 'å¦'}")
        
        try:
            bm25_indexer = BM25Indexer(logger)
            logger.info("   - åˆå§‹åŒ–BM25ç´¢å¼•å™¨å®Œæˆ")
            
            bm25_result = await bm25_indexer.build_indexes(
                limit=limit,
                test_mode=test_mode,
                data_source=data_source,
                batch_size=batch_size,
                start_batch=start_batch,
                max_batches=max_batches
            )
            results['bm25'] = bm25_result
            
            if bm25_result['success']:
                node_count = bm25_result.get('total_nodes', 0)
                logger.info(f"âœ… BM25ç´¢å¼•æ„å»ºæˆåŠŸï¼å¤„ç†äº† {node_count} ä¸ªèŠ‚ç‚¹")
                logger.info(f"   - è¯¦æƒ…: {bm25_result['message']}")
            else:
                logger.error(f"âŒ BM25ç´¢å¼•æ„å»ºå¤±è´¥: {bm25_result['message']}")
                if 'error' in bm25_result:
                    logger.error(f"   - é”™è¯¯è¯¦æƒ…: {bm25_result['error']}")
                
        except Exception as e:
            logger.error(f"âŒ BM25ç´¢å¼•æ„å»ºå¼‚å¸¸: {e}")
            results['bm25'] = {"success": False, "error": str(e)}
            
        logger.info("ğŸ” BM25ç´¢å¼•æ„å»ºé˜¶æ®µç»“æŸ\n")
    else:
        logger.info("â­ï¸ è·³è¿‡BM25ç´¢å¼•æ„å»ºï¼ˆæ ¹æ® --only å‚æ•°ï¼‰")
    
    # 2. æ„å»ºQdrantå‘é‡ç´¢å¼•  
    if only is None or only == 'qdrant':
        # QdrantIndexerçš„build_indexesä¹Ÿéœ€è¦æ”¹é€ 
        logger.warning("QdrantIndexerçš„æ”¹é€ å°šæœªå®Œæˆï¼Œæš‚æ—¶è·³è¿‡ã€‚")
        # try:
        #     qdrant_indexer = QdrantIndexer(logger)
        # ...
    
    # 3. æ„å»ºElasticsearchç´¢å¼•
    if only is None or only == 'elasticsearch':
        logger.info("ğŸ” [3/4] å¼€å§‹æ„å»ºElasticsearchç´¢å¼•...")
        logger.info(f"   - è®°å½•é™åˆ¶: {limit if limit else 'æ— é™åˆ¶'}")
        logger.info(f"   - æ•°æ®æº: {data_source}")
        logger.info(f"   - æµ‹è¯•æ¨¡å¼: {'æ˜¯' if test_mode else 'å¦'}")
        
        try:
            es_indexer = ElasticsearchIndexer(logger)
            logger.info("   - åˆå§‹åŒ–Elasticsearchç´¢å¼•å™¨å®Œæˆ")
            
            es_result = await es_indexer.build_indexes(
                limit=limit,
                test_mode=test_mode,
                data_source=data_source,
                batch_size=batch_size,
                start_batch=start_batch,
                max_batches=max_batches
            )
            results['elasticsearch'] = es_result
            
            if es_result['success']:
                record_count = es_result.get('total_records', 0)
                indexed_count = es_result.get('indexed', 0)
                logger.info(f"âœ… Elasticsearchç´¢å¼•æ„å»ºæˆåŠŸï¼ç´¢å¼•äº† {indexed_count}/{record_count} æ¡è®°å½•")
                logger.info(f"   - è¯¦æƒ…: {es_result['message']}")
            else:
                logger.error(f"âŒ Elasticsearchç´¢å¼•æ„å»ºå¤±è´¥: {es_result['message']}")
                if 'error' in es_result:
                    logger.error(f"   - é”™è¯¯è¯¦æƒ…: {es_result['error']}")
                
        except Exception as e:
            logger.error(f"âŒ Elasticsearchç´¢å¼•æ„å»ºå¼‚å¸¸: {e}")
            results['elasticsearch'] = {"success": False, "error": str(e)}
            
        logger.info("ğŸ” Elasticsearchç´¢å¼•æ„å»ºé˜¶æ®µç»“æŸ\n")
    else:
        logger.info("â­ï¸ è·³è¿‡Elasticsearchç´¢å¼•æ„å»ºï¼ˆæ ¹æ® --only å‚æ•°ï¼‰")
    
    return results


async def validate_all_indexes() -> Dict[str, Any]:
    """
    éªŒè¯æ‰€æœ‰ç´¢å¼•çš„å¥åº·çŠ¶æ€
    """
    results = {}
    config = Config()
    
    logger.info("=" * 60)
    logger.info("å¼€å§‹éªŒè¯æ‰€æœ‰ç´¢å¼•")
    logger.info("=" * 60)

    # éªŒè¯MySQLç´¢å¼•
    try:
        mysql_indexer = MySQLIndexer(logger)
        stats = await mysql_indexer.get_statistics()
        total_records = stats.get('total_records', 0)
        results['mysql'] = {"success": total_records > 0, "total_records": total_records}
    except Exception as e:
        results['mysql'] = {"success": False, "error": str(e)}

    # éªŒè¯BM25ç´¢å¼•
    try:
        bm25_indexer = BM25Indexer(logger)
        results['bm25'] = await bm25_indexer.validate_index()
    except Exception as e:
        results['bm25'] = {"success": False, "error": str(e)}

    # éªŒè¯Qdrantç´¢å¼•
    try:
        qdrant_indexer = QdrantIndexer(logger)
        results['qdrant'] = await qdrant_indexer.validate_index()
    except Exception as e:
        results['qdrant'] = {"success": False, "error": str(e)}

    # éªŒè¯Elasticsearchç´¢å¼•
    try:
        es_indexer = ElasticsearchIndexer(logger)
        results['elasticsearch'] = await es_indexer.validate_index()
    except Exception as e:
        results['elasticsearch'] = {"success": False, "error": str(e)}
        
    # ç»Ÿè®¡ç»“æœ
    logger.info("\n" + "=" * 60)
    logger.info("ç´¢å¼•éªŒè¯å®Œæˆï¼")
    
    success_count = sum(1 for r in results.values() if r.get('success', False))
    total_count = len(results)
    
    logger.info(f"å¥åº·ç´¢å¼•: {success_count}/{total_count}")
    
    for index_type, result in results.items():
        status = "âœ…" if result.get('success', False) else "âŒ"
        message = result.get('message', 'N/A')
        if not result.get('success', False):
            message = result.get('error', 'æœªçŸ¥é”™è¯¯')
            
        logger.info(f"  {status} {index_type.upper()}: {message}")
    
    logger.info("=" * 60)
    
    return {
        'success': success_count == total_count,
        'results': results,
        'summary': {
            'total': total_count,
            'success': success_count,
            'failed': total_count - success_count
        }
    }


def main():
    """
    ä¸»å‡½æ•°
    """
    parser = argparse.ArgumentParser(description='æ„å»ºæ‰€æœ‰ç±»å‹çš„ç´¢å¼•')
    parser.add_argument('--limit', type=int, default=None, help='é™åˆ¶å¤„ç†çš„è®°å½•æ•°é‡')
    parser.add_argument('--test', action='store_true', help='æµ‹è¯•æ¨¡å¼ï¼Œä¸å®é™…åˆ›å»ºç´¢å¼•')
    parser.add_argument('--only', type=str, default=None, help='åªæ„å»ºæŒ‡å®šç´¢å¼• (mysql, bm25, qdrant, elasticsearch)')
    parser.add_argument('--data-source', type=str, default='raw_files', help='æ•°æ®æº (raw_files, mysql, raw_only)')
    parser.add_argument('--validate', action='store_true', help='éªŒè¯æ‰€æœ‰ç´¢å¼•çš„å¥åº·çŠ¶æ€')
    parser.add_argument('--batch-size', type=int, default=-1, help='æ¯æ‰¹å¤„ç†çš„è®°å½•æ•°é‡ï¼ˆ-1è¡¨ç¤ºä¸åˆ†æ‰¹ï¼‰')
    parser.add_argument('--start-batch', type=int, default=0, help='ä»ç¬¬å‡ æ‰¹å¼€å§‹å¤„ç†')
    parser.add_argument('--max-batches', type=int, default=None, help='æœ€å¤§æ‰¹æ¬¡æ•°')
    parser.add_argument('--incremental', action='store_true', help='æ˜¯å¦ä¸ºå¢é‡æ„å»ºï¼ˆä»…Qdrantï¼‰')
    
    args = parser.parse_args()

    if args.validate:
        asyncio.run(validate_all_indexes())
    else:
        results = asyncio.run(build_all_indexes(
            limit=args.limit, 
            test_mode=args.test, 
            only=args.only,
            data_source=args.data_source,
            batch_size=args.batch_size,
            start_batch=args.start_batch,
            max_batches=args.max_batches,
            incremental=args.incremental
        ))
        
        # æ‰“å°æœ€ç»ˆç»Ÿè®¡ç»“æœ
        success_count = sum(1 for r in results.values() if r.get('success', False))
        total_count = len(results)
        
        logger.info("\n" + "=" * 60)
        logger.info("æ‰€æœ‰ç´¢å¼•æ„å»ºä»»åŠ¡å·²å®Œæˆï¼")
        logger.info(f"æˆåŠŸæ„å»º: {success_count}/{total_count} ä¸ªç´¢å¼•ç±»å‹")
        
        for index_type, result in results.items():
            status = "âœ…" if result.get('success', False) else "âŒ"
            message = result.get('message', result.get('error', 'æœªçŸ¥çŠ¶æ€'))
            logger.info(f"  {status} {index_type.upper()}: {message}")
        logger.info("=" * 60)

if __name__ == '__main__':
    main() 