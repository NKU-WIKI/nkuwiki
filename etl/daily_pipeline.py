#!/usr/bin/env python3
"""
ETLå¢é‡å¤„ç†ç®¡é“

è¯¥è„šæœ¬è´Ÿè´£æ‰§è¡Œå¢é‡ETLæµç¨‹çš„ç¬¬äºŒå’Œç¬¬ä¸‰é˜¶æ®µï¼š
1.  **æ‰«æä¸èŠ‚ç‚¹åŒ– (Scan & Nodify)**: æ‰«æ `/data/raw` ç›®å½•ï¼Œæ‰¾å‡ºæŒ‡å®šæ—¶é—´çª—å£å†…çš„æ–°å¢/ä¿®æ”¹æ–‡ä»¶ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºLlamaIndexçš„`TextNode`å¯¹è±¡ã€‚
2.  **å»ºç«‹ç´¢å¼• (Indexing)**: å°†æ–°ç”Ÿæˆçš„`TextNode`å¯¹è±¡é€å…¥Qdrantå»ºç«‹å‘é‡ç´¢å¼•ã€‚
3.  **ç”Ÿæˆæ´å¯Ÿ (Insight Generation)**: (å¯é€‰) åŸºäºæ–°å¢èŠ‚ç‚¹ï¼ŒæŒ‰æ¥æºåˆ†ç±»ï¼ˆå®˜æ–¹ã€ç¤¾åŒºã€é›†å¸‚ï¼‰åï¼Œåˆ†åˆ«è°ƒç”¨`text_generator`ç”Ÿæˆåˆ†ææ´å¯Ÿå¹¶å­˜å…¥æ•°æ®åº“ã€‚
"""
import sys
from pathlib import Path

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).resolve().parent.parent))

import argparse
import asyncio
import json
import re
from datetime import datetime, timedelta, timezone
from typing import Dict, List, Optional, Tuple, Literal, Any
from collections import defaultdict

import aiofiles
from llama_index.core.schema import Document, TextNode
from tqdm.asyncio import tqdm as aio_tqdm

from config import Config
from core.agent.text_generator import generate_structured_json
from core.utils.logger import register_logger
from etl.indexing.qdrant_indexer import QdrantIndexer
from etl.load import db_core
from etl.processors.chunk_cache import ChunkCacheManager
from etl.utils.const import (
    university_official_accounts,
    school_official_accounts,
    club_official_accounts,
    unofficial_accounts,
)

logger = register_logger("etl.daily_pipeline")

# å®šä¹‰æ´å¯Ÿåˆ†ç±»
InsightCategory = Literal["å®˜æ–¹", "ç¤¾åŒº", "é›†å¸‚"]

# å®šä¹‰å®˜æ–¹å’Œç¤¾åŒºæ¥æº
OFFICIAL_WECHAT_SOURCES = set(university_official_accounts + school_official_accounts)
COMMUNITY_WECHAT_SOURCES = set(club_official_accounts + unofficial_accounts)


def parse_datetime_utc(time_str: str) -> Optional[datetime]:
    """å°†å­—ç¬¦ä¸²ç¨³å¥åœ°è§£æä¸ºå¸¦UTCæ—¶åŒºçš„æ—¶é—´å¯¹è±¡"""
    if not time_str or not isinstance(time_str, str):
        return None
    try:
        # å°è¯•ISO 8601æ ¼å¼ï¼ˆå¸¦æˆ–ä¸å¸¦'Z'ï¼‰
        if time_str.endswith("Z"):
            time_str = time_str[:-1] + "+00:00"
        dt = datetime.fromisoformat(time_str)
        return dt.astimezone(timezone.utc)
    except (ValueError, TypeError):
        pass

    # å°è¯•å…¶ä»–å¸¸è§æ ¼å¼
    formats_to_try = ["%Y-%m-%d %H:%M:%S", "%Y-%m-%d"]
    for fmt in formats_to_try:
        try:
            dt = datetime.strptime(time_str, fmt)
            # å‡å®šä¸ºæœ¬åœ°æ—¶åŒºå¹¶è½¬æ¢ä¸ºUTC
            return dt.astimezone().astimezone(timezone.utc)
        except ValueError:
            continue
    logger.debug(f"æ— æ³•è§£æçš„æ—¶é—´æ ¼å¼: '{time_str}'")
    return None


async def find_new_files_in_timespan(
    data_dir: Path, start_time: datetime, end_time: datetime, platform_filter: Optional[str] = None
) -> List[Path]:
    """åœ¨/data/rawç›®å½•ä¸­é«˜æ•ˆæŸ¥æ‰¾æŒ‡å®šæ—¶é—´çª—å£å†…å‘å¸ƒçš„JSONæ–‡ä»¶"""
    logger.info(f"å¼€å§‹æ‰«æç›®å½• '{data_dir}'ï¼Œæ—¶é—´èŒƒå›´: {start_time.isoformat()} to {end_time.isoformat()}")
    if platform_filter:
        logger.info(f"ä»…æ‰«æå¹³å°: '{platform_filter}'")
    
    # 1. æ ¹æ®æ—¶é—´èŒƒå›´ï¼Œç¡®å®šéœ€è¦æ‰«æçš„å¹´æœˆç›®å½•
    target_months = set()
    current_date = start_time.date()
    while current_date <= end_time.date():
        target_months.add(current_date.strftime("%Y%m"))
        # ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªæœˆ
        # (è¿™ç§æ–¹å¼å¯ä»¥ç¨³å¥åœ°å¤„ç†æœˆä»½å¤©æ•°ä¸åŒçš„æƒ…å†µ)
        next_month_start = (current_date.replace(day=28) + timedelta(days=4)).replace(day=1)
        current_date = next_month_start

    logger.info(f"å°†ç›®æ ‡æ‰«æèŒƒå›´é™å®šäºä»¥ä¸‹å¹´æœˆç›®å½•: {sorted(list(target_months))}")

    # 2. éå†æ‰€æœ‰ platform/tag ç»„åˆï¼ŒæŸ¥æ‰¾åŒ¹é…çš„å¹´æœˆç›®å½•ï¼Œæ”¶é›†æ–‡ä»¶
    files_to_check = []
    if not data_dir.is_dir():
        logger.warning(f"æ•°æ®æºç›®å½• {data_dir} ä¸å­˜åœ¨ã€‚")
        return []

    for platform_dir in data_dir.iterdir():
        if not platform_dir.is_dir():
            continue
        if platform_filter and platform_dir.name != platform_filter:
            continue
        for tag_dir in platform_dir.iterdir():
            if not tag_dir.is_dir():
                continue
            for month_str in target_months:
                month_dir = tag_dir / month_str
                if month_dir.is_dir():
                    files_to_check.extend(month_dir.rglob("*.json"))
    
    logger.info(f"åœ¨ç›®æ ‡å¹´æœˆç›®å½•ä¸­å…±æ‰¾åˆ° {len(files_to_check)} ä¸ª .json æ–‡ä»¶å¾…ç²¾ç¡®æ£€æŸ¥ã€‚")

    # 3. å¯¹ç­›é€‰åçš„æ–‡ä»¶è¿›è¡Œç²¾ç¡®æ—¶é—´æ£€æŸ¥
    tasks = [is_file_in_timespan(file_path, start_time, end_time) for file_path in files_to_check]
    
    results = await aio_tqdm.gather(
        *tasks, desc="ç²¾ç¡®æ‰«ææ–‡ä»¶", unit="ä¸ª"
    )

    new_files = [path for path in results if path is not None]
    logger.info(f"ç²¾ç¡®æ‰«æå®Œæˆï¼Œåœ¨æ—¶é—´èŒƒå›´å†…æ‰¾åˆ° {len(new_files)} ä¸ªæ–°æ–‡ä»¶ã€‚")
    return new_files


async def is_file_in_timespan(
    file_path: Path, start_time: datetime, end_time: datetime
) -> Optional[Path]:
    """æ£€æŸ¥å•ä¸ªæ–‡ä»¶çš„å‘å¸ƒæ—¶é—´æ˜¯å¦åœ¨æŒ‡å®šèŒƒå›´å†…"""
    try:
        async with aiofiles.open(file_path, "r", encoding="utf-8") as f:
            content = await f.read()
            data = json.loads(content)
        
        publish_time_str = data.get("publish_time")
        publish_time = parse_datetime_utc(publish_time_str)
        
        if publish_time and start_time <= publish_time <= end_time:
            return file_path
    except Exception:
        # å¿½ç•¥JSONè§£æå¤±è´¥æˆ–ç¼ºå°‘æ—¶é—´å­—æ®µçš„æ–‡ä»¶
        return None
    return None


async def process_files_to_nodes(file_paths: List[Path]) -> List[TextNode]:
    """è¯»å–æ–‡ä»¶ï¼Œå¤„ç†å¹¶è½¬æ¢ä¸ºTextNodeåˆ—è¡¨"""
    if not file_paths:
        return []
    
    chunk_manager = ChunkCacheManager()
    all_nodes = []

    async def process_single_file(path: Path) -> List[TextNode]:
        try:
            async with aiofiles.open(path, "r", encoding="utf-8") as f:
                content = await f.read()
                data = json.loads(content)
            
            # ä½¿ç”¨Documentå¯¹è±¡æ¥æ‰¿è½½å†…å®¹å’Œå…ƒæ•°æ®
            doc = Document(
                text=data.get("content", ""),
                metadata={
                    "doc_id": data.get("id"),
                    "title": data.get("title"),
                    "url": data.get("url"),
                    "platform": data.get("platform"),
                    "tag": data.get("tag"),  # ç¡®ä¿tagå­—æ®µè¢«æå–
                    "publish_time": data.get("publish_time"),
                    "file_path": str(path),
                },
            )
            return await chunk_manager.chunk_documents_with_cache([doc], show_progress=False)
        except Exception as e:
            logger.warning(f"å¤„ç†æ–‡ä»¶ {path} å¤±è´¥: {e}")
            return []

    tasks = [process_single_file(path) for path in file_paths]
    results = await aio_tqdm.gather(*tasks, desc="å¤„ç†å¹¶è½¬æ¢èŠ‚ç‚¹", unit="ä¸ª")
    
    for node_list in results:
        all_nodes.extend(node_list)
        
    logger.info(f"æˆåŠŸå°† {len(file_paths)} ä¸ªæ–‡ä»¶è½¬æ¢ä¸º {len(all_nodes)} ä¸ªTextNodeã€‚")
    return all_nodes


async def build_qdrant_indexes(nodes: List[TextNode], config: Config):
    """ä¸ºæ–°èŠ‚ç‚¹å»ºç«‹Qdrantç´¢å¼•"""
    if not nodes:
        logger.warning("æ²¡æœ‰èŠ‚ç‚¹éœ€è¦ç´¢å¼•ï¼Œè·³è¿‡æ­¤æ­¥éª¤ã€‚")
        return
        
    collection_name = config.get("qdrant.collection_name")
    qdrant_indexer = QdrantIndexer(collection_name)
    logger.info(f"å¼€å§‹å‘Qdranté›†åˆ '{collection_name}' ä¸­æ·»åŠ  {len(nodes)} ä¸ªèŠ‚ç‚¹...")
    await qdrant_indexer.build_from_nodes(nodes)
    logger.info("Qdrantç´¢å¼•å»ºç«‹å®Œæˆã€‚")


async def read_raw_documents(file_paths: List[Path]) -> List[Dict[str, Any]]:
    """
    å¼‚æ­¥è¯»å–ä¸€ç»„JSONæ–‡ä»¶è·¯å¾„ï¼Œå¹¶è¿”å›å…¶å†…å®¹å­—å…¸çš„åˆ—è¡¨ã€‚
    æ¯ä¸ªè¿”å›çš„å­—å…¸ä¸­ä¼šé¢å¤–æ·»åŠ ä¸€ä¸ª `_file_path` é”®ï¼Œç”¨äºè¿½è¸ªæºæ–‡ä»¶ã€‚
    """
    async def _read_file(path: Path) -> Optional[Dict[str, Any]]:
        try:
            async with aiofiles.open(path, 'r', encoding='utf-8') as f:
                content = await f.read()
                data = json.loads(content)
                data["_file_path"] = str(path)  # æ³¨å…¥æ–‡ä»¶è·¯å¾„
                return data
        except Exception as e:
            logger.error(f"è¯»å–æˆ–è§£ææ–‡ä»¶å¤±è´¥: {path}, é”™è¯¯: {e}")
            return None

    tasks = [_read_file(path) for path in file_paths]
    results = await aio_tqdm.gather(*tasks, desc="è¯»å–åŸå§‹æ–‡æ¡£")
    return [doc for doc in results if doc]  # è¿‡æ»¤æ‰è¯»å–å¤±è´¥çš„None


def build_insight_prompt(docs: List[Dict[str, Any]], category: InsightCategory, char_limit: Optional[int] = None) -> str:
    """
    æ ¹æ®åŸå§‹æ–‡æ¡£å­—å…¸åˆ—è¡¨å’Œåˆ†ç±»æ„å»ºç”¨äºç”Ÿæˆæ´å¯Ÿçš„Promptã€‚
    ä¼šå¯¹è¾“å…¥å†…å®¹è¿›è¡Œæˆªæ–­ï¼Œä»¥ç¡®ä¿æœ€ç»ˆçš„Promptä¸è¶…è¿‡æŒ‡å®šçš„å­—ç¬¦é™åˆ¶ã€‚
    """
    # 1. å®šä¹‰æç¤ºè¯çš„å¤´éƒ¨å’Œå°¾éƒ¨
    prompt_header = f"""
ä½œä¸ºä¸€åæ ¡å›­ä¿¡æ¯åˆ†æä¸“å®¶ï¼Œè¯·å¯¹ä»¥ä¸‹æ–°å‘å¸ƒçš„ **{category}** ç›¸å…³æ ¡å›­åŠ¨æ€è¿›è¡Œåˆ†æã€æ€»ç»“å’Œå½’ç±»ï¼Œå¹¶æå–æ ¸å¿ƒæ´å¯Ÿã€‚

**åˆ†æä»»åŠ¡:**
1.  **æ€»ç»“è¦ç‚¹**: å¯¹æ‰€æœ‰åŠ¨æ€è¿›è¡Œæ¦‚æ‹¬ï¼Œå½¢æˆä¸€æ®µ100å­—ä»¥å†…çš„æ€»ä½“æ‘˜è¦ã€‚
2.  **æå–æ´å¯Ÿ**: è¯†åˆ«å‡º3-5ä¸ªæœ€é‡è¦æˆ–æœ€æœ‰è¶£çš„ä¸»é¢˜/è¶‹åŠ¿ã€‚å¯¹äºæ¯ä¸ªä¸»é¢˜ï¼Œä½ å¿…é¡»æä¾›ï¼š
    - **ä¸€ä¸ªç®€çŸ­çš„æ ‡é¢˜**
    - **ä¸€æ®µè¯¦å°½çš„åˆ†ææè¿°ï¼Œè¦æ±‚å†…å®¹å……å®ï¼Œå­—æ•°ä¸å¾—å°‘äº1000å­—**ã€‚ç®€çŸ­çš„æè¿°æ˜¯ä¸å¯æ¥å—çš„ã€‚

**è¿‘æœŸåŠ¨æ€å…¨æ–‡åˆ—è¡¨:**
"""

    prompt_footer = """

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ç»“æœï¼Œä¸è¦æ·»åŠ ä»»ä½•é¢å¤–çš„è§£é‡Šæˆ–æ³¨é‡Šï¼š
```json
{{
    "summary": "æ€»ä½“æ‘˜è¦å†…å®¹ï¼ˆ100å­—ä»¥å†…ï¼‰...",
    "insights": [
        {{
            "title": "æ´å¯Ÿä¸»é¢˜1",
            "content": "å…³äºä¸»é¢˜1çš„è¯¦å°½åˆ†æï¼ˆä¸å°‘äº1000å­—ï¼‰..."
        }},
        {{
            "title": "æ´å¯Ÿä¸»é¢˜2",
            "content": "å…³äºä¸»é¢˜2çš„è¯¦å°½åˆ†æï¼ˆä¸å°‘äº1000å­—ï¼‰..."
        }}
    ]
}}
```
"""

    # 2. è®¡ç®—æ–‡æ¡£éƒ¨åˆ†å¯ç”¨çš„å­—ç¬¦é™åˆ¶
    docs_char_limit = None
    if char_limit:
        template_overhead = len(prompt_header) + len(prompt_footer)
        docs_char_limit = char_limit - template_overhead

        # å¦‚æœé™åˆ¶å¤ªå°ï¼Œæ— æ³•å®¹çº³æ¨¡æ¿æœ¬èº«ï¼Œåˆ™è®°å½•é”™è¯¯å¹¶è¿”å›ç©º
        if docs_char_limit <= 0:
            logger.error(
                f"æ€»å­—ç¬¦é™åˆ¶({char_limit})å¤ªå°ï¼Œä¸è¶³ä»¥å®¹çº³Promptæ¨¡æ¿çš„å›ºå®šå†…å®¹({template_overhead})ã€‚"
            )
            return ""

    # 3. å¡«å……æ–‡æ¡£å†…å®¹ï¼Œç¡®ä¿ä¸è¶…è¿‡ docs_char_limit
    doc_details = []
    current_docs_len = 0
    separator = "\n\n---\n\n"

    for doc in docs:
        title = doc.get("title", "æ— æ ‡é¢˜")
        publish_time = doc.get("publish_time", "æœªçŸ¥æ—¶é—´")
        tag = doc.get("tag", "æœªçŸ¥æ¥æº")
        full_content = doc.get("content", "")
        
        detail_str = (
            f"æ¥æº: {tag}\n"
            f"æ ‡é¢˜: {title}\n"
            f"å‘å¸ƒæ—¶é—´: {publish_time}\n"
            f"å†…å®¹:\n{full_content}"
        )

        # è®¡ç®—æ–°æ–‡æ¡£åŠ å…¥åä¼šå¢åŠ çš„é•¿åº¦ï¼ˆåŒ…æ‹¬åˆ†éš”ç¬¦ï¼‰
        added_len = len(detail_str) + (len(separator) if doc_details else 0)

        if docs_char_limit and (current_docs_len + added_len > docs_char_limit):
            logger.warning(
                f"æ–‡æ¡£å†…å®¹éƒ¨åˆ†å·²è¾¾åˆ°é™åˆ¶({docs_char_limit}å­—ç¬¦)ï¼Œåœæ­¢æ·»åŠ æ›´å¤šæ–‡æ¡£ã€‚ "
                f"æ€»å­—ç¬¦é™åˆ¶: {char_limit}, æ¨¡æ¿å¼€é”€: {template_overhead}. "
                f"æ€»æ–‡æ¡£æ•°: {len(docs)}, å®é™…å¤„ç†: {len(doc_details)}."
            )
            break
        
        doc_details.append(detail_str)
        current_docs_len += added_len
    
    # 4. ç»„è£…æœ€ç»ˆçš„Prompt
    docs_str = separator.join(doc_details)
    prompt = f"{prompt_header}{docs_str}{prompt_footer}"

    logger.info(f"ä¸ºåˆ†ç±» '{category}' ç”Ÿæˆæ´å¯Ÿçš„Promptï¼Œè¾“å…¥æ€»å­—ç¬¦æ•°: {len(prompt)}")
    logger.debug(f"Prompt for '{category}': \n{prompt[:500]}...")
    return prompt


async def generate_and_save_insights(
    docs: List[Dict[str, Any]],
    config: Config,
    end_time: datetime,
    insight_char_limit: Optional[int] = None,
):
    """
    åŸºäºåŸå§‹æ–‡æ¡£åˆ—è¡¨ç”Ÿæˆæ´å¯Ÿï¼Œå¹¶å°†å…¶åˆ†ç±»å­˜å…¥æ•°æ®åº“ã€‚
    """
    if not docs:
        logger.info("æ²¡æœ‰æ–°çš„æ–‡æ¡£å¯ä¾›ç”Ÿæˆæ´å¯Ÿã€‚")
        return

    # 0. æŒ‰å‘å¸ƒæ—¶é—´å€’åºæ’åºæ–‡æ¡£ï¼Œä¼˜å…ˆå¤„ç†æœ€æ–°çš„å†…å®¹
    docs.sort(key=lambda d: parse_datetime_utc(d.get("publish_time")) or datetime.min.replace(tzinfo=timezone.utc), reverse=True)

    # 1. å®šä¹‰å®˜æ–¹æ¥æº
    official_wechat_sources = set(university_official_accounts + school_official_accounts)
    community_wechat_sources = set(club_official_accounts + unofficial_accounts)

    # 2. å°†æ–‡æ¡£åˆ†ç±»
    categorized_docs = defaultdict(list)
    for doc in docs:
        platform = doc.get("platform")
        author = doc.get("author") # ä½¿ç”¨ author å­—æ®µè¿›è¡Œåˆ†ç±»
        category = None

        if platform == "website" or (platform == "wechat" and author in official_wechat_sources):
            category = "å®˜æ–¹"
        elif platform == "wechat" and author in community_wechat_sources:
            category = "ç¤¾åŒº"
        elif platform == "market":
            category = "é›†å¸‚"

        if category:
            categorized_docs[category].append(doc)

    logger.info(
        f"æ–‡æ¡£åˆ†ç±»å®Œæˆï¼šå®˜æ–¹({len(categorized_docs['å®˜æ–¹'])}), "
        f"ç¤¾åŒº({len(categorized_docs['ç¤¾åŒº'])}), "
        f"é›†å¸‚({len(categorized_docs['é›†å¸‚'])})"
    )

    # 3. ä¸ºæ¯ä¸ªåˆ†ç±»ç”Ÿæˆå¹¶å­˜å‚¨æ´å¯Ÿ
    for category, doc_list in categorized_docs.items():
        if not doc_list:
            logger.info(f"åˆ†ç±» '{category}' ä¸­æ²¡æœ‰æ–°æ–‡æ¡£ï¼Œè·³è¿‡æ´å¯Ÿç”Ÿæˆã€‚")
            continue

        logger.info(f"å¼€å§‹ä¸ºåˆ†ç±» '{category}' ç”Ÿæˆæ´å¯Ÿ (åŸºäº {len(doc_list)} ä¸ªæ–‡æ¡£)...")
        try:
            prompt = build_insight_prompt(
                doc_list, category, char_limit=insight_char_limit
            )
            generated_data = await generate_structured_json(prompt)

            if not generated_data or "insights" not in generated_data:
                logger.warning(
                    f"ä»LLMè¿”å›çš„æ•°æ®æ ¼å¼ä¸æ­£ç¡®æˆ–ä¸ºç©ºï¼Œç¼ºå°‘'insights'ï¼Œè·³è¿‡åˆ†ç±» '{category}' çš„å­˜å‚¨ã€‚"
                )
                # è®°å½•å¯èƒ½å¯¼è‡´é—®é¢˜çš„æ–‡æ¡£è·¯å¾„
                problematic_files = [doc.get("_file_path", "æœªçŸ¥è·¯å¾„") for doc in doc_list]
                logger.warning(f"è§¦å‘é—®é¢˜çš„æ–‡æ¡£åˆ—è¡¨ (å…± {len(problematic_files)} ä¸ª): {problematic_files}")
                continue

            # å‡†å¤‡å­˜å…¥æ•°æ®åº“çš„æ•°æ®
            db_records = []
            for insight in generated_data.get("insights", []):
                db_records.append({
                    "title": insight.get("title"),
                    "content": insight.get("content"),
                    "category": category,
                    "insight_date": end_time.date(),
                })

            if db_records:
                inserted_count = await db_core.batch_insert("insights", db_records)
                logger.info(f"æˆåŠŸä¸ºåˆ†ç±» '{category}' æ’å…¥ {inserted_count} æ¡æ´å¯Ÿåˆ°æ•°æ®åº“ã€‚")

        except Exception as e:
            logger.error(f"ä¸ºåˆ†ç±» '{category}' ç”Ÿæˆæˆ–å­˜å‚¨æ´å¯Ÿå¤±è´¥: {e}", exc_info=True)


def get_time_window(args: argparse.Namespace) -> Tuple[datetime, datetime]:
    """
    æ ¹æ®å‘½ä»¤è¡Œå‚æ•°è®¡ç®—å¹¶è¿”å›UTCæ—¶é—´çª—å£ã€‚
    å¦‚æœåªæä¾›äº†æ—¥æœŸ (YYYY-MM-DD)ï¼Œåˆ™å°†å¼€å§‹æ—¶é—´è®¾ä¸ºå½“å¤©00:00:00ï¼Œç»“æŸæ—¶é—´è®¾ä¸ºå½“å¤©23:59:59ã€‚
    """
    # ç¡®å®šç»“æŸæ—¶é—´
    if args.end_time:
        end_time_utc = parse_datetime_utc(args.end_time)
        # å¦‚æœæ˜¯çº¯æ—¥æœŸæ ¼å¼ï¼Œåˆ™æ‰©å±•åˆ°å½“å¤©ç»“æŸ
        if end_time_utc and re.match(r"^\d{4}-\d{2}-\d{2}$", args.end_time):
            end_time_utc = end_time_utc.replace(hour=23, minute=59, second=59, microsecond=999999)
    else:
        # --hours å‚æ•°å¢å¼º: å¦‚æœæœªæä¾› end_timeï¼Œåˆ™ end_time é»˜è®¤ä¸ºå½“å‰æ—¶åˆ»
        end_time_utc = datetime.now(timezone.utc)

    # ç¡®å®šå¼€å§‹æ—¶é—´
    if args.start_time:
        start_time_utc = parse_datetime_utc(args.start_time)
        # å¦‚æœæ˜¯çº¯æ—¥æœŸæ ¼å¼ï¼Œç¡®ä¿ä¸ºå½“å¤©å¼€å§‹ (å°½ç®¡ parse_datetime_utc å·²å¤„ç†ï¼Œä½†ä¸ºæ¸…æ™°èµ·è§)
        if start_time_utc and re.match(r"^\d{4}-\d{2}-\d{2}$", args.start_time):
            start_time_utc = start_time_utc.replace(hour=0, minute=0, second=0, microsecond=0)
    else:
        start_time_utc = end_time_utc - timedelta(hours=args.hours)

    if not start_time_utc or not end_time_utc:
        raise ValueError("æ— æ³•è§£æå¼€å§‹æˆ–ç»“æŸæ—¶é—´ï¼Œè¯·æ£€æŸ¥è¾“å…¥æ ¼å¼ã€‚")

    return start_time_utc, end_time_utc


async def main(args: argparse.Namespace):
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    try:
        start_time, end_time = get_time_window(args)
    except ValueError as e:
        logger.error(e)
        return

    steps = {s.strip() for s in args.steps.lower().split(",")}
    run_all = "all" in steps

    logger.info("=" * 60)
    logger.info(f"ğŸš€ å¯åŠ¨å¢é‡ETLç®¡é“ï¼Œæ—¶é—´çª—å£: {start_time.isoformat()} -> {end_time.isoformat()}")
    logger.info(f"ğŸ”© æ‰§è¡Œæ­¥éª¤: {', '.join(sorted(list(steps)))}")
    logger.info(f"ğŸ“š æ•°æ®æºç›®å½•: {args.data_dir}")
    logger.info("=" * 60)

    config = Config()
    nodes: List[TextNode] = []
    
    # æ­¥éª¤ 1: æ‰«ææ–‡ä»¶
    if run_all or "scan" in steps:
        logger.info("========== æ­¥éª¤ 1: æ‰«ææ–‡ä»¶ ==========")
        file_paths = await find_new_files_in_timespan(
            Path(args.data_dir), start_time, end_time, platform_filter=args.platform
        )

    # --- æ­¥éª¤ 2: è¯»å–åŸå§‹æ–‡ä»¶ (æ´å¯Ÿæ­¥éª¤éœ€è¦) ---
    if "insight" in steps:
        if not file_paths and "scan" not in steps:
            logger.warning("æœªæ‰§è¡Œ'scan'æ­¥éª¤ï¼Œä¸”æœªæä¾›æ–‡ä»¶ï¼Œ'insight'æ­¥éª¤å°†è¢«è·³è¿‡ã€‚")
        else:
            logger.info(f"========== æ­¥éª¤ 2 (æ´å¯Ÿ): è¯»å– {len(file_paths)} ä¸ªåŸå§‹æ–‡ä»¶ ==========")
            docs = await read_raw_documents(file_paths)
            
            logger.info(f"========== æ­¥éª¤ 3 (æ´å¯Ÿ): ç”Ÿæˆæ´å¯Ÿ ==========")
            await generate_and_save_insights(
                docs, config, end_time, insight_char_limit=args.insight_char_limit
            )

    # --- æ­¥éª¤ 3: è½¬æ¢å’Œç´¢å¼• (ç´¢å¼•æ­¥éª¤éœ€è¦) ---
    if "index" in steps:
        if not file_paths and "scan" not in steps:
            logger.warning("æœªæ‰§è¡Œ'scan'æ­¥éª¤ï¼Œä¸”æœªæä¾›æ–‡ä»¶ï¼Œ'index'æ­¥éª¤å°†è¢«è·³è¿‡ã€‚")
        else:
            logger.info(f"========== æ­¥éª¤ 2 (ç´¢å¼•): è½¬æ¢ {len(file_paths)} ä¸ªæ–‡ä»¶ä¸ºèŠ‚ç‚¹ ==========")
            nodes = await process_files_to_nodes(file_paths)
            
            logger.info(f"========== æ­¥éª¤ 3 (ç´¢å¼•): å»ºç«‹Qdrantç´¢å¼• ==========")
            await build_qdrant_indexes(nodes, config)

    logger.info("âœ… ETLç®¡é“æ‰§è¡Œå®Œæ¯•ã€‚")

# /opt/venv/bin/python etl/daily_pipeline.py --hours 24 --steps scan,index,insight
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="NKUWikiå¢é‡ETLç®¡é“")
    parser.add_argument(
        "--data_dir",
        type=str,
        default="/data/raw",
        help="åŸå§‹æ•°æ®å­˜å‚¨ç›®å½•",
    )
    parser.add_argument(
        "--steps",
        type=str,
        default="all",
        help="è¦æ‰§è¡Œçš„æ­¥éª¤ï¼Œç”¨é€—å·åˆ†éš” (scan, index, insight, all)",
    )
    parser.add_argument(
        "--start_time",
        type=str,
        help="å¤„ç†çš„å¼€å§‹æ—¶é—´ (æ”¯æŒ 'YYYY-MM-DD' æˆ– ISOæ ¼å¼, e.g., '2023-10-27T00:00:00+08:00')",
    )
    parser.add_argument(
        "--end_time",
        type=str,
        help="å¤„ç†çš„ç»“æŸæ—¶é—´ (æ”¯æŒ 'YYYY-MM-DD' æˆ– ISOæ ¼å¼, e.g., '2023-10-28T00:00:00+08:00')",
    )
    parser.add_argument(
        "--hours",
        type=int,
        default=24,
        help="å¦‚æœæœªæä¾›start_timeï¼Œåˆ™ä»end_timeå›æº¯çš„å°æ—¶æ•°",
    )
    parser.add_argument(
        "--platform",
        type=str,
        default=None,
        help="åªå¤„ç†ç‰¹å®šå¹³å°çš„æ•°æ® (e.g., 'wechat')",
    )
    parser.add_argument(
        "--insight-char-limit",
        type=int,
        default=58000,
        help="ç”Ÿæˆæ´å¯Ÿæ—¶è¾“å…¥ç»™å¤§è¯­è¨€æ¨¡å‹çš„æœ€å¤§å­—ç¬¦æ•°é™åˆ¶ã€‚é»˜è®¤ä¸º60000ï¼Œä»¥é€‚é…å¸¸è§çš„å¤§æ¨¡å‹ä¸Šä¸‹æ–‡çª—å£ã€‚",
    )

    args = parser.parse_args()
    asyncio.run(main(args))

