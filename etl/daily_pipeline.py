#!/usr/bin/env python3
"""
ETLå¢é‡å¤„ç†ç®¡é“

è¯¥è„šæœ¬è´Ÿè´£æ‰§è¡Œå¢é‡ETLæµç¨‹çš„ç¬¬äºŒå’Œç¬¬ä¸‰é˜¶æ®µï¼š
1.  **æ‰«æä¸èŠ‚ç‚¹åŒ– (Scan & Nodify)**: æ‰«æ `/data/raw` ç›®å½•ï¼Œæ‰¾å‡ºæŒ‡å®šæ—¶é—´çª—å£å†…çš„æ–°å¢/ä¿®æ”¹æ–‡ä»¶ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºLlamaIndexçš„`TextNode`å¯¹è±¡ã€‚
2.  **å»ºç«‹ç´¢å¼• (Indexing)**: å°†æ–°ç”Ÿæˆçš„`TextNode`å¯¹è±¡é€å…¥Qdrantå»ºç«‹å‘é‡ç´¢å¼•ã€‚
3.  **ç”Ÿæˆæ´å¯Ÿ (Insight Generation)**: (å¯é€‰) åŸºäºæ–°å¢èŠ‚ç‚¹ï¼ŒæŒ‰æ¥æºåˆ†ç±»ï¼ˆå®˜æ–¹ã€ç¤¾åŒºã€é›†å¸‚ï¼‰åï¼Œåˆ†åˆ«è°ƒç”¨`text_generator`ç”Ÿæˆåˆ†ææ´å¯Ÿå¹¶å­˜å…¥æ•°æ®åº“ã€‚
"""
import sys
from pathlib import Path

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).resolve().parent.parent))

import argparse
import asyncio
import json
import re
from datetime import datetime, timedelta, timezone
from typing import Dict, List, Optional, Tuple, Literal, Any
from collections import defaultdict

import aiofiles
from llama_index.core.schema import Document, TextNode
from tqdm.asyncio import tqdm as aio_tqdm

from config import Config
from core.agent.text_generator import generate_structured_json
from core.utils.logger import register_logger
from etl.indexing.bm25_indexer import BM25Indexer
from etl.indexing.qdrant_indexer import QdrantIndexer
from etl.load import db_core
from etl.load.db_pool_manager import close_db_pool, init_db_pool
from etl.processors.chunk_cache import ChunkCacheManager
from etl import QDRANT_COLLECTION
from etl.utils.const import (
    university_official_accounts,
    school_official_accounts,
    club_official_accounts,
    unofficial_accounts,
)

logger = register_logger("etl.daily_pipeline")

# å®šä¹‰æ´å¯Ÿåˆ†ç±»
InsightCategory = Literal["å®˜æ–¹", "ç¤¾åŒº", "é›†å¸‚"]

# å®šä¹‰å®˜æ–¹å’Œç¤¾åŒºæ¥æº
OFFICIAL_WECHAT_SOURCES = set(university_official_accounts + school_official_accounts)
COMMUNITY_WECHAT_SOURCES = set(club_official_accounts + unofficial_accounts)


def parse_datetime_utc(time_str: str) -> Optional[datetime]:
    """å°†å­—ç¬¦ä¸²ç¨³å¥åœ°è§£æä¸ºå¸¦UTCæ—¶åŒºçš„æ—¶é—´å¯¹è±¡"""
    if not time_str or not isinstance(time_str, str):
        return None
    try:
        # å°è¯•ISO 8601æ ¼å¼ï¼ˆå¸¦æˆ–ä¸å¸¦'Z'ï¼‰
        if time_str.endswith("Z"):
            time_str = time_str[:-1] + "+00:00"
        dt = datetime.fromisoformat(time_str)
        return dt.astimezone(timezone.utc)
    except (ValueError, TypeError):
        pass

    # å°è¯•å…¶ä»–å¸¸è§æ ¼å¼
    formats_to_try = ["%Y-%m-%d %H:%M:%S", "%Y-%m-%d"]
    for fmt in formats_to_try:
        try:
            dt = datetime.strptime(time_str, fmt)
            # å‡å®šä¸ºæœ¬åœ°æ—¶åŒºå¹¶è½¬æ¢ä¸ºUTC
            return dt.astimezone().astimezone(timezone.utc)
        except ValueError:
            continue
    logger.debug(f"æ— æ³•è§£æçš„æ—¶é—´æ ¼å¼: '{time_str}'")
    return None


async def find_new_files_in_timespan(
    data_dir: Path, start_time: datetime, end_time: datetime, platform_filter: Optional[str] = None
) -> List[Path]:
    """åœ¨/data/rawç›®å½•ä¸­é«˜æ•ˆæŸ¥æ‰¾æŒ‡å®šæ—¶é—´çª—å£å†…å‘å¸ƒçš„JSONæ–‡ä»¶"""
    logger.info(f"å¼€å§‹æ‰«æç›®å½• '{data_dir}'ï¼Œæ—¶é—´èŒƒå›´: {start_time.isoformat()} to {end_time.isoformat()}")
    if platform_filter:
        logger.info(f"ä»…æ‰«æå¹³å°: '{platform_filter}'")
    
    # 1. æ ¹æ®æ—¶é—´èŒƒå›´ï¼Œç¡®å®šéœ€è¦æ‰«æçš„å¹´æœˆç›®å½•
    target_months = set()
    current_date = start_time.date()
    while current_date <= end_time.date():
        target_months.add(current_date.strftime("%Y%m"))
        # ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªæœˆ
        # (è¿™ç§æ–¹å¼å¯ä»¥ç¨³å¥åœ°å¤„ç†æœˆä»½å¤©æ•°ä¸åŒçš„æƒ…å†µ)
        next_month_start = (current_date.replace(day=28) + timedelta(days=4)).replace(day=1)
        current_date = next_month_start

    logger.info(f"å°†ç›®æ ‡æ‰«æèŒƒå›´é™å®šäºä»¥ä¸‹å¹´æœˆç›®å½•: {sorted(list(target_months))}")

    # 2. éå†æ‰€æœ‰ platform/tag ç»„åˆï¼ŒæŸ¥æ‰¾åŒ¹é…çš„å¹´æœˆç›®å½•ï¼Œæ”¶é›†æ–‡ä»¶
    files_to_check = []
    if not data_dir.is_dir():
        logger.warning(f"æ•°æ®æºç›®å½• {data_dir} ä¸å­˜åœ¨ã€‚")
        return []

    for platform_dir in data_dir.iterdir():
        if not platform_dir.is_dir():
            continue
        if platform_filter and platform_dir.name != platform_filter:
            continue
        for tag_dir in platform_dir.iterdir():
            if not tag_dir.is_dir():
                continue
            for month_str in target_months:
                month_dir = tag_dir / month_str
                if month_dir.is_dir():
                    files_to_check.extend(month_dir.rglob("*.json"))
    
    logger.info(f"åœ¨ç›®æ ‡å¹´æœˆç›®å½•ä¸­å…±æ‰¾åˆ° {len(files_to_check)} ä¸ª .json æ–‡ä»¶å¾…ç²¾ç¡®æ£€æŸ¥ã€‚")

    # 3. å¯¹ç­›é€‰åçš„æ–‡ä»¶è¿›è¡Œç²¾ç¡®æ—¶é—´æ£€æŸ¥
    tasks = [is_file_in_timespan(file_path, start_time, end_time) for file_path in files_to_check]
    
    results = await aio_tqdm.gather(
        *tasks, desc="ç²¾ç¡®æ‰«ææ–‡ä»¶", unit="ä¸ª"
    )

    new_files = [path for path in results if path is not None]
    logger.info(f"ç²¾ç¡®æ‰«æå®Œæˆï¼Œåœ¨æ—¶é—´èŒƒå›´å†…æ‰¾åˆ° {len(new_files)} ä¸ªæ–°æ–‡ä»¶ã€‚")
    return new_files


async def is_file_in_timespan(
    file_path: Path, start_time: datetime, end_time: datetime
) -> Optional[Path]:
    """æ£€æŸ¥å•ä¸ªæ–‡ä»¶çš„å‘å¸ƒæ—¶é—´æ˜¯å¦åœ¨æŒ‡å®šèŒƒå›´å†…"""
    try:
        async with aiofiles.open(file_path, "r", encoding="utf-8") as f:
            content = await f.read()
            data = json.loads(content)
        
        publish_time_str = data.get("publish_time")
        publish_time = parse_datetime_utc(publish_time_str)
        
        if publish_time and start_time <= publish_time <= end_time:
            return file_path
    except Exception:
        # å¿½ç•¥JSONè§£æå¤±è´¥æˆ–ç¼ºå°‘æ—¶é—´å­—æ®µçš„æ–‡ä»¶
        return None
    return None


async def process_files_to_nodes(file_paths: List[Path]) -> List[TextNode]:
    """è¯»å–æ–‡ä»¶ï¼Œå¤„ç†å¹¶è½¬æ¢ä¸ºTextNodeåˆ—è¡¨"""
    if not file_paths:
        return []
    
    chunk_manager = ChunkCacheManager()
    all_nodes = []

    async def process_single_file(path: Path) -> List[TextNode]:
        try:
            async with aiofiles.open(path, "r", encoding="utf-8") as f:
                content = await f.read()
                data = json.loads(content)
            
            # ä½¿ç”¨Documentå¯¹è±¡æ¥æ‰¿è½½å†…å®¹å’Œå…ƒæ•°æ®
            doc = Document(
                text=data.get("content", ""),
                metadata={
                    "doc_id": data.get("id"),
                    "title": data.get("title"),
                    "url": data.get("url"),
                    "platform": data.get("platform"),
                    "tag": data.get("tag"),  # ç¡®ä¿tagå­—æ®µè¢«æå–
                    "publish_time": data.get("publish_time"),
                    "file_path": str(path),
                },
            )
            return await chunk_manager.chunk_documents_with_cache([doc], show_progress=False)
        except Exception as e:
            logger.warning(f"å¤„ç†æ–‡ä»¶ {path} å¤±è´¥: {e}")
            return []

    tasks = [process_single_file(path) for path in file_paths]
    results = await aio_tqdm.gather(*tasks, desc="å¤„ç†å¹¶è½¬æ¢èŠ‚ç‚¹", unit="ä¸ª")
    
    for node_list in results:
        all_nodes.extend(node_list)
        
    logger.info(f"æˆåŠŸå°† {len(file_paths)} ä¸ªæ–‡ä»¶è½¬æ¢ä¸º {len(all_nodes)} ä¸ªTextNodeã€‚")
    return all_nodes


async def build_qdrant_indexes(nodes: List[TextNode]):
    """ä¸ºæ–°èŠ‚ç‚¹å»ºç«‹Qdrantç´¢å¼•"""
    if not nodes:
        logger.warning("æ²¡æœ‰èŠ‚ç‚¹éœ€è¦ç´¢å¼•ï¼Œè·³è¿‡Qdrantç´¢å¼•æ­¥éª¤ã€‚")
        return
    qdrant_indexer = QdrantIndexer(QDRANT_COLLECTION)
    logger.info(f"å¼€å§‹å‘Qdranté›†åˆ '{QDRANT_COLLECTION}' ä¸­æ·»åŠ  {len(nodes)} ä¸ªèŠ‚ç‚¹...")
    await qdrant_indexer.build_from_nodes(nodes)
    logger.info("Qdrantç´¢å¼•å»ºç«‹å®Œæˆã€‚")


async def build_es_indexes(nodes: List[TextNode]):
    """ä¸ºæ–°èŠ‚ç‚¹å»ºç«‹Elasticsearchç´¢å¼•"""
    if not nodes:
        logger.warning("æ²¡æœ‰èŠ‚ç‚¹éœ€è¦ç´¢å¼•ï¼Œè·³è¿‡Elasticsearchç´¢å¼•æ­¥éª¤ã€‚")
        return
    es_indexer = ElasticsearchIndexer()
    logger.info(f"å¼€å§‹ä¸º {len(nodes)} ä¸ªèŠ‚ç‚¹å»ºç«‹Elasticsearchç´¢å¼•...")
    await es_indexer.build_from_nodes(nodes)
    logger.info("Elasticsearchç´¢å¼•å»ºç«‹å®Œæˆã€‚")


async def build_bm25_indexes(nodes: List[TextNode]):
    """ä¸ºæ–°èŠ‚ç‚¹å»ºç«‹BM25ç´¢å¼•"""
    if not nodes:
        logger.warning("æ²¡æœ‰èŠ‚ç‚¹éœ€è¦ç´¢å¼•ï¼Œè·³è¿‡BM25ç´¢å¼•æ­¥éª¤ã€‚")
        return
    bm25_indexer = BM25Indexer()
    logger.info(f"å¼€å§‹ä¸º {len(nodes)} ä¸ªèŠ‚ç‚¹å»ºç«‹BM25ç´¢å¼•...")
    await bm25_indexer.build_from_nodes(nodes)
    logger.info("BM25ç´¢å¼•å»ºç«‹å®Œæˆã€‚")


async def read_raw_documents(file_paths: List[Path]) -> List[Dict[str, Any]]:
    """
    å¼‚æ­¥è¯»å–ä¸€ç»„JSONæ–‡ä»¶è·¯å¾„ï¼Œå¹¶è¿”å›å…¶å†…å®¹å­—å…¸çš„åˆ—è¡¨ã€‚
    æ¯ä¸ªè¿”å›çš„å­—å…¸ä¸­ä¼šé¢å¤–æ·»åŠ ä¸€ä¸ª `_file_path` é”®ï¼Œç”¨äºè¿½è¸ªæºæ–‡ä»¶ã€‚
    """
    async def _read_file(path: Path) -> Optional[Dict[str, Any]]:
        try:
            async with aiofiles.open(path, 'r', encoding='utf-8') as f:
                content = await f.read()
                data = json.loads(content)
                data["_file_path"] = str(path)  # æ³¨å…¥æ–‡ä»¶è·¯å¾„
                return data
        except Exception as e:
            logger.error(f"è¯»å–æˆ–è§£ææ–‡ä»¶å¤±è´¥: {path}, é”™è¯¯: {e}")
            return None

    tasks = [_read_file(path) for path in file_paths]
    results = await aio_tqdm.gather(*tasks, desc="è¯»å–åŸå§‹æ–‡æ¡£")
    return [doc for doc in results if doc]  # è¿‡æ»¤æ‰è¯»å–å¤±è´¥çš„None


def build_insight_prompt(docs: List[Dict[str, Any]], category: InsightCategory, char_limit: Optional[int] = None) -> str:
    """
    æ ¹æ®åŸå§‹æ–‡æ¡£å­—å…¸åˆ—è¡¨å’Œåˆ†ç±»æ„å»ºç”¨äºç”Ÿæˆæ´å¯Ÿçš„Promptã€‚
    ä¼šå¯¹è¾“å…¥å†…å®¹è¿›è¡Œæˆªæ–­ï¼Œä»¥ç¡®ä¿æœ€ç»ˆçš„Promptä¸è¶…è¿‡æŒ‡å®šçš„å­—ç¬¦é™åˆ¶ã€‚
    """
    # 1. å®šä¹‰æç¤ºè¯çš„å¤´éƒ¨å’Œå°¾éƒ¨
    prompt_header = f"""
ä½œä¸ºä¸€åæ ¡å›­ä¿¡æ¯åˆ†æä¸“å®¶ï¼Œè¯·å¯¹ä»¥ä¸‹æ–°å‘å¸ƒçš„ **{category}** ç›¸å…³æ ¡å›­åŠ¨æ€è¿›è¡Œåˆ†æã€æ€»ç»“å’Œå½’ç±»ï¼Œå¹¶æå–æ ¸å¿ƒæ´å¯Ÿã€‚

**åˆ†æä»»åŠ¡:**
1.  **æ€»ç»“è¦ç‚¹**: å¯¹æ‰€æœ‰åŠ¨æ€è¿›è¡Œæ¦‚æ‹¬ï¼Œå½¢æˆä¸€æ®µ100å­—ä»¥å†…çš„æ€»ä½“æ‘˜è¦ã€‚
2.  **æå–æ´å¯Ÿ**: è¯†åˆ«å‡º3-5ä¸ªæœ€é‡è¦æˆ–æœ€æœ‰è¶£çš„ä¸»é¢˜/è¶‹åŠ¿ã€‚å¯¹äºæ¯ä¸ªä¸»é¢˜ï¼Œä½ å¿…é¡»æä¾›ï¼š
    - **ä¸€ä¸ªç®€çŸ­çš„æ ‡é¢˜**
    - **ä¸€æ®µè¯¦å°½çš„åˆ†ææè¿°ï¼Œè¦æ±‚å†…å®¹å……å®ï¼Œå­—æ•°ä¸å¾—å°‘äº1000å­—**ã€‚ç®€çŸ­çš„æè¿°æ˜¯ä¸å¯æ¥å—çš„ã€‚

**è¿‘æœŸåŠ¨æ€å…¨æ–‡åˆ—è¡¨:**
"""

    prompt_footer = """

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ç»“æœï¼Œä¸è¦æ·»åŠ ä»»ä½•é¢å¤–çš„è§£é‡Šæˆ–æ³¨é‡Šï¼š
```json
{{
    "summary": "æ€»ä½“æ‘˜è¦å†…å®¹ï¼ˆ100å­—ä»¥å†…ï¼‰...",
    "insights": [
        {{
            "title": "æ´å¯Ÿä¸»é¢˜1",
            "content": "å…³äºä¸»é¢˜1çš„è¯¦å°½åˆ†æï¼ˆä¸å°‘äº1000å­—ï¼‰..."
        }},
        {{
            "title": "æ´å¯Ÿä¸»é¢˜2",
            "content": "å…³äºä¸»é¢˜2çš„è¯¦å°½åˆ†æï¼ˆä¸å°‘äº1000å­—ï¼‰..."
        }}
    ]
}}
```
"""

    # 2. è®¡ç®—æ–‡æ¡£éƒ¨åˆ†å¯ç”¨çš„å­—ç¬¦é™åˆ¶
    docs_char_limit = None
    if char_limit:
        template_overhead = len(prompt_header) + len(prompt_footer)
        docs_char_limit = char_limit - template_overhead

        # å¦‚æœé™åˆ¶å¤ªå°ï¼Œæ— æ³•å®¹çº³æ¨¡æ¿æœ¬èº«ï¼Œåˆ™è®°å½•é”™è¯¯å¹¶è¿”å›ç©º
        if docs_char_limit <= 0:
            logger.error(
                f"æ€»å­—ç¬¦é™åˆ¶({char_limit})å¤ªå°ï¼Œä¸è¶³ä»¥å®¹çº³Promptæ¨¡æ¿çš„å›ºå®šå†…å®¹({template_overhead})ã€‚"
            )
            return ""

    # 3. å¡«å……æ–‡æ¡£å†…å®¹ï¼Œç¡®ä¿ä¸è¶…è¿‡ docs_char_limit
    doc_details = []
    current_docs_len = 0
    separator = "\n\n---\n\n"

    for doc in docs:
        title = doc.get("title", "æ— æ ‡é¢˜")
        publish_time = doc.get("publish_time", "æœªçŸ¥æ—¶é—´")
        tag = doc.get("tag", "æœªçŸ¥æ¥æº")
        full_content = doc.get("content", "")
        
        detail_str = (
            f"æ¥æº: {tag}\n"
            f"æ ‡é¢˜: {title}\n"
            f"å‘å¸ƒæ—¶é—´: {publish_time}\n"
            f"å†…å®¹:\n{full_content}"
        )

        # è®¡ç®—æ–°æ–‡æ¡£åŠ å…¥åä¼šå¢åŠ çš„é•¿åº¦ï¼ˆåŒ…æ‹¬åˆ†éš”ç¬¦ï¼‰
        added_len = len(detail_str) + (len(separator) if doc_details else 0)

        if docs_char_limit and (current_docs_len + added_len > docs_char_limit):
            logger.warning(
                f"æ–‡æ¡£å†…å®¹éƒ¨åˆ†å·²è¾¾åˆ°é™åˆ¶({docs_char_limit}å­—ç¬¦)ï¼Œåœæ­¢æ·»åŠ æ›´å¤šæ–‡æ¡£ã€‚ "
                f"æ€»å­—ç¬¦é™åˆ¶: {char_limit}, æ¨¡æ¿å¼€é”€: {template_overhead}. "
                f"æ€»æ–‡æ¡£æ•°: {len(docs)}, å®é™…å¤„ç†: {len(doc_details)}."
            )
            break
        
        doc_details.append(detail_str)
        current_docs_len += added_len
    
    # 4. ç»„è£…æœ€ç»ˆçš„Prompt
    docs_str = separator.join(doc_details)
    prompt = f"{prompt_header}{docs_str}{prompt_footer}"

    logger.info(f"ä¸ºåˆ†ç±» '{category}' ç”Ÿæˆæ´å¯Ÿçš„Promptï¼Œè¾“å…¥æ€»å­—ç¬¦æ•°: {len(prompt)}")
    logger.debug(f"Prompt for '{category}': \n{prompt[:500]}...")
    return prompt


async def generate_and_save_insights(
    docs: List[Dict[str, Any]],
    end_time: datetime,
    insight_char_limit: Optional[int] = None,
):
    """
    åŸºäºåŸå§‹æ–‡æ¡£åˆ—è¡¨ç”Ÿæˆæ´å¯Ÿï¼Œå¹¶å°†å…¶åˆ†ç±»å­˜å…¥æ•°æ®åº“ã€‚
    """
    if not docs:
        logger.info("æ²¡æœ‰æ–°çš„æ–‡æ¡£å¯ä¾›ç”Ÿæˆæ´å¯Ÿã€‚")
        return

    # 0. æŒ‰å‘å¸ƒæ—¶é—´å€’åºæ’åºæ–‡æ¡£ï¼Œä¼˜å…ˆå¤„ç†æœ€æ–°çš„å†…å®¹
    docs.sort(key=lambda d: parse_datetime_utc(d.get("publish_time")) or datetime.min.replace(tzinfo=timezone.utc), reverse=True)

    # 1. å®šä¹‰å®˜æ–¹æ¥æº
    official_wechat_sources = set(university_official_accounts + school_official_accounts)
    community_wechat_sources = set(club_official_accounts + unofficial_accounts)

    # 2. å°†æ–‡æ¡£åˆ†ç±»
    categorized_docs = defaultdict(list)
    for doc in docs:
        platform = doc.get("platform")
        author = doc.get("author") # ä½¿ç”¨ author å­—æ®µè¿›è¡Œåˆ†ç±»
        category = None

        if platform == "website" or (platform == "wechat" and author in official_wechat_sources):
            category = "å®˜æ–¹"
        elif platform == "wechat" and author in community_wechat_sources:
            category = "ç¤¾åŒº"
        elif platform == "market":
            category = "é›†å¸‚"

        if category:
            categorized_docs[category].append(doc)

    logger.info(
        f"æ–‡æ¡£åˆ†ç±»å®Œæˆï¼šå®˜æ–¹({len(categorized_docs['å®˜æ–¹'])}), "
        f"ç¤¾åŒº({len(categorized_docs['ç¤¾åŒº'])}), "
        f"é›†å¸‚({len(categorized_docs['é›†å¸‚'])})"
    )

    # 3. ä¸ºæ¯ä¸ªåˆ†ç±»ç”Ÿæˆå¹¶å­˜å‚¨æ´å¯Ÿ
    for category, doc_list in categorized_docs.items():
        if not doc_list:
            logger.info(f"åˆ†ç±» '{category}' ä¸­æ²¡æœ‰æ–°æ–‡æ¡£ï¼Œè·³è¿‡æ´å¯Ÿç”Ÿæˆã€‚")
            continue

        # ç¡®å®šè¯¥åˆ†ç±»æ´å¯Ÿçš„æ—¥æœŸï¼Œåº”åŸºäºè¯¥åˆ†ç±»ä¸‹æœ€æ–°çš„æ–‡æ¡£å‘å¸ƒæ—¥æœŸ
        # doc_list å·²æŒ‰å‘å¸ƒæ—¥æœŸé™åºæ’åºï¼Œæ‰€ä»¥ç¬¬ä¸€ä¸ªæ–‡æ¡£å°±æ˜¯æœ€æ–°çš„
        latest_publish_time_str = doc_list[0].get("publish_time")
        insight_date = parse_datetime_utc(latest_publish_time_str)
        if not insight_date:
            # å¦‚æœæœ€æ–°çš„æ–‡æ¡£æ²¡æœ‰æœ‰æ•ˆçš„å‘å¸ƒæ—¶é—´ï¼Œåˆ™å›é€€åˆ°ä½¿ç”¨end_time
            logger.warning(
                f"æ— æ³•ä»åˆ†ç±» '{category}' çš„æœ€æ–°æ–‡æ¡£ä¸­è§£æå‘å¸ƒæ—¥æœŸ "
                f"(è·¯å¾„: {doc_list[0].get('_file_path')})ï¼Œå°†ä½¿ç”¨ä»»åŠ¡ç»“æŸæ—¥æœŸä½œä¸ºæ´å¯Ÿæ—¥æœŸã€‚"
            )
            insight_date = end_time
        
        insight_date = insight_date.date() # å–æ—¥æœŸéƒ¨åˆ†

        logger.info(f"å¼€å§‹ä¸ºåˆ†ç±» '{category}' ç”Ÿæˆæ´å¯Ÿ (åŸºäº {len(doc_list)} ä¸ªæ–‡æ¡£)ï¼Œæ´å¯Ÿæ—¥æœŸ: {insight_date}")
        try:
            prompt = build_insight_prompt(
                doc_list, category, char_limit=insight_char_limit
            )
            generated_data = await generate_structured_json(prompt)

            if not generated_data or not isinstance(generated_data, dict) or "insights" not in generated_data:
                logger.error(
                    f"LLMè¿”å›çš„æ•°æ®æ ¼å¼ä¸æ­£ç¡®æˆ–ç¼ºå°‘'insights'é”®ï¼Œè·³è¿‡åˆ†ç±» '{category}' çš„å­˜å‚¨ã€‚",
                    extra={"llm_response": generated_data},
                )
                # è®°å½•å¯èƒ½å¯¼è‡´é—®é¢˜çš„æ–‡æ¡£è·¯å¾„
                problematic_files = [doc.get("_file_path", "æœªçŸ¥è·¯å¾„") for doc in doc_list]
                logger.warning(f"è§¦å‘é—®é¢˜çš„æ–‡æ¡£åˆ—è¡¨ (å…± {len(problematic_files)} ä¸ª): {problematic_files}")
                continue
            
            insights_list = generated_data.get("insights", [])
            if not insights_list or not isinstance(insights_list, list):
                logger.warning(
                    f"LLMè¿”å›çš„'insights'ä¸ºç©ºåˆ—è¡¨æˆ–æ ¼å¼ä¸æ­£ç¡®ï¼Œè·³è¿‡åˆ†ç±» '{category}' çš„å­˜å‚¨ã€‚",
                    extra={"llm_response": generated_data},
                )
                continue

            # å‡†å¤‡å­˜å…¥æ•°æ®åº“çš„æ•°æ®
            db_records = []
            for insight in insights_list:
                # ç¡®ä¿ insight æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œå¹¶ä¸”æœ‰ title å’Œ content
                if isinstance(insight, dict) and "title" in insight and "content" in insight:
                    db_records.append({
                        "title": insight.get("title"),
                        "content": insight.get("content"),
                        "category": category,
                        "insight_date": insight_date,
                    })
                else:
                    logger.warning(f"åˆ†ç±» '{category}' ä¸­æœ‰ä¸€æ¡æ´å¯Ÿæ ¼å¼ä¸æ­£ç¡®ï¼Œå·²è·³è¿‡: {insight}")

            if db_records:
                inserted_count = await db_core.batch_insert("insights", db_records)
                logger.info(f"æˆåŠŸä¸ºåˆ†ç±» '{category}' æ’å…¥ {inserted_count} æ¡æ´å¯Ÿåˆ°æ•°æ®åº“ã€‚")
            else:
                logger.warning(f"ä¸ºåˆ†ç±» '{category}' å‡†å¤‡äº† 0 æ¡æœ‰æ•ˆçš„æ´å¯Ÿè®°å½•ï¼Œæœªæ‰§è¡Œæ•°æ®åº“æ’å…¥ã€‚")

        except Exception as e:
            logger.error(f"ä¸ºåˆ†ç±» '{category}' ç”Ÿæˆæˆ–å­˜å‚¨æ´å¯Ÿå¤±è´¥: {e}", exc_info=True)


def get_time_window(args: argparse.Namespace) -> Tuple[datetime, datetime]:
    """æ ¹æ®å‘½ä»¤è¡Œå‚æ•°è®¡ç®—å¹¶è¿”å›UTCæ—¶é—´çª—å£ (start_time, end_time)"""
    now = datetime.now(timezone.utc)

    # ä¼˜å…ˆå¤„ç† --start_time å’Œ --end_time
    if args.start_time or args.end_time:
        start_time_utc = parse_datetime_utc(args.start_time) if args.start_time else None
        end_time_utc = parse_datetime_utc(args.end_time) if args.end_time else now

        if start_time_utc and args.start_time and re.match(r"^\d{4}-\d{2}-\d{2}$", args.start_time):
            start_time_utc = start_time_utc.replace(hour=0, minute=0, second=0, microsecond=0)

        if end_time_utc and args.end_time and re.match(r"^\d{4}-\d{2}-\d{2}$", args.end_time):
            end_time_utc = end_time_utc.replace(hour=23, minute=59, second=59, microsecond=999999)
        
        if start_time_utc is None:
            # å¦‚æœåªæä¾›äº† end_timeï¼Œåˆ™é»˜è®¤ä»24å°æ—¶å‰å¼€å§‹
            start_time_utc = end_time_utc - timedelta(hours=24)

    # ç„¶åå¤„ç† --daysï¼Œè¿™ä¼šè¦†ç›– --hours
    elif args.days is not None:
        today_midnight = now.replace(hour=0, minute=0, second=0, microsecond=0)
        end_time_utc = today_midnight - timedelta(microseconds=1)  # æ˜¨å¤© 23:59:59.999999
        start_time_utc = today_midnight - timedelta(days=args.days) # Nå¤©å‰çš„ 00:00:00
    
    # æ¥ç€å¤„ç† --hours
    elif args.hours is not None:
        end_time_utc = now
        start_time_utc = end_time_utc - timedelta(hours=args.hours)
    
    # æœ€åæ˜¯é»˜è®¤æƒ…å†µ
    else:
        # é»˜è®¤å›æº¯1å¤© (æ»šåŠ¨çª—å£)
        end_time_utc = now
        start_time_utc = end_time_utc - timedelta(days=1)

    if start_time_utc >= end_time_utc:
        raise ValueError(
            f"è®¡ç®—å‡ºçš„å¼€å§‹æ—¶é—´ {start_time_utc.isoformat()} ä¸èƒ½æ™šäºæˆ–ç­‰äºç»“æŸæ—¶é—´ {end_time_utc.isoformat()}"
        )

    return start_time_utc, end_time_utc


async def main(args: argparse.Namespace):
    """ETLç®¡é“ä¸»å‡½æ•°"""
    await init_db_pool()
    try:
        start_time, end_time = get_time_window(args)
        steps = {s.strip() for s in args.steps.split(",")}

        if "all" in steps:
            steps.update(["scan", "qdrant", "es", "bm25", "insight"])
        # å…¼å®¹æ—§çš„ 'index' æ­¥éª¤
        if "index" in steps:
            steps.update(["qdrant", "es", "bm25"])
            steps.discard("index")

        logger.info("=" * 60)
        logger.info(f"ğŸš€ å¯åŠ¨å¢é‡ETLç®¡é“ï¼Œæ—¶é—´çª—å£: {start_time.isoformat()} -> {end_time.isoformat()}")
        logger.info(f"ğŸ”© æ‰§è¡Œæ­¥éª¤: {', '.join(sorted(list(steps)))}")
        logger.info(f"ğŸ“š æ•°æ®æºç›®å½•: {args.data_dir}")
        logger.info("=" * 60)

        # --- æ­¥éª¤ 1: æ‰«ææ–°æ–‡ä»¶ ---
        file_paths = []
        downstream_steps = {"qdrant", "es", "bm25", "insight"}
        # å¦‚æœç”¨æˆ·æ˜ç¡®è¦æ±‚æ‰«æï¼Œæˆ–è¦æ±‚æ‰§è¡Œä»»ä½•éœ€è¦æ–‡ä»¶çš„ä¸‹æ¸¸æ­¥éª¤ï¼Œåˆ™å¿…é¡»æ‰«æ
        if "scan" in steps or any(s in steps for s in downstream_steps):
            logger.info("========== æ­¥éª¤ 1: æ‰«ææ–°æ–‡ä»¶ ==========")
            file_paths = await find_new_files_in_timespan(
                args.data_dir, start_time, end_time, args.platform
            )
            logger.info(f"æ‰«æå®Œæˆï¼Œæ‰¾åˆ° {len(file_paths)} ä¸ªæ–°æ–‡ä»¶ã€‚")
            if not file_paths:
                logger.info("æ²¡æœ‰æ‰¾åˆ°æ–°æ–‡ä»¶ï¼Œæµç¨‹æå‰ç»“æŸã€‚")
                return
        else:
            logger.info("æœªæŒ‡å®šéœ€è¦å¤„ç†æ•°æ®çš„æ­¥éª¤ (å¦‚ qdrant, insight)ï¼Œæµç¨‹ç»“æŸã€‚")
            return

        # --- (éšå¼) æ­¥éª¤ 2: è½¬æ¢æ–‡ä»¶ä¸ºèŠ‚ç‚¹ (ä»…åœ¨éœ€è¦ç´¢å¼•æ—¶)---
        nodes = []
        indexing_steps = {"qdrant", "bm25"}
        if any(s in steps for s in indexing_steps):
            logger.info(f"========== æ­¥éª¤ 2: ä¸º {len(file_paths)} ä¸ªæ–‡ä»¶è½¬æ¢èŠ‚ç‚¹ ==========")
            nodes = await process_files_to_nodes(file_paths)
            if not nodes:
                logger.warning("æœªèƒ½ä»æ–‡ä»¶è½¬æ¢å‡ºä»»ä½•èŠ‚ç‚¹ï¼Œç´¢å¼•æ­¥éª¤å°†ä¸ä¼šæ‰§è¡Œã€‚")

        # --- æ­¥éª¤ 3: å»ºç«‹å„ç±»ç´¢å¼• ---
        if "qdrant" in steps:
            logger.info(f"========== æ­¥éª¤ 3a: ä¸º {len(nodes)} ä¸ªèŠ‚ç‚¹å»ºç«‹Qdrantç´¢å¼• ==========")
            await build_qdrant_indexes(nodes)
        
        if "es" in steps:
            logger.info(f"========== æ­¥éª¤ 3b: ä¸º {len(nodes)} ä¸ªèŠ‚ç‚¹å»ºç«‹Elasticsearchç´¢å¼• ==========")
            await build_es_indexes(nodes)

        if "bm25" in steps:
            logger.info(f"========== æ­¥éª¤ 3c: ä¸º {len(nodes)} ä¸ªèŠ‚ç‚¹å»ºç«‹BM25ç´¢å¼• ==========")
            await build_bm25_indexes(nodes)

        # --- æ­¥éª¤ 4: ç”Ÿæˆæ´å¯Ÿ ---
        if "insight" in steps:
            logger.info(f"========== æ­¥éª¤ 4: ä¸º {len(file_paths)} ä¸ªæ–‡ä»¶ç”Ÿæˆæ´å¯Ÿ ==========")
            # æ´å¯Ÿç”Ÿæˆéœ€è¦åŸå§‹æ–‡ä»¶å†…å®¹
            raw_docs = await read_raw_documents(file_paths)
            await generate_and_save_insights(raw_docs, end_time, args.insight_char_limit)

        logger.info("âœ… ETLç®¡é“æ‰€æœ‰æŒ‡å®šæ­¥éª¤æ‰§è¡Œå®Œæ¯•ã€‚")
    finally:
        await close_db_pool()

def main_cli():
    """å‘½ä»¤è¡Œæ¥å£"""
    parser = argparse.ArgumentParser(description="å¢é‡ETLå¤„ç†ç®¡é“")
    parser.add_argument(
        "--data_dir",
        type=Path,
        default=Path("/data/raw"),
        help="è¦æ‰«æçš„æ ¹æ•°æ®ç›®å½•",
    )
    parser.add_argument(
        "--steps",
        type=str,
        default="scan,qdrant,es,bm25,insight",
        help="è¦æ‰§è¡Œçš„ETLæ­¥éª¤ï¼Œä»¥é€—å·åˆ†éš”ã€‚ "
             "å¯é€‰å€¼: all, scan, qdrant, es, bm25, insightã€‚ "
             "'all' å°†æ‰§è¡Œæ‰€æœ‰æ­¥éª¤ã€‚ "
             "'index' (å…¼å®¹æ—§ç‰ˆ) ä¼šæ‰§è¡Œ qdrant, es, bm25ã€‚",
    )
    parser.add_argument(
        "--hours", type=int, help="ä»ç°åœ¨å¼€å§‹å›æº¯çš„å°æ—¶æ•°"
    )
    parser.add_argument(
        "--days", type=int, help="ä»ç°åœ¨å¼€å§‹å›æº¯çš„å¤©æ•°ã€‚å¦‚æœæœªæŒ‡å®šä»»ä½•æ—¶é—´å‚æ•°ï¼Œé»˜è®¤ä¸º1å¤©ã€‚"
    )
    parser.add_argument(
        "--start_time", type=str, help="å¼€å§‹æ—¶é—´ (æ ¼å¼: 'YYYY-MM-DD HH:MM:SS')"
    )
    parser.add_argument(
        "--end_time", type=str, help="ç»“æŸæ—¶é—´ (æ ¼å¼: 'YYYY-MM-DD HH:MM:SS')"
    )
    parser.add_argument(
        "--platform", type=str, help="åªæ‰«æç‰¹å®šå¹³å° (ä¾‹å¦‚ 'wechat', 'website')"
    )
    parser.add_argument(
        "--insight_char_limit", type=int, default=64 * 1000, help="ç”Ÿæˆæ´å¯Ÿæ—¶è¾“å…¥ç»™LLMçš„å­—ç¬¦æ•°é™åˆ¶"
    )

    args = parser.parse_args()
    asyncio.run(main(args))


if __name__ == "__main__":
    main_cli()
