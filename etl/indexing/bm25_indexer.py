"""
BM25ç´¢å¼•æ„å»ºå™¨

è´Ÿè´£ä»MySQLæ•°æ®æ„å»ºBM25æ–‡æœ¬æ£€ç´¢ç´¢å¼•ã€‚
"""

import os
import sys
import pickle
import logging
import asyncio
from pathlib import Path
from typing import List, Dict, Any, Optional
import jieba
import aiofiles
import aiofiles.os
from tqdm.asyncio import tqdm
import json

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).resolve().parent.parent.parent))
from config import Config
from etl.load import db_core
from etl.retrieval.retrievers import BM25Retriever
from llama_index.core.schema import BaseNode, TextNode
# å¯¼å…¥ETLæ¨¡å—çš„ç»Ÿä¸€è·¯å¾„é…ç½®
from etl import INDEX_PATH, NLTK_PATH, RAW_PATH

logger = logging.getLogger(__name__)


class BM25Indexer:
    """BM25ç´¢å¼•æ„å»ºå™¨
    
    è´Ÿè´£ä»MySQLæ•°æ®æ„å»ºBM25æ–‡æœ¬æ£€ç´¢ç´¢å¼•ï¼Œæ”¯æŒä¸­æ–‡åˆ†è¯å’Œåœç”¨è¯è¿‡æ»¤ã€‚
    """
    
    def __init__(self, config: Dict[str, Any], logger: Optional[logging.Logger] = None):
        self.config = config
        self.logger = logger or logging.getLogger(__name__)
        
        # ä½¿ç”¨ETLæ¨¡å—ç»Ÿä¸€é…ç½®çš„è·¯å¾„
        self.output_path = config.get('etl.retrieval.bm25.nodes_path', 
                                     str(INDEX_PATH / 'bm25_nodes.pkl'))
        self.stopwords_path = config.get('etl.retrieval.bm25.stopwords_path', 
                                        str(NLTK_PATH / 'hit_stopwords.txt'))
        
        # åˆ†å—å‚æ•°ï¼ˆå¯é€‰ï¼Œç”¨äºæ”¯æŒé•¿æ–‡æ¡£ï¼‰
        self.enable_chunking = config.get('etl.retrieval.bm25.enable_chunking', False)
        self.chunk_size = config.get('etl.chunking.chunk_size', 512)
        self.chunk_overlap = config.get('etl.chunking.chunk_overlap', 200)
        
    async def build_indexes(self, 
                     limit: int = None, 
                     bm25_type: int = 0,
                     test_mode: bool = False,
                     data_source: str = "raw_files",
                     batch_size: int = -1,
                     start_batch: int = 0,
                     max_batches: int = None) -> Dict[str, Any]:
        """
        æ„å»ºBM25ç´¢å¼•
        
        Args:
            limit: é™åˆ¶å¤„ç†çš„è®°å½•æ•°é‡ï¼ŒNoneè¡¨ç¤ºå¤„ç†æ‰€æœ‰
            bm25_type: BM25ç®—æ³•ç±»å‹ (0: BM25Okapi, 1: BM25)
            test_mode: æµ‹è¯•æ¨¡å¼ï¼Œä¸å®é™…ä¿å­˜æ–‡ä»¶
            data_source: æ•°æ®æºç±»å‹ ("raw_files"=æ··åˆæ¨¡å¼, "mysql"=ä»…MySQL, "raw_only"=ä»…åŸå§‹æ–‡ä»¶)
            batch_size: æ¯æ‰¹å¤„ç†çš„è®°å½•æ•°é‡ï¼Œ-1è¡¨ç¤ºä¸åˆ†æ‰¹
            start_batch: ä»ç¬¬å‡ æ‰¹å¼€å§‹å¤„ç†ï¼ˆåˆ†æ‰¹æ„å»ºï¼‰
            max_batches: æœ€å¤§æ‰¹æ¬¡æ•°ï¼ŒNoneè¡¨ç¤ºå¤„ç†æ‰€æœ‰æ‰¹æ¬¡
            
        Returns:
            æ„å»ºç»“æœç»Ÿè®¡
        """
        self.logger.info(f"å¼€å§‹æ„å»ºBM25ç´¢å¼•ï¼Œè¾“å‡ºè·¯å¾„: {self.output_path}")
        
        try:
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            if not test_mode:
                await aiofiles.os.makedirs(os.path.dirname(self.output_path), exist_ok=True)
            
            # åŠ è½½åœç”¨è¯
            print("ğŸ” åŠ è½½åœç”¨è¯...")
            stopwords = await self._load_stopwords()
            self.logger.info(f"åŠ è½½äº† {len(stopwords)} ä¸ªåœç”¨è¯")
            
            # æ ¹æ®æ•°æ®æºç±»å‹åŠ è½½æ•°æ®
            if data_source == "raw_files":
                print("ğŸ“ æ··åˆæ¨¡å¼ï¼šä»åŸå§‹æ–‡ä»¶+PageRankæ•°æ®...")
                nodes = await self._load_nodes_hybrid(limit)
                self.logger.info(f"æ··åˆæ¨¡å¼åŠ è½½äº† {len(nodes)} ä¸ªèŠ‚ç‚¹")
            elif data_source == "mysql":
                print("ğŸ“Š ä»MySQLæ•°æ®åº“åŠ è½½æ•°æ®...")
                nodes = await self._load_nodes_from_mysql(limit)
                self.logger.info(f"ä»MySQLåŠ è½½äº† {len(nodes)} ä¸ªèŠ‚ç‚¹")
            elif data_source == "raw_only":
                print("ğŸ“ ä»…ä»åŸå§‹JSONæ–‡ä»¶åŠ è½½æ•°æ®...")
                nodes = await self._load_nodes_from_raw_files(limit)
                self.logger.info(f"ä»åŸå§‹æ–‡ä»¶åŠ è½½äº† {len(nodes)} ä¸ªèŠ‚ç‚¹")
            else:
                print("ğŸ“ é»˜è®¤æ··åˆæ¨¡å¼ï¼šä»åŸå§‹æ–‡ä»¶+PageRankæ•°æ®...")
                nodes = await self._load_nodes_hybrid(limit)
                self.logger.info(f"æ··åˆæ¨¡å¼åŠ è½½äº† {len(nodes)} ä¸ªèŠ‚ç‚¹")
            
            if not nodes:
                self.logger.warning("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•èŠ‚ç‚¹æ•°æ®")
                return {"total_nodes": 0, "success": False, "message": "æ²¡æœ‰æ‰¾åˆ°æ•°æ®"}
            
            # å¯é€‰çš„æ–‡æ¡£åˆ†å—ï¼ˆç”¨äºæ”¯æŒé•¿æ–‡æ¡£ï¼‰
            if self.enable_chunking:
                print("ğŸ“„ è¿›è¡Œæ–‡æ¡£åˆ†å—...")
                nodes = await self._chunk_nodes_with_cache(nodes)
                self.logger.info(f"åˆ†å—åæ€»è®¡ {len(nodes)} ä¸ªèŠ‚ç‚¹")
            
            # åˆ›å»ºjiebaåˆ†è¯å™¨å¯¹è±¡ï¼ˆå…·æœ‰cutæ–¹æ³•ï¼‰
            import jieba
            
            # æ„å»ºBM25ç´¢å¼•ï¼ˆè¯¦ç»†è¿›åº¦æ˜¾ç¤ºï¼‰
            print("ğŸ”§ æ„å»ºBM25æ£€ç´¢å™¨...")
            bm25_retriever = await self._build_bm25_retriever_with_progress(
                nodes, jieba, stopwords, bm25_type
            )
            
            # ä¿å­˜ç´¢å¼•
            if not test_mode:
                print("ğŸ’¾ ä¿å­˜ç´¢å¼•æ–‡ä»¶...")
                with tqdm(total=1, desc="ä¿å­˜ç´¢å¼•", unit="æ–‡ä»¶") as pbar:
                    # ä½¿ç”¨BM25æ£€ç´¢å™¨çš„è‡ªå®šä¹‰ä¿å­˜æ–¹æ³•
                    bm25_retriever.save_to_pickle(self.output_path)
                    pbar.update(1)
                self.logger.info(f"BM25æ£€ç´¢å™¨å·²ä¿å­˜åˆ°: {self.output_path}")
            else:
                self.logger.info("æµ‹è¯•æ¨¡å¼ï¼šè·³è¿‡æ–‡ä»¶ä¿å­˜")
            
            print("âœ… BM25ç´¢å¼•æ„å»ºå®Œæˆ!")
            return {
                "total_nodes": len(nodes),
                "success": True,
                "output_path": self.output_path,
                "bm25_type": bm25_type,
                "message": f"æˆåŠŸæ„å»ºBM25ç´¢å¼•ï¼ŒåŒ…å« {len(nodes)} ä¸ªèŠ‚ç‚¹"
            }
            
        except Exception as e:
            print(f"âŒ æ„å»ºå¤±è´¥: {e}")
            self.logger.error(f"æ„å»ºBM25ç´¢å¼•æ—¶å‡ºé”™: {e}")
            return {
                "total_nodes": 0, 
                "success": False, 
                "error": str(e),
                "message": f"BM25ç´¢å¼•æ„å»ºå¤±è´¥: {str(e)}"
            }
    
    async def _load_stopwords(self) -> List[str]:
        """å¼‚æ­¥åŠ è½½åœç”¨è¯"""
        stopwords = []
        
        try:
            exists = False
            try:
                await aiofiles.os.stat(self.stopwords_path)
                exists = True
            except FileNotFoundError:
                exists = False

            if not exists:
                self.logger.warning(f"åœç”¨è¯æ–‡ä»¶ä¸å­˜åœ¨: {self.stopwords_path}")
                return stopwords
            
            async with aiofiles.open(self.stopwords_path, 'r', encoding='utf-8') as f:
                stopwords = [line.strip() for line in await f.readlines() if line.strip()]
            self.logger.info(f"æˆåŠŸåŠ è½½ {len(stopwords)} ä¸ªåœç”¨è¯")
        except Exception as e:
            self.logger.error(f"åŠ è½½åœç”¨è¯æ—¶å‡ºé”™: {e}")
        
        return stopwords

    async def _load_nodes_from_raw_files(self, limit: int = None) -> List[BaseNode]:
        """ä»åŸå§‹JSONæ–‡ä»¶åŠ è½½èŠ‚ç‚¹æ•°æ®ï¼ˆæ¨èæ–¹æ³•ï¼‰"""
        nodes = []
        
        try:
            # å®šä¹‰æ•°æ®æºé…ç½®
            data_sources = [
                {
                    'name': 'website',
                    'path': RAW_PATH / 'website' / 'nku',
                    'platform': 'website'
                },
                {
                    'name': 'wechat', 
                    'path': RAW_PATH / 'wechat' / 'nku',
                    'platform': 'wechat'
                },
                {
                    'name': 'market',
                    'path': RAW_PATH / 'market' / 'nku', 
                    'platform': 'market'
                },
                {
                    'name': 'wxapp',
                    'path': RAW_PATH / 'wxapp',
                    'platform': 'wxapp'
                }
            ]
            
            # ç»Ÿè®¡æ€»æ–‡ä»¶æ•°
            total_files = 0
            source_file_counts = {}
            
            for source in data_sources:
                if source['path'].exists():
                    json_files = list(source['path'].rglob('*.json'))
                    # è¿‡æ»¤æ‰ç‰¹æ®Šæ–‡ä»¶
                    json_files = [f for f in json_files if not f.name.startswith(('scraped_', 'counter.', 'lock.'))]
                    source_file_counts[source['name']] = len(json_files)
                    total_files += len(json_files)
                else:
                    source_file_counts[source['name']] = 0
                    self.logger.warning(f"æ•°æ®æºè·¯å¾„ä¸å­˜åœ¨: {source['path']}")
            
            if limit:
                total_files = min(total_files, limit)
            
            self.logger.info(f"é¢„è®¡å¤„ç† {total_files} ä¸ªJSONæ–‡ä»¶")
            for name, count in source_file_counts.items():
                if count > 0:
                    self.logger.info(f"  {name}: {count} ä¸ªæ–‡ä»¶")
            
            processed_count = 0
            
            # ä½¿ç”¨æ€»ä½“è¿›åº¦æ¡
            with tqdm(total=total_files, desc="åŠ è½½åŸå§‹æ•°æ®", unit="æ–‡ä»¶") as pbar:
                for source in data_sources:
                    if not source['path'].exists():
                        continue
                        
                    # è·å–è¯¥æ•°æ®æºçš„JSONæ–‡ä»¶
                    json_files = list(source['path'].rglob('*.json'))
                    json_files = [f for f in json_files if not f.name.startswith(('scraped_', 'counter.', 'lock.'))]
                    
                    if not json_files:
                        self.logger.info(f"{source['name']} æ²¡æœ‰æ‰¾åˆ°JSONæ–‡ä»¶")
                        continue
                    
                    self.logger.info(f"å¤„ç† {source['name']} æ•°æ®æº: {len(json_files)} ä¸ªæ–‡ä»¶")
                    
                    # å¤„ç†è¯¥æ•°æ®æºçš„æ–‡ä»¶
                    source_desc = f"å¤„ç†{source['name']}"
                    for json_file in tqdm(json_files, desc=source_desc, unit="æ–‡ä»¶", leave=False):
                        if limit and processed_count >= limit:
                            break
                            
                        try:
                            # è¯»å–JSONæ–‡ä»¶
                            async with aiofiles.open(json_file, 'r', encoding='utf-8') as f:
                                content_str = await f.read()
                            
                            if not content_str.strip():
                                continue
                            
                            data = json.loads(content_str)
                            
                            # å¤„ç†ä¸åŒçš„æ•°æ®æ ¼å¼
                            if isinstance(data, list):
                                records = data
                            elif isinstance(data, dict) and 'data' in data:
                                records = data['data'] if isinstance(data['data'], list) else [data['data']]
                            else:
                                records = [data]
                            
                            # è½¬æ¢æ¯æ¡è®°å½•ä¸ºèŠ‚ç‚¹
                            for record in records:
                                try:
                                    node = await self._create_node_from_record(record, source['platform'], json_file)
                                    if node:
                                        nodes.append(node)
                                        
                                except Exception as e:
                                    self.logger.warning(f"å¤„ç†è®°å½•æ—¶å‡ºé”™ {json_file}: {e}")
                                    continue
                            
                            processed_count += 1
                            pbar.update(1)
                            
                        except Exception as e:
                            self.logger.warning(f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™ {json_file}: {e}")
                            pbar.update(1)
                            continue
                    
                    if limit and processed_count >= limit:
                        break
            
            self.logger.info(f"æ€»è®¡ä»åŸå§‹æ–‡ä»¶åŠ è½½ {len(nodes)} ä¸ªèŠ‚ç‚¹")
            
        except Exception as e:
            self.logger.error(f"ä»åŸå§‹æ–‡ä»¶åŠ è½½æ•°æ®æ—¶å‡ºé”™: {e}")
        
        return nodes

    async def _create_node_from_record(self, record: Dict[str, Any], platform: str, source_file: Path) -> Optional[TextNode]:
        """ä»è®°å½•åˆ›å»ºTextNode"""
        try:
            # åŸºæœ¬å­—æ®µæå–
            content = record.get('content', '')
            title = record.get('title', record.get('name', ''))
            
            # å†…å®¹éªŒè¯
            if not content or not content.strip():
                return None
            
            if not title:
                title = f"æ–‡æ¡£_{record.get('id', 'unknown')}"
            
            # åˆå¹¶æ ‡é¢˜å’Œå†…å®¹
            full_text = f"{title}\n{content}" if title else content
            
            # æå–å…¶ä»–å…ƒæ•°æ®
            author = record.get('author', record.get('nickname', ''))
            url = record.get('original_url', record.get('url', record.get('link', '')))
            publish_time = record.get('publish_time', record.get('create_time', record.get('time', '')))
            
            # ç”Ÿæˆå”¯ä¸€ID
            record_id = record.get('id', str(abs(hash(url + title)) % 1000000))
            source_id = f"{platform}_{record_id}"
            
            # æ„é€ URLï¼ˆå¦‚æœæ²¡æœ‰ï¼‰
            if not url:
                if platform == 'wxapp':
                    url = f"wxapp://post/{record_id}"
                else:
                    url = f"{platform}://item/{record_id}"
            
            # åˆ›å»ºTextNode
            node = TextNode(
                text=full_text,
                metadata={
                    'source_id': source_id,
                    'id': record_id,
                    'url': url,
                    'title': title,
                    'author': author,
                    'original_url': url,
                    'publish_time': str(publish_time),
                    'source': platform,
                    'platform': platform,
                    'pagerank_score': float(record.get('pagerank_score', 0.0)),
                    'source_file': str(source_file),
                    'data_source': 'raw_files'
                }
            )
            
            return node
            
        except Exception as e:
            self.logger.warning(f"åˆ›å»ºèŠ‚ç‚¹æ—¶å‡ºé”™: {e}")
            return None

    async def _load_nodes_from_mysql(self, limit: int = None) -> List[BaseNode]:
        """ä»MySQLåŠ è½½èŠ‚ç‚¹æ•°æ®ï¼ˆæ”¯æŒå¤šè¡¨ï¼šwebsite_nku, wechat_nku, wxapp_postï¼‰"""
        nodes = []
        
        try:
            # å®šä¹‰è¡¨é…ç½®ï¼Œç»Ÿä¸€å­—æ®µæ˜ å°„
            table_configs = [
                {
                    'table': 'website_nku',
                    'fields': {
                        'id': 'id',
                        'url': 'original_url', 
                        'title': 'title',
                        'content': 'content',
                        'author': 'author',
                        'publish_time': 'publish_time',
                        'platform': 'platform',
                        'pagerank_score': 'pagerank_score'
                    },
                    'platform_name': 'website'
                },
                {
                    'table': 'wechat_nku',
                    'fields': {
                        'id': 'id',
                        'url': 'original_url',
                        'title': 'title', 
                        'content': 'content',
                        'author': 'author',
                        'publish_time': 'publish_time',
                        'platform': 'platform',
                        'pagerank_score': 'COALESCE(0.0, 0.0)'  # wechatè¡¨æ²¡æœ‰pagerank_scoreå­—æ®µï¼Œè®¾ä¸ºé»˜è®¤å€¼
                    },
                    'platform_name': 'wechat'
                },
                {
                    'table': 'wxapp_post',
                    'fields': {
                        'id': 'id',
                        'url': 'CONCAT("wxapp://post/", id)',  # wxappæ²¡æœ‰URLï¼Œæ„é€ ä¸€ä¸ª
                        'title': 'title',
                        'content': 'content', 
                        'author': 'nickname',
                        'publish_time': 'create_time',  # wxappä½¿ç”¨create_time
                        'platform': '"wxapp"',  # å›ºå®šå€¼
                        'pagerank_score': 'COALESCE(0.0, 0.0)'  # wxappè¡¨æ²¡æœ‰pagerank_scoreå­—æ®µï¼Œè®¾ä¸ºé»˜è®¤å€¼
                    },
                    'where_clause': 'status = 1 AND is_deleted = 0',  # wxappç‰¹æœ‰çš„ç­›é€‰æ¡ä»¶
                    'platform_name': 'wxapp'
                }
            ]
            
            # å…ˆè·å–å„è¡¨çš„æ€»æ•°é‡ï¼Œç”¨äºè¿›åº¦æ¡
            table_counts = {}
            for config in table_configs:
                try:
                    count_sql = f"""
                    SELECT COUNT(*) as total
                    FROM {config['table']}
                    WHERE content IS NOT NULL AND content != ''
                    """
                    if 'where_clause' in config:
                        count_sql += f" AND {config['where_clause']}"
                    
                    count_result = await db_core.execute_query(count_sql, fetch=True)
                    table_counts[config['table']] = count_result[0]['total'] if count_result else 0
                except Exception as e:
                    self.logger.warning(f"è·å–{config['table']}æ•°é‡æ—¶å‡ºé”™: {e}")
                    table_counts[config['table']] = 0
            
            total_expected = sum(table_counts.values())
            if limit:
                total_expected = min(total_expected, limit * len(table_configs))
            
            self.logger.info(f"é¢„è®¡ä»{len(table_configs)}ä¸ªè¡¨åŠ è½½çº¦{total_expected}æ¡è®°å½•")
            
            # ä½¿ç”¨æ€»ä½“è¿›åº¦æ¡
            with tqdm(total=total_expected, desc="åŠ è½½MySQLæ•°æ®", unit="æ¡") as pbar:
                # ä»æ¯ä¸ªè¡¨åŠ è½½æ•°æ®
                for config in table_configs:
                    try:
                        # æ„å»ºå­—æ®µé€‰æ‹©
                        field_selections = []
                        for alias, field_expr in config['fields'].items():
                            field_selections.append(f"{field_expr} as {alias}")
                        
                        # æ„å»ºSQLæŸ¥è¯¢
                        sql = f"""
                        SELECT {', '.join(field_selections)}
                        FROM {config['table']}
                        WHERE content IS NOT NULL AND content != ''
                        """
                        
                        # æ·»åŠ è¡¨ç‰¹å®šçš„WHEREæ¡ä»¶
                        if 'where_clause' in config:
                            sql += f" AND {config['where_clause']}"
                        
                        sql += " ORDER BY id"
                        
                        # åº”ç”¨é™åˆ¶ï¼ˆå¦‚æœæŒ‡å®šï¼‰
                        table_limit = limit if limit else None
                        if table_limit:
                            sql += f" LIMIT {table_limit}"
                        
                        self.logger.debug(f"æŸ¥è¯¢{config['table']}è¡¨")
                        
                        # æ‰§è¡ŒæŸ¥è¯¢
                        records = await db_core.execute_query(sql, fetch=True)
                        
                        if not records:
                            self.logger.info(f"{config['table']}ä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•è®°å½•")
                            continue
                        
                        self.logger.info(f"ä»{config['table']}åŠ è½½äº† {len(records)} æ¡è®°å½•")
                        
                        # è½¬æ¢ä¸ºTextNodeï¼ˆå¸¦è¿›åº¦æ¡ï¼‰
                        table_desc = f"å¤„ç†{config['table']}"
                        for record in tqdm(records, desc=table_desc, unit="æ¡", leave=False):
                            try:
                                # æ„å»ºèŠ‚ç‚¹æ–‡æœ¬å†…å®¹
                                content = record.get('content', '')
                                title = record.get('title', '')
                                
                                # åˆå¹¶æ ‡é¢˜å’Œå†…å®¹
                                full_text = f"{title}\n{content}" if title else content
                                
                                # åˆ›å»ºTextNodeï¼ˆä½¿ç”¨ç»Ÿä¸€çš„å…ƒæ•°æ®æ˜ å°„ï¼‰
                                node = TextNode(
                                    text=full_text,
                                    metadata={
                                        'source_id': f"{config['platform_name']}_{record.get('id')}",  # å”¯ä¸€æ ‡è¯†
                                        'id': record.get('id'),
                                        'title': title,
                                        'author': record.get('author', ''),
                                        'original_url': record.get('original_url', ''),
                                        'publish_time': str(record.get('publish_time', '')),
                                        'platform': config['platform_name'],
                                        'pagerank_score': float(record.get('pagerank_score', 0.0)),
                                        'table_name': config['table']  # æ ‡è®°æ¥æºè¡¨
                                    }
                                )
                                
                                nodes.append(node)
                                pbar.update(1)  # æ›´æ–°æ€»ä½“è¿›åº¦
                                
                            except Exception as e:
                                self.logger.warning(f"å¤„ç†{config['table']}è®°å½•æ—¶å‡ºé”™: {e}")
                                continue
                    
                    except Exception as e:
                        self.logger.error(f"ä»{config['table']}åŠ è½½æ•°æ®æ—¶å‡ºé”™: {e}")
                        continue
            
            self.logger.info(f"æ€»è®¡æˆåŠŸåŠ è½½ {len(nodes)} ä¸ªèŠ‚ç‚¹")
            
        except Exception as e:
            self.logger.error(f"ä»MySQLåŠ è½½æ•°æ®æ—¶å‡ºé”™: {e}")
        
        return nodes
    
    async def _build_bm25_retriever_with_progress(self, nodes: List[BaseNode], tokenizer, stopwords: List[str], bm25_type: int) -> 'BM25Retriever':
        """æ„å»ºBM25æ£€ç´¢å™¨å¹¶æ˜¾ç¤ºè¯¦ç»†è¿›åº¦"""
        from etl.processors.nodes import get_node_content
        from etl.retrieval.retrievers import tokenize_and_remove_stopwords
        
        def _build_bm25():
            """åœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œçš„åŒæ­¥æ„å»ºå‡½æ•°"""
            # æ­¥éª¤1: é¢„å¤„ç†æ–‡æœ¬å’Œåˆ†è¯
            corpus = []
            
            with tqdm(nodes, desc="åˆ†è¯å¤„ç†", unit="æ–‡æ¡£") as pbar:
                for node in pbar:
                    try:
                        # è·å–èŠ‚ç‚¹å†…å®¹
                        content = get_node_content(node, embed_type=0)
                        # åˆ†è¯å¹¶å»é™¤åœç”¨è¯
                        tokens = tokenize_and_remove_stopwords(tokenizer, content, stopwords=stopwords)
                        corpus.append(tokens)
                        pbar.set_postfix({'å·²å¤„ç†': len(corpus)})
                    except Exception as e:
                        self.logger.warning(f"åˆ†è¯å¤„ç†èŠ‚ç‚¹å¤±è´¥: {e}")
                        corpus.append([])  # æ·»åŠ ç©ºåˆ—è¡¨é¿å…ç´¢å¼•é”™è¯¯
            
            if not any(corpus):
                raise ValueError("æ‰€æœ‰æ–‡æ¡£åœ¨åˆ†è¯åéƒ½ä¸ºç©º")
            
            # æ­¥éª¤2: æ„å»ºBM25ç´¢å¼•
            with tqdm(total=1, desc="æ„å»ºBM25ç´¢å¼•", unit="ç´¢å¼•") as pbar:
                try:
                    if bm25_type == 1:
                        import bm25s
                        bm25_instance = bm25s.BM25(k1=1.5, b=0.75)
                        bm25_instance.index(corpus)
                    else:
                        from rank_bm25 import BM25Okapi
                        bm25_instance = BM25Okapi(corpus, k1=1.5, b=0.75, epsilon=0.25)
                    pbar.update(1)
                    pbar.set_postfix({'çŠ¶æ€': 'å®Œæˆ'})
                except Exception as e:
                    raise ValueError(f"BM25ç´¢å¼•æ„å»ºå¤±è´¥: {str(e)}")
            
            # æ­¥éª¤3: åˆ›å»ºæ£€ç´¢å™¨
            with tqdm(total=1, desc="åˆå§‹åŒ–æ£€ç´¢å™¨", unit="æ£€ç´¢å™¨") as pbar:
                retriever = BM25Retriever(
                    nodes=nodes,
                    tokenizer=tokenizer,
                    stopwords=stopwords,
                    bm25_type=bm25_type,
                    fast_mode=False,  # ç¡®ä¿ç«‹å³åˆå§‹åŒ–
                    verbose=True
                )
                # æ‰‹åŠ¨è®¾ç½®å·²æ„å»ºçš„BM25ç´¢å¼•å’Œè¯­æ–™åº“
                retriever._corpus = corpus
                retriever.bm25 = bm25_instance
                retriever._initialized = True
                
                pbar.update(1)
                pbar.set_postfix({'çŠ¶æ€': 'å®Œæˆ'})
            
            return retriever
        
        # åœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œæ„å»ºè¿‡ç¨‹
        loop = asyncio.get_running_loop()
        return await loop.run_in_executor(None, _build_bm25)

    async def _chunk_nodes_with_cache(self, doc_nodes: List[BaseNode]) -> List[BaseNode]:
        """ä½¿ç”¨ç¼“å­˜å¯¹æ–‡æ¡£èŠ‚ç‚¹è¿›è¡Œåˆ†å—"""
        from etl.processors.chunk_cache import chunk_documents_cached
        
        self.logger.info(f"å¼€å§‹åˆ†å— {len(doc_nodes)} ä¸ªæ–‡æ¡£èŠ‚ç‚¹...")
        
        # ä½¿ç”¨ç¼“å­˜è¿›è¡Œåˆ†å—
        chunked_nodes = await chunk_documents_cached(
            doc_nodes=doc_nodes,
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            force_refresh=False,
            show_progress=True
        )
        
        self.logger.info(f"åˆ†å—å®Œæˆï¼Œæ€»è®¡ {len(chunked_nodes)} ä¸ªæ–‡æœ¬å—")
        return chunked_nodes

    def _chinese_tokenizer(self, text: str) -> List[str]:
        """ä¸­æ–‡åˆ†è¯å™¨"""
        try:
            import jieba
            # ä½¿ç”¨jiebaè¿›è¡Œä¸­æ–‡åˆ†è¯
            tokens = list(jieba.cut(text))
            # è¿‡æ»¤æ‰ç©ºç™½å’Œå•å­—ç¬¦token
            tokens = [token.strip() for token in tokens if len(token.strip()) > 1]
            return tokens
        except Exception as e:
            self.logger.warning(f"åˆ†è¯æ—¶å‡ºé”™: {e}")
            return text.split()  # é™çº§åˆ°ç®€å•ç©ºæ ¼åˆ†å‰²
    
    async def validate_index(self) -> Dict[str, Any]:
        """å¼‚æ­¥éªŒè¯ç´¢å¼•æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”æœ‰æ•ˆ"""
        exists = False
        try:
            await aiofiles.os.stat(self.output_path)
            exists = True
        except FileNotFoundError:
            exists = False

        if not exists:
            return {
                "valid": False,
                "message": f"ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨: {self.output_path}"
            }
        
        def _load_pickle(data: bytes) -> Any:
            return pickle.loads(data)

        try:
            async with aiofiles.open(self.output_path, 'rb') as f:
                data = await f.read()
            
            loop = asyncio.get_running_loop()
            nodes = await loop.run_in_executor(None, _load_pickle, data)
            
            if not isinstance(nodes, list) or len(nodes) == 0:
                return {
                    "valid": False,
                    "message": "ç´¢å¼•æ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®æˆ–ä¸ºç©º"
                }
            
            return {
                "valid": True,
                "message": f"ç´¢å¼•æ–‡ä»¶æœ‰æ•ˆï¼ŒåŒ…å« {len(nodes)} ä¸ªèŠ‚ç‚¹",
                "node_count": len(nodes)
            }
            
        except Exception as e:
            return {
                "valid": False,
                "message": f"è¯»å–ç´¢å¼•æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}"
            }

    async def _load_nodes_hybrid(self, limit: int = None) -> List[BaseNode]:
        """æ··åˆåŠ è½½æ–¹æ¡ˆï¼šä»åŸå§‹æ–‡ä»¶åŠ è½½åŸºç¡€æ•°æ®ï¼Œä»MySQLè¡¥å……PageRankåˆ†æ•°ï¼ˆæ¨èï¼‰"""
        nodes = []
        
        try:
            # ç¬¬ä¸€æ­¥ï¼šä»åŸå§‹æ–‡ä»¶åŠ è½½åŸºç¡€æ•°æ®
            self.logger.info("ğŸ“ ç¬¬ä¸€æ­¥ï¼šä»åŸå§‹JSONæ–‡ä»¶åŠ è½½åŸºç¡€æ•°æ®...")
            raw_nodes = await self._load_nodes_from_raw_files(limit)
            
            if not raw_nodes:
                self.logger.warning("åŸå§‹æ–‡ä»¶ä¸­æ²¡æœ‰æ•°æ®ï¼Œå›é€€åˆ°MySQLæ¨¡å¼")
                return await self._load_nodes_from_mysql(limit)
            
            # ç¬¬äºŒæ­¥ï¼šä»MySQLåŠ è½½PageRankåˆ†æ•°æ˜ å°„
            self.logger.info("ğŸ“Š ç¬¬äºŒæ­¥ï¼šä»MySQLåŠ è½½PageRankåˆ†æ•°...")
            pagerank_mapping = await self._load_pagerank_mapping()
            
            if pagerank_mapping:
                self.logger.info(f"æˆåŠŸåŠ è½½ {len(pagerank_mapping)} ä¸ªPageRankåˆ†æ•°")
                
                # ç¬¬ä¸‰æ­¥ï¼šä¸ºèŠ‚ç‚¹è¡¥å……PageRankåˆ†æ•°
                updated_count = 0
                with tqdm(raw_nodes, desc="è¡¥å……PageRankåˆ†æ•°", unit="èŠ‚ç‚¹") as pbar:
                    for node in pbar:
                        url = node.metadata.get('original_url', '')
                        if url in pagerank_mapping:
                            node.metadata['pagerank_score'] = float(pagerank_mapping[url])
                            updated_count += 1
                        pbar.set_postfix({'å·²æ›´æ–°': updated_count})
                
                self.logger.info(f"ä¸º {updated_count} ä¸ªèŠ‚ç‚¹è¡¥å……äº†PageRankåˆ†æ•°")
            else:
                self.logger.warning("æ²¡æœ‰æ‰¾åˆ°PageRankåˆ†æ•°ï¼Œæ‰€æœ‰èŠ‚ç‚¹å°†ä½¿ç”¨é»˜è®¤å€¼0.0")
            
            nodes = raw_nodes
            self.logger.info(f"æ··åˆåŠ è½½å®Œæˆï¼Œæ€»è®¡ {len(nodes)} ä¸ªèŠ‚ç‚¹")
            
        except Exception as e:
            self.logger.error(f"æ··åˆåŠ è½½å¤±è´¥: {e}")
            self.logger.info("å°è¯•å›é€€åˆ°MySQLæ¨¡å¼...")
            try:
                nodes = await self._load_nodes_from_mysql(limit)
            except Exception as mysql_error:
                self.logger.error(f"MySQLå›é€€ä¹Ÿå¤±è´¥: {mysql_error}")
                nodes = []
        
        return nodes

    async def _load_pagerank_mapping(self) -> Dict[str, float]:
        """ä»MySQLåŠ è½½PageRankåˆ†æ•°æ˜ å°„"""
        try:
            # é¦–å…ˆå°è¯•ä»website_nkuè¡¨è·å–ï¼ˆå·²æ•´åˆçš„PageRankåˆ†æ•°ï¼‰
            query = """
            SELECT original_url, pagerank_score 
            FROM website_nku 
            WHERE pagerank_score > 0
            """
            records = await db_core.execute_query(query, fetch=True)
            
            if records:
                mapping = {record['original_url']: float(record['pagerank_score']) for record in records}
                self.logger.info(f"ä»website_nkuè¡¨åŠ è½½äº† {len(mapping)} ä¸ªPageRankåˆ†æ•°")
                return mapping
            
            # å¦‚æœwebsite_nkuè¡¨æ²¡æœ‰æ•°æ®ï¼Œå°è¯•ä»pagerank_scoresè¡¨è·å–
            query = """
            SELECT url, pagerank_score 
            FROM pagerank_scores
            """
            records = await db_core.execute_query(query, fetch=True)
            
            if records:
                mapping = {record['url']: float(record['pagerank_score']) for record in records}
                self.logger.info(f"ä»pagerank_scoresè¡¨åŠ è½½äº† {len(mapping)} ä¸ªPageRankåˆ†æ•°")
                return mapping
            
            self.logger.warning("ä¸¤ä¸ªè¡¨ä¸­éƒ½æ²¡æœ‰æ‰¾åˆ°PageRankæ•°æ®")
            return {}
            
        except Exception as e:
            self.logger.warning(f"åŠ è½½PageRankåˆ†æ•°æ—¶å‡ºé”™: {e}")
            return {}


# å‘åå…¼å®¹çš„å‡½æ•°æ¥å£
def build_bm25_index(
    output_path: str = None,
    stopwords_path: str = None,
    bm25_type: int = 0,
    limit: int = None,
    data_source: str = "raw_files"
) -> Dict[str, Any]:
    """
    æ„å»ºBM25ç´¢å¼•ï¼ˆå‘åå…¼å®¹æ¥å£ï¼‰
    
    Args:
        output_path: ç´¢å¼•è¾“å‡ºè·¯å¾„
        stopwords_path: åœç”¨è¯æ–‡ä»¶è·¯å¾„
        bm25_type: BM25ç®—æ³•ç±»å‹ (0: BM25Okapi, 1: BM25)
        limit: é™åˆ¶å¤„ç†çš„è®°å½•æ•°é‡ï¼ŒNoneè¡¨ç¤ºå¤„ç†æ‰€æœ‰
        data_source: æ•°æ®æºç±»å‹ ("raw_files"=æ··åˆæ¨¡å¼, "mysql"=ä»…MySQL, "raw_only"=ä»…åŸå§‹æ–‡ä»¶)
        
    Returns:
        æ„å»ºç»“æœç»Ÿè®¡
    """
    config = Config()
    
    # è®¾ç½®é…ç½®å‚æ•°
    indexer_config = {
        'etl.retrieval.bm25.nodes_path': output_path or config.get('etl.retrieval.bm25.nodes_path'),
        'etl.retrieval.bm25.stopwords_path': stopwords_path or config.get('etl.retrieval.bm25.stopwords_path')
    }
    
    # åˆ›å»ºç´¢å¼•æ„å»ºå™¨
    indexer = BM25Indexer(indexer_config)
    
    # æ„å»ºç´¢å¼•
    return asyncio.run(indexer.build_indexes(limit=limit, bm25_type=bm25_type, data_source=data_source)) 