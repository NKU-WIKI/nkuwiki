"""
Qdrantå‘é‡ç´¢å¼•æ„å»ºå™¨

è´Ÿè´£ä»MySQLæ•°æ®æ„å»ºQdrantå‘é‡æ£€ç´¢ç´¢å¼•ã€‚
"""

import os
import sys
import logging
import asyncio
import json
from pathlib import Path
from typing import List, Dict, Any, Optional
import aiofiles
import aiofiles.os
from tqdm.asyncio import tqdm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).resolve().parent.parent.parent))
from config import Config
from etl.load import db_core
from etl.embedding.hf_embeddings import HuggingFaceEmbedding
from llama_index.core.schema import BaseNode, TextNode
from llama_index.core import VectorStoreIndex, StorageContext
from llama_index.vector_stores.qdrant import QdrantVectorStore
from qdrant_client import QdrantClient, models, AsyncQdrantClient
# å¯¼å…¥ETLæ¨¡å—çš„ç»Ÿä¸€è·¯å¾„é…ç½®
from etl import QDRANT_URL, COLLECTION_NAME, VECTOR_SIZE, MODELS_PATH, RAW_PATH

logger = logging.getLogger(__name__)


class QdrantIndexer:
    """Qdrantå‘é‡ç´¢å¼•æ„å»ºå™¨
    
    è´Ÿè´£ä»MySQLæ•°æ®æ„å»ºQdrantå‘é‡æ£€ç´¢ç´¢å¼•ï¼Œæ”¯æŒè¯­ä¹‰åµŒå…¥å’Œæ–‡æœ¬åˆ†å—ã€‚
    """
    
    def __init__(self, config: Dict[str, Any], logger: Optional[logging.Logger] = None):
        self.config = config
        self.logger = logger or logging.getLogger(__name__)
        
        # ä½¿ç”¨ETLæ¨¡å—ç»Ÿä¸€é…ç½®çš„å‚æ•°
        self.collection_name = config.get('etl.data.qdrant.collection', COLLECTION_NAME)
        self.qdrant_url = config.get('etl.data.qdrant.url', QDRANT_URL)
        self.embedding_model = config.get('etl.embedding.name', 'BAAI/bge-large-zh-v1.5')
        self.vector_size = config.get('etl.data.qdrant.vector_size', VECTOR_SIZE)
        self.chunk_size = config.get('etl.chunking.chunk_size', 512)
        self.chunk_overlap = config.get('etl.chunking.chunk_overlap', 200)
        
    async def build_indexes(self, 
                     limit: int = None, 
                     batch_size: int = 100,
                     test_mode: bool = False,
                     data_source: str = "raw_files",
                     start_batch: int = 0,
                     max_batches: int = None,
                     incremental: bool = False) -> Dict[str, Any]:
        """
        æ„å»ºQdrantå‘é‡ç´¢å¼•
        
        Args:
            limit: é™åˆ¶å¤„ç†çš„è®°å½•æ•°é‡ï¼ŒNoneè¡¨ç¤ºå¤„ç†æ‰€æœ‰
            batch_size: æ‰¹å¤„ç†å¤§å°
            test_mode: æµ‹è¯•æ¨¡å¼ï¼Œä¸å®é™…åˆ›å»ºç´¢å¼•
            data_source: æ•°æ®æºç±»å‹ ("raw_files"=æ··åˆæ¨¡å¼, "mysql"=ä»…MySQL, "raw_only"=ä»…åŸå§‹æ–‡ä»¶)
            start_batch: ä»ç¬¬å‡ æ‰¹å¼€å§‹å¤„ç†ï¼ˆåˆ†æ‰¹æ„å»ºï¼‰
            max_batches: æœ€å¤§æ‰¹æ¬¡æ•°ï¼ŒNoneè¡¨ç¤ºå¤„ç†æ‰€æœ‰æ‰¹æ¬¡
            incremental: æ˜¯å¦å¢é‡æ„å»ºï¼ˆä¸åˆ é™¤ç°æœ‰é›†åˆï¼‰
            
        Returns:
            æ„å»ºç»“æœç»Ÿè®¡
        """
        self.logger.info(f"å¼€å§‹æ„å»ºQdrantå‘é‡ç´¢å¼•ï¼Œé›†åˆ: {self.collection_name}")
        
        try:
            # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
            embed_model = await self._init_embedding_model()
            if not embed_model:
                return {"success": False, "error": "åµŒå…¥æ¨¡å‹åˆå§‹åŒ–å¤±è´¥", "message": "åµŒå…¥æ¨¡å‹åˆå§‹åŒ–å¤±è´¥"}
            
            # åˆå§‹åŒ–å¼‚æ­¥Qdrantå®¢æˆ·ç«¯
            qdrant_client = AsyncQdrantClient(url=self.qdrant_url)
            
            # åˆ›å»ºæˆ–é‡å»ºé›†åˆ
            if not test_mode:
                await self._setup_collection(qdrant_client, incremental=incremental)
            
            # æ ¹æ®æ•°æ®æºç±»å‹åŠ è½½æ•°æ®
            if data_source == "raw_files":
                print("ğŸ“ æ··åˆæ¨¡å¼ï¼šä»åŸå§‹æ–‡ä»¶+PageRankæ•°æ®...")
                nodes = await self._load_nodes_hybrid(limit)
                self.logger.info(f"æ··åˆæ¨¡å¼åŠ è½½äº† {len(nodes)} ä¸ªèŠ‚ç‚¹")
            elif data_source == "mysql":
                print("ğŸ“Š ä»MySQLæ•°æ®åº“åŠ è½½æ•°æ®...")
                nodes = await self._load_and_chunk_nodes_from_mysql(limit)
                self.logger.info(f"ä»MySQLåŠ è½½äº† {len(nodes)} ä¸ªèŠ‚ç‚¹")
            elif data_source == "raw_only":
                print("ğŸ“ ä»…ä»åŸå§‹JSONæ–‡ä»¶åŠ è½½æ•°æ®...")
                nodes = await self._load_and_chunk_nodes_from_raw_files(limit)
                self.logger.info(f"ä»åŸå§‹æ–‡ä»¶åŠ è½½äº† {len(nodes)} ä¸ªèŠ‚ç‚¹")
            else:
                print("ğŸ“ é»˜è®¤æ··åˆæ¨¡å¼ï¼šä»åŸå§‹æ–‡ä»¶+PageRankæ•°æ®...")
                nodes = await self._load_nodes_hybrid(limit)
                self.logger.info(f"æ··åˆæ¨¡å¼åŠ è½½äº† {len(nodes)} ä¸ªèŠ‚ç‚¹")
            
            if not nodes:
                self.logger.warning("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•èŠ‚ç‚¹æ•°æ®")
                return {"total_nodes": 0, "success": False, "message": "æ²¡æœ‰æ‰¾åˆ°æ•°æ®"}
            
            # æ„å»ºå‘é‡ç´¢å¼•
            if not test_mode:
                vector_store = QdrantVectorStore(
                    aclient=qdrant_client,
                    collection_name=self.collection_name
                )
                
                # ä¸ºèŠ‚ç‚¹ç”ŸæˆåµŒå…¥ï¼ˆå¸¦è¿›åº¦æ¡ï¼‰
                print("ğŸ”® ç”Ÿæˆå‘é‡åµŒå…¥...")
                nodes = await self._generate_embeddings_with_progress(embed_model, nodes, batch_size)

                # å¼‚æ­¥æ‰¹é‡æ·»åŠ èŠ‚ç‚¹åˆ°Qdrantï¼ˆå¸¦è¿›åº¦æ¡ï¼‰
                print("ğŸ“¤ ä¸Šä¼ å‘é‡åˆ°Qdrant...")
                await self._upload_to_qdrant_with_progress(vector_store, nodes, batch_size)
                
                self.logger.info(f"Qdrantå‘é‡ç´¢å¼•æ„å»ºå®Œæˆï¼Œé›†åˆ: {self.collection_name}")
            else:
                self.logger.info("æµ‹è¯•æ¨¡å¼ï¼šè·³è¿‡ç´¢å¼•æ„å»º")
            
            print("âœ… Qdrantå‘é‡ç´¢å¼•æ„å»ºå®Œæˆ!")
            return {
                "total_nodes": len(nodes),
                "success": True,
                "collection_name": self.collection_name,
                "vector_size": self.vector_size,
                "embedding_model": self.embedding_model,
                "data_source": data_source,
                "message": f"æˆåŠŸæ„å»ºQdrantç´¢å¼•ï¼ŒåŒ…å« {len(nodes)} ä¸ªå‘é‡"
            }
            
        except Exception as e:
            print(f"âŒ æ„å»ºå¤±è´¥: {e}")
            self.logger.error(f"æ„å»ºQdrantç´¢å¼•æ—¶å‡ºé”™: {e}")
            return {
                "total_nodes": 0, 
                "success": False, 
                "error": str(e),
                "message": f"Qdrantç´¢å¼•æ„å»ºå¤±è´¥: {str(e)}"
            }

    async def _generate_embeddings_with_progress(self, embed_model, nodes: List[BaseNode], batch_size: int) -> List[BaseNode]:
        """ä¸ºèŠ‚ç‚¹ç”ŸæˆåµŒå…¥å¹¶æ˜¾ç¤ºè¿›åº¦ï¼ˆä¼˜åŒ–å†…å­˜ç®¡ç†ï¼‰"""
        # å¤„ç†batch_size=-1çš„æƒ…å†µï¼Œè¡¨ç¤ºä¸åˆ†æ‰¹ï¼Œä¸€æ¬¡æ€§å¤„ç†
        if batch_size == -1:
            batch_size = len(nodes)
        
        total_batches = (len(nodes) + batch_size - 1) // batch_size
        
        self.logger.info(f"å¼€å§‹ç”ŸæˆåµŒå…¥: {len(nodes)} ä¸ªèŠ‚ç‚¹, æ‰¹æ¬¡å¤§å°: {batch_size}, æ€»æ‰¹æ¬¡: {total_batches}")
        
        # å¯¹äºå¤§æ•°æ®é›†ï¼Œåœ¨ç”ŸæˆåµŒå…¥çš„åŒæ—¶è¿›è¡Œåƒåœ¾å›æ”¶ä»¥é‡Šæ”¾å†…å­˜
        import gc
        
        with tqdm(total=len(nodes), desc="ç”Ÿæˆå‘é‡åµŒå…¥", unit="èŠ‚ç‚¹") as pbar:
            for i in range(0, len(nodes), batch_size):
                batch = nodes[i:i + batch_size]
                batch_num = i//batch_size + 1
                
                self.logger.debug(f"å¤„ç†æ‰¹æ¬¡ {batch_num}/{total_batches}, èŠ‚ç‚¹æ•°: {len(batch)}")
                
                try:
                    # ä¸ºæ‰¹æ¬¡èŠ‚ç‚¹ç”ŸæˆåµŒå…¥
                    batch_with_embeddings = await embed_model.acall(batch)
                    # æ›´æ–°åŸå§‹nodesåˆ—è¡¨
                    nodes[i:i + len(batch_with_embeddings)] = batch_with_embeddings
                    pbar.update(len(batch))
                    pbar.set_postfix({'æ‰¹æ¬¡': f"{batch_num}/{total_batches}"})
                    
                    self.logger.debug(f"æ‰¹æ¬¡ {batch_num} å®Œæˆ, å¤„ç†äº† {len(batch)} ä¸ªèŠ‚ç‚¹")
                    
                    # æ¯100æ‰¹æ‰§è¡Œä¸€æ¬¡åƒåœ¾å›æ”¶é‡Šæ”¾å†…å­˜
                    if batch_num % 100 == 0:
                        gc.collect()
                        self.logger.debug(f"æ‰¹æ¬¡ {batch_num} åæ‰§è¡Œåƒåœ¾å›æ”¶")
                    
                except Exception as e:
                    self.logger.warning(f"æ‰¹æ¬¡ {batch_num} åµŒå…¥ç”Ÿæˆå¤±è´¥: {e}")
                    pbar.update(len(batch))
        
        self.logger.info(f"åµŒå…¥ç”Ÿæˆå®Œæˆï¼Œæ€»è®¡å¤„ç† {len(nodes)} ä¸ªèŠ‚ç‚¹")
        return nodes

    async def _upload_to_qdrant_with_progress(self, vector_store, nodes: List[BaseNode], batch_size: int):
        """å°†èŠ‚ç‚¹ä¸Šä¼ åˆ°Qdrantå¹¶æ˜¾ç¤ºè¿›åº¦ï¼ˆä¼˜åŒ–å†…å­˜ç®¡ç†å’Œé”™è¯¯å¤„ç†ï¼‰"""
        # å¤„ç†batch_size=-1çš„æƒ…å†µï¼Œè¡¨ç¤ºä¸åˆ†æ‰¹ï¼Œä¸€æ¬¡æ€§å¤„ç†
        if batch_size == -1:
            batch_size = len(nodes)
        
        total_batches = (len(nodes) + batch_size - 1) // batch_size
        
        self.logger.info(f"å¼€å§‹ä¸Šä¼ åˆ°Qdrant: {len(nodes)} ä¸ªèŠ‚ç‚¹, æ‰¹æ¬¡å¤§å°: {batch_size}, æ€»æ‰¹æ¬¡: {total_batches}")
        
        import gc
        successful_uploads = 0
        failed_uploads = 0
        
        with tqdm(total=len(nodes), desc="ä¸Šä¼ åˆ°Qdrant", unit="èŠ‚ç‚¹") as pbar:
            for i in range(0, len(nodes), batch_size):
                batch = nodes[i:i + batch_size]
                batch_num = i//batch_size + 1
                
                self.logger.debug(f"ä¸Šä¼ æ‰¹æ¬¡ {batch_num}/{total_batches}, èŠ‚ç‚¹æ•°: {len(batch)}")
                
                try:
                    await vector_store.async_add(nodes=batch)
                    successful_uploads += len(batch)
                    pbar.update(len(batch))
                    pbar.set_postfix({
                        'æ‰¹æ¬¡': f"{batch_num}/{total_batches}", 
                        'æˆåŠŸ': successful_uploads,
                        'å¤±è´¥': failed_uploads
                    })
                    
                    self.logger.debug(f"æ‰¹æ¬¡ {batch_num} ä¸Šä¼ å®Œæˆ, ä¸Šä¼ äº† {len(batch)} ä¸ªèŠ‚ç‚¹")
                    
                    # æ¯50æ‰¹æ‰§è¡Œä¸€æ¬¡åƒåœ¾å›æ”¶
                    if batch_num % 50 == 0:
                        gc.collect()
                        self.logger.debug(f"æ‰¹æ¬¡ {batch_num} åæ‰§è¡Œåƒåœ¾å›æ”¶")
                    
                except Exception as e:
                    failed_uploads += len(batch)
                    self.logger.warning(f"æ‰¹æ¬¡ {batch_num} ä¸Šä¼ å¤±è´¥: {e}")
                    pbar.update(len(batch))
                    pbar.set_postfix({
                        'æ‰¹æ¬¡': f"{batch_num}/{total_batches}", 
                        'æˆåŠŸ': successful_uploads,
                        'å¤±è´¥': failed_uploads
                    })
        
        self.logger.info(f"Qdrantä¸Šä¼ å®Œæˆï¼ŒæˆåŠŸä¸Šä¼  {successful_uploads} ä¸ªèŠ‚ç‚¹ï¼Œå¤±è´¥ {failed_uploads} ä¸ªèŠ‚ç‚¹")

    async def _init_embedding_model(self) -> Optional[HuggingFaceEmbedding]:
        """å¼‚æ­¥åˆå§‹åŒ–åµŒå…¥æ¨¡å‹"""
        def _load_model():
            """åœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œçš„åŒæ­¥åŠ è½½å‡½æ•°"""
            try:
                # ä½¿ç”¨ETLæ¨¡å—ç»Ÿä¸€é…ç½®çš„æ¨¡å‹è·¯å¾„
                models_path = str(MODELS_PATH)
                
                # è®¾ç½®HuggingFaceç¼“å­˜ç›®å½•
                os.environ['HF_HOME'] = models_path
                os.environ['TRANSFORMERS_CACHE'] = models_path
                os.environ['HF_HUB_CACHE'] = models_path
                os.environ['SENTENCE_TRANSFORMERS_HOME'] = models_path
                
                self.logger.info(f"åˆå§‹åŒ–åµŒå…¥æ¨¡å‹: {self.embedding_model}")
                embed_model = HuggingFaceEmbedding(
                    model_name=self.embedding_model,
                    device='cpu'  # å¼ºåˆ¶ä½¿ç”¨CPU
                )
                
                self.logger.info("åµŒå…¥æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
                return embed_model
                
            except Exception as e:
                self.logger.error(f"åˆå§‹åŒ–åµŒå…¥æ¨¡å‹å¤±è´¥: {e}")
                return None
        
        try:
            print("ğŸ¤– åˆå§‹åŒ–åµŒå…¥æ¨¡å‹...")
            loop = asyncio.get_running_loop()
            return await loop.run_in_executor(None, _load_model)
        except Exception as e:
            self.logger.error(f"æ‰§è¡ŒåµŒå…¥æ¨¡å‹åˆå§‹åŒ–æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return None

    async def _setup_collection(self, client: AsyncQdrantClient, incremental: bool = False):
        """è®¾ç½®Qdranté›†åˆ
        
        Args:
            client: Qdrantå¼‚æ­¥å®¢æˆ·ç«¯
            incremental: æ˜¯å¦å¢é‡æ„å»ºï¼ˆä¸åˆ é™¤ç°æœ‰é›†åˆï¼‰
        """
        try:
            print("ğŸ—‚ï¸  è®¾ç½®Qdranté›†åˆ...")
            # æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨
            collections = await client.get_collections()
            existing_collections = [col.name for col in collections.collections]
            
            if self.collection_name in existing_collections:
                if incremental:
                    self.logger.info(f"å¢é‡æ¨¡å¼: ä¿ç•™ç°æœ‰é›†åˆ {self.collection_name}")
                    # è·å–ç°æœ‰é›†åˆä¿¡æ¯
                    collection_info = await client.get_collection(self.collection_name)
                    current_count = collection_info.points_count
                    self.logger.info(f"ç°æœ‰é›†åˆåŒ…å« {current_count} ä¸ªå‘é‡")
                    print(f"ğŸ“Š ç°æœ‰é›†åˆåŒ…å« {current_count} ä¸ªå‘é‡ï¼Œå°†è¿›è¡Œå¢é‡æ›´æ–°")
                    return current_count
                else:
                    self.logger.info(f"å®Œå…¨é‡å»ºæ¨¡å¼: åˆ é™¤ç°æœ‰é›†åˆ {self.collection_name}")
                    await client.delete_collection(self.collection_name)
            
            # åˆ›å»ºæ–°é›†åˆï¼ˆä»…åœ¨éå¢é‡æ¨¡å¼æˆ–é›†åˆä¸å­˜åœ¨æ—¶ï¼‰
            if not incremental or self.collection_name not in existing_collections:
                self.logger.info(f"åˆ›å»ºæ–°é›†åˆ: {self.collection_name}, å‘é‡ç»´åº¦: {self.vector_size}")
                await client.create_collection(
                    collection_name=self.collection_name,
                    vectors_config=models.VectorParams(
                        size=self.vector_size,
                        distance=models.Distance.COSINE
                    )
                )
                return 0  # æ–°é›†åˆåˆå§‹å‘é‡æ•°ä¸º0
            
        except Exception as e:
            self.logger.error(f"è®¾ç½®Qdranté›†åˆæ—¶å‡ºé”™: {e}")
            raise

    async def _load_nodes_from_raw_files(self, limit: int = None) -> List[BaseNode]:
        """ä»åŸå§‹JSONæ–‡ä»¶åŠ è½½èŠ‚ç‚¹æ•°æ®ï¼ˆæ¨èæ–¹æ³•ï¼‰"""
        nodes = []
        
        try:
            # å®šä¹‰æ•°æ®æºé…ç½®
            data_sources = [
                {
                    'name': 'website',
                    'path': RAW_PATH / 'website' / 'nku',
                    'platform': 'website'
                },
                {
                    'name': 'wechat', 
                    'path': RAW_PATH / 'wechat' / 'nku',
                    'platform': 'wechat'
                },
                {
                    'name': 'market',
                    'path': RAW_PATH / 'market' / 'nku', 
                    'platform': 'market'
                },
                {
                    'name': 'wxapp',
                    'path': RAW_PATH / 'wxapp',
                    'platform': 'wxapp'
                }
            ]
            
            # ç»Ÿè®¡æ€»æ–‡ä»¶æ•°
            total_files = 0
            source_file_counts = {}
            
            for source in data_sources:
                if source['path'].exists():
                    json_files = list(source['path'].rglob('*.json'))
                    # è¿‡æ»¤æ‰ç‰¹æ®Šæ–‡ä»¶
                    json_files = [f for f in json_files if not f.name.startswith(('scraped_', 'counter.', 'lock.'))]
                    source_file_counts[source['name']] = len(json_files)
                    total_files += len(json_files)
                else:
                    source_file_counts[source['name']] = 0
                    self.logger.warning(f"æ•°æ®æºè·¯å¾„ä¸å­˜åœ¨: {source['path']}")
            
            if limit:
                total_files = min(total_files, limit)
            
            self.logger.info(f"é¢„è®¡å¤„ç† {total_files} ä¸ªJSONæ–‡ä»¶")
            for name, count in source_file_counts.items():
                if count > 0:
                    self.logger.info(f"  {name}: {count} ä¸ªæ–‡ä»¶")
            
            processed_count = 0
            
            # ä½¿ç”¨æ€»ä½“è¿›åº¦æ¡
            with tqdm(total=total_files, desc="åŠ è½½åŸå§‹æ•°æ®", unit="æ–‡ä»¶") as pbar:
                for source in data_sources:
                    if not source['path'].exists():
                        continue
                        
                    # è·å–è¯¥æ•°æ®æºçš„JSONæ–‡ä»¶
                    json_files = list(source['path'].rglob('*.json'))
                    json_files = [f for f in json_files if not f.name.startswith(('scraped_', 'counter.', 'lock.'))]
                    
                    if not json_files:
                        self.logger.info(f"{source['name']} æ²¡æœ‰æ‰¾åˆ°JSONæ–‡ä»¶")
                        continue
                    
                    self.logger.info(f"å¤„ç† {source['name']} æ•°æ®æº: {len(json_files)} ä¸ªæ–‡ä»¶")
                    
                    # å¤„ç†è¯¥æ•°æ®æºçš„æ–‡ä»¶
                    source_desc = f"å¤„ç†{source['name']}"
                    for json_file in tqdm(json_files, desc=source_desc, unit="æ–‡ä»¶", leave=False):
                        if limit and processed_count >= limit:
                            break
                            
                        try:
                            # è¯»å–JSONæ–‡ä»¶
                            async with aiofiles.open(json_file, 'r', encoding='utf-8') as f:
                                content_str = await f.read()
                            
                            if not content_str.strip():
                                continue
                            
                            data = json.loads(content_str)
                            
                            # å¤„ç†ä¸åŒçš„æ•°æ®æ ¼å¼
                            if isinstance(data, list):
                                records = data
                            elif isinstance(data, dict) and 'data' in data:
                                records = data['data'] if isinstance(data['data'], list) else [data['data']]
                            else:
                                records = [data]
                            
                            # è½¬æ¢æ¯æ¡è®°å½•ä¸ºèŠ‚ç‚¹
                            for record in records:
                                try:
                                    node = await self._create_node_from_record(record, source['platform'], json_file)
                                    if node:
                                        nodes.append(node)
                                        
                                except Exception as e:
                                    self.logger.warning(f"å¤„ç†è®°å½•æ—¶å‡ºé”™ {json_file}: {e}")
                                    continue
                            
                            processed_count += 1
                            pbar.update(1)
                            
                        except Exception as e:
                            self.logger.warning(f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™ {json_file}: {e}")
                            pbar.update(1)
                            continue
                    
                    if limit and processed_count >= limit:
                        break
            
            self.logger.info(f"æ€»è®¡ä»åŸå§‹æ–‡ä»¶åŠ è½½ {len(nodes)} ä¸ªèŠ‚ç‚¹")
            
        except Exception as e:
            self.logger.error(f"ä»åŸå§‹æ–‡ä»¶åŠ è½½æ•°æ®æ—¶å‡ºé”™: {e}")
        
        return nodes

    async def _create_node_from_record(self, record: Dict[str, Any], platform: str, source_file: Path) -> Optional[TextNode]:
        """ä»è®°å½•åˆ›å»ºTextNode"""
        try:
            # åŸºæœ¬å­—æ®µæå–
            content = record.get('content', '')
            title = record.get('title', record.get('name', ''))
            
            # å†…å®¹éªŒè¯
            if not content or not content.strip():
                return None
            
            if not title:
                title = f"æ–‡æ¡£_{record.get('id', 'unknown')}"
            
            # åˆå¹¶æ ‡é¢˜å’Œå†…å®¹
            full_text = f"{title}\n{content}" if title else content
            
            # æå–å…¶ä»–å…ƒæ•°æ®
            author = record.get('author', record.get('nickname', ''))
            url = record.get('original_url', record.get('url', record.get('link', '')))
            publish_time = record.get('publish_time', record.get('create_time', record.get('time', '')))
            
            # ç”Ÿæˆå”¯ä¸€ID
            record_id = record.get('id', str(abs(hash(url + title)) % 1000000))
            source_id = f"{platform}_{record_id}"
            
            # æ„é€ URLï¼ˆå¦‚æœæ²¡æœ‰ï¼‰
            if not url:
                if platform == 'wxapp':
                    url = f"wxapp://post/{record_id}"
                else:
                    url = f"{platform}://item/{record_id}"
            
            # åˆ›å»ºTextNode
            node = TextNode(
                text=full_text,
                metadata={
                    'source_id': source_id,
                    'id': record_id,
                    'url': url,
                    'title': title,
                    'author': author,
                    'original_url': url,
                    'publish_time': str(publish_time),
                    'source': platform,
                    'platform': platform,
                    'pagerank_score': float(record.get('pagerank_score', 0.0)),
                    'source_file': str(source_file),
                    'data_source': 'raw_files'
                }
            )
            
            return node
            
        except Exception as e:
            self.logger.warning(f"åˆ›å»ºèŠ‚ç‚¹æ—¶å‡ºé”™: {e}")
            return None

    async def _load_and_chunk_nodes_from_raw_files(self, limit: int = None) -> List[BaseNode]:
        """ä»åŸå§‹æ–‡ä»¶åŠ è½½æ•°æ®å¹¶è¿›è¡Œæ–‡æœ¬åˆ†å—"""
        # é¦–å…ˆåŠ è½½åŸå§‹èŠ‚ç‚¹
        raw_nodes = await self._load_nodes_from_raw_files(limit)
        
        if not raw_nodes:
            return []
        
        # è¿›è¡Œæ–‡æœ¬åˆ†å—
        return await self._chunk_nodes_with_progress(raw_nodes)

    async def _load_and_chunk_nodes_from_mysql(self, limit: int = None) -> List[BaseNode]:
        """ä»MySQLåŠ è½½æ•°æ®å¹¶è¿›è¡Œæ–‡æœ¬åˆ†å—"""
        nodes = []
        
        try:
            # æ„å»ºSQLæŸ¥è¯¢
            sql = """
            SELECT id, original_url, title, content, publish_time, platform, pagerank_score, author
            FROM website_nku
            WHERE content IS NOT NULL AND content != ''
            ORDER BY id
            """
            
            if limit:
                sql += f" LIMIT {limit}"
            
            # æ‰§è¡ŒæŸ¥è¯¢
            records = await db_core.execute_query(sql, fetch=True)
            
            if not records:
                self.logger.warning("MySQLä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•è®°å½•")
                return nodes
            
            # å¤„ç†æ¯æ¡è®°å½•ï¼Œåˆ›å»ºæ–‡æ¡£èŠ‚ç‚¹
            doc_nodes = []
            for record in tqdm(records, desc="å¤„ç†MySQLè®°å½•", unit="æ¡"):
                try:
                    # æ„å»ºæ–‡æ¡£å†…å®¹
                    content = record.get('content', '')
                    title = record.get('title', '')
                    
                    # åˆå¹¶æ ‡é¢˜å’Œå†…å®¹
                    full_text = f"{title}\n{content}" if title else content
                    
                    # åˆ›å»ºæ–‡æ¡£èŠ‚ç‚¹
                    doc_node = TextNode(
                        text=full_text,
                                        metadata={
                    'source_id': record.get('id'),
                    'id': record.get('id'),
                    'url': record.get('original_url', ''),
                    'title': title,
                    'author': record.get('author', ''),
                    'original_url': record.get('original_url', ''),
                    'publish_time': str(record.get('publish_time', '')),
                    'source': record.get('platform', ''),
                    'platform': record.get('platform', ''),
                            'pagerank_score': float(record.get('pagerank_score', 0.0)),
                            'data_source': 'mysql'
                        }
                    )
                    
                    doc_nodes.append(doc_node)
                    
                except Exception as e:
                    self.logger.warning(f"å¤„ç†è®°å½•ID {record.get('id')} æ—¶å‡ºé”™: {e}")
                    continue
            
            # è¿›è¡Œæ–‡æœ¬åˆ†å—
            nodes = await self._chunk_nodes_with_progress(doc_nodes)
            
        except Exception as e:
            self.logger.error(f"ä»MySQLåŠ è½½æ•°æ®æ—¶å‡ºé”™: {e}")
        
        return nodes

    async def _chunk_nodes_with_progress(self, doc_nodes: List[BaseNode]) -> List[BaseNode]:
        """å¯¹æ–‡æ¡£èŠ‚ç‚¹è¿›è¡Œåˆ†å—å¹¶æ˜¾ç¤ºè¿›åº¦ï¼ˆä½¿ç”¨ç¼“å­˜ä¼˜åŒ–ï¼‰"""
        from etl.processors.chunk_cache import chunk_documents_cached
        
        self.logger.info(f"å¼€å§‹åˆ†å— {len(doc_nodes)} ä¸ªæ–‡æ¡£èŠ‚ç‚¹...")
        
        # ä½¿ç”¨ç¼“å­˜è¿›è¡Œåˆ†å—
        chunked_nodes = await chunk_documents_cached(
            doc_nodes=doc_nodes,
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            force_refresh=False,
            show_progress=True
        )
        
        self.logger.info(f"åˆ†å—å®Œæˆï¼Œæ€»è®¡ {len(chunked_nodes)} ä¸ªæ–‡æœ¬å—")
        return chunked_nodes

    async def _load_nodes_hybrid(self, limit: int = None) -> List[BaseNode]:
        """æ··åˆåŠ è½½æ–¹æ¡ˆï¼šä»åŸå§‹æ–‡ä»¶åŠ è½½åŸºç¡€æ•°æ®ï¼Œä»MySQLè¡¥å……PageRankåˆ†æ•°ï¼ˆæ¨èï¼‰"""
        try:
            # ç¬¬ä¸€æ­¥ï¼šä»åŸå§‹æ–‡ä»¶åŠ è½½å¹¶åˆ†å—
            self.logger.info("ğŸ“ ç¬¬ä¸€æ­¥ï¼šä»åŸå§‹JSONæ–‡ä»¶åŠ è½½å¹¶åˆ†å—æ•°æ®...")
            raw_nodes = await self._load_and_chunk_nodes_from_raw_files(limit)
            
            if not raw_nodes:
                self.logger.warning("åŸå§‹æ–‡ä»¶ä¸­æ²¡æœ‰æ•°æ®ï¼Œå›é€€åˆ°MySQLæ¨¡å¼")
                return await self._load_and_chunk_nodes_from_mysql(limit)
            
            # ç¬¬äºŒæ­¥ï¼šä»MySQLåŠ è½½PageRankåˆ†æ•°æ˜ å°„
            self.logger.info("ğŸ“Š ç¬¬äºŒæ­¥ï¼šä»MySQLåŠ è½½PageRankåˆ†æ•°...")
            pagerank_mapping = await self._load_pagerank_mapping()
            
            if pagerank_mapping:
                self.logger.info(f"æˆåŠŸåŠ è½½ {len(pagerank_mapping)} ä¸ªPageRankåˆ†æ•°")
                
                # ç¬¬ä¸‰æ­¥ï¼šä¸ºèŠ‚ç‚¹è¡¥å……PageRankåˆ†æ•°
                updated_count = 0
                with tqdm(raw_nodes, desc="è¡¥å……PageRankåˆ†æ•°", unit="èŠ‚ç‚¹") as pbar:
                    for node in pbar:
                        url = node.metadata.get('url', '')
                        if url in pagerank_mapping:
                            node.metadata['pagerank_score'] = float(pagerank_mapping[url])
                            updated_count += 1
                        pbar.set_postfix({'å·²æ›´æ–°': updated_count})
                
                self.logger.info(f"ä¸º {updated_count} ä¸ªèŠ‚ç‚¹è¡¥å……äº†PageRankåˆ†æ•°")
            else:
                self.logger.warning("æ²¡æœ‰æ‰¾åˆ°PageRankåˆ†æ•°ï¼Œæ‰€æœ‰èŠ‚ç‚¹å°†ä½¿ç”¨é»˜è®¤å€¼0.0")
            
            self.logger.info(f"æ··åˆåŠ è½½å®Œæˆï¼Œæ€»è®¡ {len(raw_nodes)} ä¸ªèŠ‚ç‚¹")
            return raw_nodes
            
        except Exception as e:
            self.logger.error(f"æ··åˆåŠ è½½å¤±è´¥: {e}")
            self.logger.info("å°è¯•å›é€€åˆ°MySQLæ¨¡å¼...")
            try:
                return await self._load_and_chunk_nodes_from_mysql(limit)
            except Exception as mysql_error:
                self.logger.error(f"MySQLå›é€€ä¹Ÿå¤±è´¥: {mysql_error}")
                return []

    async def _load_pagerank_mapping(self) -> Dict[str, float]:
        """ä»MySQLåŠ è½½PageRankåˆ†æ•°æ˜ å°„"""
        try:
            # é¦–å…ˆå°è¯•ä»website_nkuè¡¨è·å–ï¼ˆå·²æ•´åˆçš„PageRankåˆ†æ•°ï¼‰
            query = """
            SELECT original_url, pagerank_score 
            FROM website_nku 
            WHERE pagerank_score > 0
            """
            records = await db_core.execute_query(query, fetch=True)
            
            if records:
                mapping = {record['original_url']: float(record['pagerank_score']) for record in records}
                self.logger.info(f"ä»website_nkuè¡¨åŠ è½½äº† {len(mapping)} ä¸ªPageRankåˆ†æ•°")
                return mapping
            
            # å¦‚æœwebsite_nkuè¡¨æ²¡æœ‰æ•°æ®ï¼Œå°è¯•ä»pagerank_scoresè¡¨è·å–
            query = """
            SELECT url, pagerank_score 
            FROM pagerank_scores
            """
            records = await db_core.execute_query(query, fetch=True)
            
            if records:
                mapping = {record['url']: float(record['pagerank_score']) for record in records}
                self.logger.info(f"ä»pagerank_scoresè¡¨åŠ è½½äº† {len(mapping)} ä¸ªPageRankåˆ†æ•°")
                return mapping
            
            self.logger.warning("ä¸¤ä¸ªè¡¨ä¸­éƒ½æ²¡æœ‰æ‰¾åˆ°PageRankæ•°æ®")
            return {}
            
        except Exception as e:
            self.logger.warning(f"åŠ è½½PageRankåˆ†æ•°æ—¶å‡ºé”™: {e}")
            return {}

    async def validate_index(self) -> Dict[str, Any]:
        """å¼‚æ­¥éªŒè¯Qdrantç´¢å¼•æ˜¯å¦å­˜åœ¨ä¸”æœ‰æ•ˆ"""
        self.logger.info(f"å¼€å§‹éªŒè¯Qdrantç´¢å¼•: {self.collection_name}")
        client = AsyncQdrantClient(url=self.qdrant_url)
        
        try:
            # æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨
            collections_response = await client.get_collections()
            collection_names = [col.name for col in collections_response.collections]

            if self.collection_name not in collection_names:
                self.logger.warning(f"é›†åˆ '{self.collection_name}' ä¸å­˜åœ¨ã€‚")
                return {"status": "missing", "collection_name": self.collection_name}

            # è·å–é›†åˆä¿¡æ¯
            collection_info = await client.get_collection(collection_name=self.collection_name)
            vector_count = collection_info.vectors_count

            # è·å–ä¸€äº›ç¤ºä¾‹æ–‡æ¡£ (è¿™é‡Œç”¨ scroll æ¥å£)
            scroll_response = await client.scroll(
                collection_name=self.collection_name,
                limit=5,
                with_payload=True
            )
            sample_docs = [record.payload for record in scroll_response[0]]

            self.logger.info(f"é›†åˆ '{self.collection_name}' éªŒè¯æˆåŠŸï¼ŒåŒ…å« {vector_count} ä¸ªå‘é‡ã€‚")

            return {
                "status": "ok",
                "collection_name": self.collection_name,
                "vector_count": vector_count,
                "vector_size": collection_info.vectors_config.params.size,
                "distance_metric": collection_info.vectors_config.params.distance.name,
                "sample_documents": sample_docs,
            }
        except Exception as e:
            self.logger.error(f"éªŒè¯Qdrantç´¢å¼•æ—¶å‡ºé”™: {e}")
            return {"status": "error", "error_message": str(e)}
        finally:
            await client.close()


# å‘åå…¼å®¹çš„å‡½æ•°æ¥å£
async def build_qdrant_index(
    collection_name: str = None,
    embedding_model: str = None,
    qdrant_url: str = None,
    chunk_size: int = 512,
    chunk_overlap: int = 200,
    limit: int = None,
    batch_size: int = 100,
    data_source: str = "raw_files"
) -> Dict[str, Any]:
    """
    æ„å»ºQdrantå‘é‡ç´¢å¼•ï¼ˆå‘åå…¼å®¹æ¥å£ï¼‰
    
    Args:
        data_source: æ•°æ®æºç±»å‹ ("raw_files"=æ··åˆæ¨¡å¼, "mysql"=ä»…MySQL, "raw_only"=ä»…åŸå§‹æ–‡ä»¶)
    """
    config = Config()
    
    # è®¾ç½®é…ç½®å‚æ•°
    indexer_config = {
        'etl.data.qdrant.collection': collection_name or config.get('etl.data.qdrant.collection'),
        'etl.data.qdrant.url': qdrant_url or config.get('etl.data.qdrant.url'),
        'etl.embedding.name': embedding_model or config.get('etl.embedding.name'),
        'etl.embedding.chunking.chunk_size': chunk_size,
        'etl.embedding.chunking.chunk_overlap': chunk_overlap,
        'etl.data.base_path': config.get('etl.data.base_path'),
        'etl.data.models.path': config.get('etl.data.models.path'),
        'etl.data.qdrant.vector_size': config.get('etl.data.qdrant.vector_size')
    }
    
    # åˆ›å»ºç´¢å¼•æ„å»ºå™¨
    indexer = QdrantIndexer(indexer_config)
    
    # æ„å»ºç´¢å¼•
    return await indexer.build_indexes(limit=limit, batch_size=batch_size, data_source=data_source) 