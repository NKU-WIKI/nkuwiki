"""
Qdrantå‘é‡ç´¢å¼•æ„å»ºå™¨

è´Ÿè´£ä»MySQLæ•°æ®æ„å»ºQdrantå‘é‡æ£€ç´¢ç´¢å¼•ã€‚
"""

import os
import sys
import logging
import asyncio
import json
from pathlib import Path
from typing import List, Dict, Any, Optional
import aiofiles
import aiofiles.os
from tqdm.asyncio import tqdm
from llama_index.core.schema import BaseNode, TextNode
from llama_index.core.node_parser import JSONNodeParser
from llama_index.core.text_splitter import SentenceSplitter
from llama_index.core import VectorStoreIndex, StorageContext
from llama_index.vector_stores.qdrant import QdrantVectorStore
from qdrant_client import models, AsyncQdrantClient
from qdrant_client.http.models import UpdateStatus
from etl.embedding.hf_embeddings import HuggingFaceEmbedding
from config import Config
from core.utils.logger import register_logger

# æ–°å¢æ¯æ—¥æ•°æ®ä¸´æ—¶è·¯å¾„
DAILY_PATH = Path(__file__).resolve().parent.parent / "etl_temp" / "daily_data"

logger = logging.getLogger(__name__)


class QdrantIndexer:
    """Qdrantå‘é‡ç´¢å¼•æ„å»ºå™¨
    
    è´Ÿè´£ä»MySQLæ•°æ®æ„å»ºQdrantå‘é‡æ£€ç´¢ç´¢å¼•ï¼Œæ”¯æŒè¯­ä¹‰åµŒå…¥å’Œæ–‡æœ¬åˆ†å—ã€‚
    """
    
    def __init__(self, config: Config):
        self.logger = register_logger(f"{__name__}.{self.__class__.__name__}")
        self.config = config
        
        # ä»é…ç½®ä¸­è¯»å–Qdrantè®¾ç½®
        qdrant_url = config.get("etl.data.qdrant.url")
        qdrant_api_key = config.get("etl.data.qdrant.api_key")
        self.collection_name = config.get("etl.data.qdrant.collection", "main_index")
        
        # åˆå§‹åŒ–å¼‚æ­¥å®¢æˆ·ç«¯
        self.client = AsyncQdrantClient(url=qdrant_url, api_key=qdrant_api_key, prefer_grpc=True)
        
        # æ¨¡å‹å’Œè§£æå™¨è®¾ç½®
        self.embedding_model = config.get('etl.embedding.model', 'BAAI/bge-large-zh-v1.5')
        self.parser = JSONNodeParser()
        self.chunk_size = config.get('etl.chunking.chunk_size', 512)
        self.chunk_overlap = config.get('etl.chunking.chunk_overlap', 200)
        self.splitter = SentenceSplitter(chunk_size=self.chunk_size, chunk_overlap=self.chunk_overlap)

        # å°†åµŒå…¥æ¨¡å‹åˆå§‹åŒ–æ¨è¿Ÿåˆ°å®é™…éœ€è¦æ—¶
        self.embed_model = None

    async def _ensure_collection_exists(self):
        """ç¡®ä¿Qdranté›†åˆå­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º"""
        try:
            collections_response = await self.client.get_collections()
            existing_collections = {collection.name for collection in collections_response.collections}
            
            if self.collection_name in existing_collections:
                self.logger.info(f"é›†åˆ '{self.collection_name}' å·²å­˜åœ¨ã€‚")
            else:
                self.logger.info(f"é›†åˆ '{self.collection_name}' ä¸å­˜åœ¨ï¼Œæ­£åœ¨åˆ›å»º...")
                await self.client.create_collection(
                    collection_name=self.collection_name,
                    vectors_config=models.VectorParams(size=1024, distance=models.Distance.COSINE) # sizeéœ€è¦ä¸æ¨¡å‹åŒ¹é…
                )
                self.logger.info(f"é›†åˆ '{self.collection_name}' åˆ›å»ºæˆåŠŸã€‚")
        except Exception as e:
            self.logger.error(f"æ£€æŸ¥æˆ–åˆ›å»ºé›†åˆ '{self.collection_name}' æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            # æŠ›å‡ºå¼‚å¸¸ä»¥ä¸­æ–­åç»­æ“ä½œ
            raise

    async def build_indexes(self, 
                     limit: int = None, 
                     batch_size: int = 100,
                     test_mode: bool = False,
                     data_source: str = "raw_files",
                     start_batch: int = 0,
                     max_batches: int = None,
                     incremental: bool = False,
                     source_path: Optional[Path] = None) -> Dict[str, Any]:
        """
        æ„å»ºQdrantå‘é‡ç´¢å¼•
        
        Args:
            limit: é™åˆ¶å¤„ç†çš„è®°å½•æ•°é‡ï¼ŒNoneè¡¨ç¤ºå¤„ç†æ‰€æœ‰
            batch_size: æ‰¹å¤„ç†å¤§å°
            test_mode: æµ‹è¯•æ¨¡å¼ï¼Œä¸å®é™…åˆ›å»ºç´¢å¼•
            data_source: æ•°æ®æºç±»å‹ ("raw_files", "mysql", "raw_only", "custom_path")
            start_batch: ä»ç¬¬å‡ æ‰¹å¼€å§‹å¤„ç†ï¼ˆåˆ†æ‰¹æ„å»ºï¼‰
            max_batches: æœ€å¤§æ‰¹æ¬¡æ•°ï¼ŒNoneè¡¨ç¤ºå¤„ç†æ‰€æœ‰æ‰¹æ¬¡
            incremental: æ˜¯å¦å¢é‡æ„å»ºï¼ˆä¸åˆ é™¤ç°æœ‰é›†åˆï¼‰
            source_path: å½“ data_source ä¸º "custom_path" æ—¶ï¼ŒæŒ‡å®šè¦æ‰«æçš„ç›®å½•è·¯å¾„
            
        Returns:
            æ„å»ºç»“æœç»Ÿè®¡
        """
        self.logger.info(f"å¼€å§‹æ„å»ºQdrantå‘é‡ç´¢å¼•ï¼Œé›†åˆ: {self.collection_name}")
        
        try:
            # 1. ç¡®è®¤é›†åˆå­˜åœ¨
            await self._ensure_collection_exists()

            # 2. å¦‚æœæ˜¯æµ‹è¯•æ¨¡å¼ï¼ŒåªåŠ è½½èŠ‚ç‚¹ï¼Œä¸è¿›è¡Œç´¢å¼•
            if test_mode:
                self.logger.info("æµ‹è¯•æ¨¡å¼ï¼šè·³è¿‡ç´¢å¼•æ„å»º")
                nodes = await self._load_nodes_recursively(source_path)
                return {"success": True, "nodes": nodes}
            
            # 3. åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ (ä»…åœ¨éæµ‹è¯•æ¨¡å¼ä¸‹)
            if self.embed_model is None:
                self.embed_model = await self._init_embedding_model()
                if self.embed_model is None:
                    return {"success": False, "error": "åµŒå…¥æ¨¡å‹åˆå§‹åŒ–å¤±è´¥"}

            # 4. åŠ è½½å’Œå¤„ç†èŠ‚ç‚¹
            if data_source == "raw_files":
                print("ğŸ“ æ··åˆæ¨¡å¼ï¼šä»åŸå§‹æ–‡ä»¶+PageRankæ•°æ®...")
                nodes = await self._load_nodes_hybrid(limit)
                self.logger.info(f"æ··åˆæ¨¡å¼åŠ è½½äº† {len(nodes)} ä¸ªèŠ‚ç‚¹")
            elif data_source == "mysql":
                print("ğŸ“Š ä»MySQLæ•°æ®åº“åŠ è½½æ•°æ®...")
                nodes = await self._load_and_chunk_nodes_from_mysql(limit)
                self.logger.info(f"ä»MySQLåŠ è½½äº† {len(nodes)} ä¸ªèŠ‚ç‚¹")
            elif data_source == "raw_only":
                print("ğŸ“ ä»…ä»åŸå§‹JSONæ–‡ä»¶åŠ è½½æ•°æ®...")
                nodes = await self._load_and_chunk_nodes_from_raw_files(limit)
                self.logger.info(f"ä»åŸå§‹æ–‡ä»¶åŠ è½½äº† {len(nodes)} ä¸ªèŠ‚ç‚¹")
            elif data_source == "custom_path":
                if not source_path:
                    self.logger.error("ä½¿ç”¨ 'custom_path' æ•°æ®æºæ—¶å¿…é¡»æä¾› source_path")
                    return {"success": False, "message": "ç¼ºå°‘ source_path å‚æ•°"}
                print(f"ğŸ“‚ ä»è‡ªå®šä¹‰è·¯å¾„é€’å½’åŠ è½½æ•°æ®: {source_path}")
                nodes = await self._load_nodes_recursively(source_path, limit)
                self.logger.info(f"ä»è‡ªå®šä¹‰è·¯å¾„ {source_path} åŠ è½½äº† {len(nodes)} ä¸ªèŠ‚ç‚¹")
            elif data_source == "daily_files":
                print("ğŸ“… ä»æ¯æ—¥å¢é‡æ–‡ä»¶åŠ è½½æ•°æ®...")
                nodes = await self._load_nodes_from_daily_files(limit)
                self.logger.info(f"ä»æ¯æ—¥å¢é‡æ–‡ä»¶åŠ è½½äº† {len(nodes)} ä¸ªèŠ‚ç‚¹")
            else:
                print("ğŸ“ é»˜è®¤æ··åˆæ¨¡å¼ï¼šä»åŸå§‹æ–‡ä»¶+PageRankæ•°æ®...")
                nodes = await self._load_nodes_hybrid(limit)
                self.logger.info(f"æ··åˆæ¨¡å¼åŠ è½½äº† {len(nodes)} ä¸ªèŠ‚ç‚¹")
            
            if not nodes:
                self.logger.warning("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•èŠ‚ç‚¹æ•°æ®")
                return {"total_nodes": 0, "success": False, "message": "æ²¡æœ‰æ‰¾åˆ°æ•°æ®"}
            
            # æ„å»ºå‘é‡ç´¢å¼•
            if not test_mode:
                vector_store = QdrantVectorStore(
                    aclient=self.client,
                    collection_name=self.collection_name
                )
                
                # ä¸ºèŠ‚ç‚¹ç”ŸæˆåµŒå…¥ï¼ˆå¸¦è¿›åº¦æ¡ï¼‰
                print("ğŸ”® ç”Ÿæˆå‘é‡åµŒå…¥...")
                nodes = await self._generate_embeddings_with_progress(self.embed_model, nodes, batch_size)

                # å¼‚æ­¥æ‰¹é‡æ·»åŠ èŠ‚ç‚¹åˆ°Qdrantï¼ˆå¸¦è¿›åº¦æ¡ï¼‰
                print("ğŸ“¤ ä¸Šä¼ å‘é‡åˆ°Qdrant...")
                await self._upload_to_qdrant_with_progress(vector_store, nodes, batch_size)
                
                self.logger.info(f"Qdrantå‘é‡ç´¢å¼•æ„å»ºå®Œæˆï¼Œé›†åˆ: {self.collection_name}")
            else:
                self.logger.info("æµ‹è¯•æ¨¡å¼ï¼šè·³è¿‡ç´¢å¼•æ„å»º")
            
            print("âœ… Qdrantå‘é‡ç´¢å¼•æ„å»ºå®Œæˆ!")
            return {
                "nodes": nodes,
                "total_nodes": len(nodes),
                "success": True,
                "collection_name": self.collection_name,
                "vector_size": self.chunk_size,
                "embedding_model": self.embedding_model,
                "data_source": data_source,
                "message": f"æˆåŠŸæ„å»ºQdrantç´¢å¼•ï¼ŒåŒ…å« {len(nodes)} ä¸ªå‘é‡"
            }
            
        except Exception as e:
            print(f"âŒ æ„å»ºå¤±è´¥: {e}")
            self.logger.error(f"æ„å»ºQdrantç´¢å¼•æ—¶å‡ºé”™: {e}")
            return {
                "total_nodes": 0, 
                "success": False, 
                "error": str(e),
                "message": f"Qdrantç´¢å¼•æ„å»ºå¤±è´¥: {str(e)}"
            }

    async def _generate_embeddings_with_progress(self, embed_model, nodes: List[BaseNode], batch_size: int) -> List[BaseNode]:
        """ä¸ºèŠ‚ç‚¹ç”ŸæˆåµŒå…¥å¹¶æ˜¾ç¤ºè¿›åº¦ï¼ˆä¼˜åŒ–å†…å­˜ç®¡ç†ï¼‰"""
        # å¤„ç†batch_size=-1çš„æƒ…å†µï¼Œè¡¨ç¤ºä¸åˆ†æ‰¹ï¼Œä¸€æ¬¡æ€§å¤„ç†
        if batch_size == -1:
            batch_size = len(nodes)
        
        total_batches = (len(nodes) + batch_size - 1) // batch_size
        
        self.logger.info(f"å¼€å§‹ç”ŸæˆåµŒå…¥: {len(nodes)} ä¸ªèŠ‚ç‚¹, æ‰¹æ¬¡å¤§å°: {batch_size}, æ€»æ‰¹æ¬¡: {total_batches}")
        
        # å¯¹äºå¤§æ•°æ®é›†ï¼Œåœ¨ç”ŸæˆåµŒå…¥çš„åŒæ—¶è¿›è¡Œåƒåœ¾å›æ”¶ä»¥é‡Šæ”¾å†…å­˜
        import gc
        
        with tqdm(total=len(nodes), desc="ç”Ÿæˆå‘é‡åµŒå…¥", unit="èŠ‚ç‚¹") as pbar:
            for i in range(0, len(nodes), batch_size):
                batch = nodes[i:i + batch_size]
                batch_num = i//batch_size + 1
                
                self.logger.debug(f"å¤„ç†æ‰¹æ¬¡ {batch_num}/{total_batches}, èŠ‚ç‚¹æ•°: {len(batch)}")
                
                try:
                    # ä¸ºæ‰¹æ¬¡èŠ‚ç‚¹ç”ŸæˆåµŒå…¥
                    batch_with_embeddings = await embed_model.acall(batch)
                    # æ›´æ–°åŸå§‹nodesåˆ—è¡¨
                    nodes[i:i + len(batch_with_embeddings)] = batch_with_embeddings
                    pbar.update(len(batch))
                    pbar.set_postfix({'æ‰¹æ¬¡': f"{batch_num}/{total_batches}"})
                    
                    self.logger.debug(f"æ‰¹æ¬¡ {batch_num} å®Œæˆ, å¤„ç†äº† {len(batch)} ä¸ªèŠ‚ç‚¹")
                    
                    # æ¯100æ‰¹æ‰§è¡Œä¸€æ¬¡åƒåœ¾å›æ”¶é‡Šæ”¾å†…å­˜
                    if batch_num % 100 == 0:
                        gc.collect()
                        self.logger.debug(f"æ‰¹æ¬¡ {batch_num} åæ‰§è¡Œåƒåœ¾å›æ”¶")
                    
                except Exception as e:
                    self.logger.warning(f"æ‰¹æ¬¡ {batch_num} åµŒå…¥ç”Ÿæˆå¤±è´¥: {e}")
                    pbar.update(len(batch))
        
        self.logger.info(f"åµŒå…¥ç”Ÿæˆå®Œæˆï¼Œæ€»è®¡å¤„ç† {len(nodes)} ä¸ªèŠ‚ç‚¹")
        return nodes

    async def _upload_to_qdrant_with_progress(self, vector_store, nodes: List[BaseNode], batch_size: int):
        """å°†èŠ‚ç‚¹ä¸Šä¼ åˆ°Qdrantå¹¶æ˜¾ç¤ºè¿›åº¦ï¼ˆä¼˜åŒ–å†…å­˜ç®¡ç†å’Œé”™è¯¯å¤„ç†ï¼‰"""
        # å¤„ç†batch_size=-1çš„æƒ…å†µï¼Œè¡¨ç¤ºä¸åˆ†æ‰¹ï¼Œä¸€æ¬¡æ€§å¤„ç†
        if batch_size == -1:
            batch_size = len(nodes)
        
        total_batches = (len(nodes) + batch_size - 1) // batch_size
        
        self.logger.info(f"å¼€å§‹ä¸Šä¼ åˆ°Qdrant: {len(nodes)} ä¸ªèŠ‚ç‚¹, æ‰¹æ¬¡å¤§å°: {batch_size}, æ€»æ‰¹æ¬¡: {total_batches}")
        
        import gc
        successful_uploads = 0
        failed_uploads = 0
        
        with tqdm(total=len(nodes), desc="ä¸Šä¼ åˆ°Qdrant", unit="èŠ‚ç‚¹") as pbar:
            for i in range(0, len(nodes), batch_size):
                batch = nodes[i:i + batch_size]
                batch_num = i//batch_size + 1
                
                self.logger.debug(f"ä¸Šä¼ æ‰¹æ¬¡ {batch_num}/{total_batches}, èŠ‚ç‚¹æ•°: {len(batch)}")
                
                try:
                    await vector_store.async_add(nodes=batch)
                    successful_uploads += len(batch)
                    pbar.update(len(batch))
                    pbar.set_postfix({
                        'æ‰¹æ¬¡': f"{batch_num}/{total_batches}", 
                        'æˆåŠŸ': successful_uploads,
                        'å¤±è´¥': failed_uploads
                    })
                    
                    self.logger.debug(f"æ‰¹æ¬¡ {batch_num} ä¸Šä¼ å®Œæˆ, ä¸Šä¼ äº† {len(batch)} ä¸ªèŠ‚ç‚¹")
                    
                    # æ¯50æ‰¹æ‰§è¡Œä¸€æ¬¡åƒåœ¾å›æ”¶
                    if batch_num % 50 == 0:
                        gc.collect()
                        self.logger.debug(f"æ‰¹æ¬¡ {batch_num} åæ‰§è¡Œåƒåœ¾å›æ”¶")
                    
                except Exception as e:
                    failed_uploads += len(batch)
                    self.logger.warning(f"æ‰¹æ¬¡ {batch_num} ä¸Šä¼ å¤±è´¥: {e}")
                    pbar.update(len(batch))
                    pbar.set_postfix({
                        'æ‰¹æ¬¡': f"{batch_num}/{total_batches}", 
                        'æˆåŠŸ': successful_uploads,
                        'å¤±è´¥': failed_uploads
                    })
        
        self.logger.info(f"Qdrantä¸Šä¼ å®Œæˆï¼ŒæˆåŠŸä¸Šä¼  {successful_uploads} ä¸ªèŠ‚ç‚¹ï¼Œå¤±è´¥ {failed_uploads} ä¸ªèŠ‚ç‚¹")

    async def _init_embedding_model(self) -> Optional[HuggingFaceEmbedding]:
        """å¼‚æ­¥åˆå§‹åŒ–åµŒå…¥æ¨¡å‹"""
        def _load_model():
            """åœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œçš„åŒæ­¥åŠ è½½å‡½æ•°"""
            try:
                # ä½¿ç”¨ETLæ¨¡å—ç»Ÿä¸€é…ç½®çš„æ¨¡å‹è·¯å¾„
                models_path = str(MODELS_PATH)
                
                # è®¾ç½®HuggingFaceç¼“å­˜ç›®å½•
                os.environ['HF_HOME'] = models_path
                os.environ['TRANSFORMERS_CACHE'] = models_path
                os.environ['HF_HUB_CACHE'] = models_path
                os.environ['SENTENCE_TRANSFORMERS_HOME'] = models_path
                
                self.logger.info(f"åˆå§‹åŒ–åµŒå…¥æ¨¡å‹: {self.embedding_model}")
                embed_model = HuggingFaceEmbedding(
                    model_name=self.embedding_model,
                    device='cpu'  # å¼ºåˆ¶ä½¿ç”¨CPU
                )
                
                self.logger.info("åµŒå…¥æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
                return embed_model
                
            except Exception as e:
                self.logger.error(f"åˆå§‹åŒ–åµŒå…¥æ¨¡å‹å¤±è´¥: {e}")
                return None
        
        try:
            print("ğŸ¤– åˆå§‹åŒ–åµŒå…¥æ¨¡å‹...")
            loop = asyncio.get_running_loop()
            return await loop.run_in_executor(None, _load_model)
        except Exception as e:
            self.logger.error(f"æ‰§è¡ŒåµŒå…¥æ¨¡å‹åˆå§‹åŒ–æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return None

    async def _setup_collection(self, client: AsyncQdrantClient, incremental: bool = False):
        """è®¾ç½®Qdranté›†åˆ
        
        Args:
            client: Qdrantå¼‚æ­¥å®¢æˆ·ç«¯
            incremental: æ˜¯å¦å¢é‡æ„å»ºï¼ˆä¸åˆ é™¤ç°æœ‰é›†åˆï¼‰
        """
        try:
            print("ğŸ—‚ï¸  è®¾ç½®Qdranté›†åˆ...")
            # æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨
            collections = await client.get_collections()
            existing_collections = [col.name for col in collections.collections]
            
            if self.collection_name in existing_collections:
                if incremental:
                    self.logger.info(f"å¢é‡æ¨¡å¼: ä¿ç•™ç°æœ‰é›†åˆ {self.collection_name}")
                    # è·å–ç°æœ‰é›†åˆä¿¡æ¯
                    collection_info = await client.get_collection(self.collection_name)
                    current_count = collection_info.points_count
                    self.logger.info(f"ç°æœ‰é›†åˆåŒ…å« {current_count} ä¸ªå‘é‡")
                    print(f"ğŸ“Š ç°æœ‰é›†åˆåŒ…å« {current_count} ä¸ªå‘é‡ï¼Œå°†è¿›è¡Œå¢é‡æ›´æ–°")
                    return current_count
                else:
                    self.logger.info(f"å®Œå…¨é‡å»ºæ¨¡å¼: åˆ é™¤ç°æœ‰é›†åˆ {self.collection_name}")
                    await client.delete_collection(self.collection_name)
            
            # åˆ›å»ºæ–°é›†åˆï¼ˆä»…åœ¨éå¢é‡æ¨¡å¼æˆ–é›†åˆä¸å­˜åœ¨æ—¶ï¼‰
            if not incremental or self.collection_name not in existing_collections:
                self.logger.info(f"åˆ›å»ºæ–°é›†åˆ: {self.collection_name}, å‘é‡ç»´åº¦: {self.chunk_size}")
                await client.create_collection(
                    collection_name=self.collection_name,
                    vectors_config=models.VectorParams(
                        size=self.chunk_size,
                        distance=models.Distance.COSINE
                    )
                )
                return 0  # æ–°é›†åˆåˆå§‹å‘é‡æ•°ä¸º0
            
        except Exception as e:
            self.logger.error(f"è®¾ç½®Qdranté›†åˆæ—¶å‡ºé”™: {e}")
            raise

    async def _load_nodes_from_raw_files(self, limit: int = None) -> List[BaseNode]:
        """ä»åŸå§‹JSONæ–‡ä»¶åŠ è½½èŠ‚ç‚¹æ•°æ®ï¼ˆæ¨èæ–¹æ³•ï¼‰"""
        nodes = []
        
        try:
            # å®šä¹‰æ•°æ®æºé…ç½®
            data_sources = [
                {
                    'name': 'website',
                    'path': RAW_PATH / 'website' / 'nku',
                    'platform': 'website'
                },
                {
                    'name': 'wechat', 
                    'path': RAW_PATH / 'wechat' / 'nku',
                    'platform': 'wechat'
                },
                {
                    'name': 'market',
                    'path': RAW_PATH / 'market' / 'nku', 
                    'platform': 'market'
                },
                {
                    'name': 'wxapp',
                    'path': RAW_PATH / 'wxapp',
                    'platform': 'wxapp'
                }
            ]
            
            # ç»Ÿè®¡æ€»æ–‡ä»¶æ•°
            total_files = 0
            source_file_counts = {}
            
            for source in data_sources:
                if source['path'].exists():
                    json_files = list(source['path'].rglob('*.json'))
                    # è¿‡æ»¤æ‰ç‰¹æ®Šæ–‡ä»¶
                    json_files = [f for f in json_files if not f.name.startswith(('scraped_', 'counter.', 'lock.'))]
                    source_file_counts[source['name']] = len(json_files)
                    total_files += len(json_files)
                else:
                    source_file_counts[source['name']] = 0
                    self.logger.warning(f"æ•°æ®æºè·¯å¾„ä¸å­˜åœ¨: {source['path']}")
            
            if limit:
                total_files = min(total_files, limit)
            
            self.logger.info(f"é¢„è®¡å¤„ç† {total_files} ä¸ªJSONæ–‡ä»¶")
            for name, count in source_file_counts.items():
                if count > 0:
                    self.logger.info(f"  {name}: {count} ä¸ªæ–‡ä»¶")
            
            processed_count = 0
            
            # ä½¿ç”¨æ€»ä½“è¿›åº¦æ¡
            with tqdm(total=total_files, desc="åŠ è½½åŸå§‹æ•°æ®", unit="æ–‡ä»¶") as pbar:
                for source in data_sources:
                    if not source['path'].exists():
                        continue
                        
                    # è·å–è¯¥æ•°æ®æºçš„JSONæ–‡ä»¶
                    json_files = list(source['path'].rglob('*.json'))
                    json_files = [f for f in json_files if not f.name.startswith(('scraped_', 'counter.', 'lock.'))]
                    
                    if not json_files:
                        self.logger.info(f"{source['name']} æ²¡æœ‰æ‰¾åˆ°JSONæ–‡ä»¶")
                        continue
                    
                    self.logger.info(f"å¤„ç† {source['name']} æ•°æ®æº: {len(json_files)} ä¸ªæ–‡ä»¶")
                    
                    # å¤„ç†è¯¥æ•°æ®æºçš„æ–‡ä»¶
                    source_desc = f"å¤„ç†{source['name']}"
                    for json_file in tqdm(json_files, desc=source_desc, unit="æ–‡ä»¶", leave=False):
                        if limit and processed_count >= limit:
                            break
                            
                        try:
                            # è¯»å–JSONæ–‡ä»¶
                            async with aiofiles.open(json_file, 'r', encoding='utf-8') as f:
                                content_str = await f.read()
                            
                            if not content_str.strip():
                                continue
                            
                            data = json.loads(content_str)
                            
                            # å¤„ç†ä¸åŒçš„æ•°æ®æ ¼å¼
                            if isinstance(data, list):
                                records = data
                            elif isinstance(data, dict) and 'data' in data:
                                records = data['data'] if isinstance(data['data'], list) else [data['data']]
                            else:
                                records = [data]
                            
                            # è½¬æ¢æ¯æ¡è®°å½•ä¸ºèŠ‚ç‚¹
                            for record in records:
                                try:
                                    node = await self._create_node_from_record(record, source['platform'], json_file)
                                    if node:
                                        nodes.append(node)
                                        
                                except Exception as e:
                                    self.logger.warning(f"å¤„ç†è®°å½•æ—¶å‡ºé”™ {json_file}: {e}")
                                    continue
                            
                            processed_count += 1
                            pbar.update(1)
                            
                        except Exception as e:
                            self.logger.warning(f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™ {json_file}: {e}")
                            pbar.update(1)
                            continue
                    
                    if limit and processed_count >= limit:
                        break
            
            self.logger.info(f"æ€»è®¡ä»åŸå§‹æ–‡ä»¶åŠ è½½ {len(nodes)} ä¸ªèŠ‚ç‚¹")
            
        except Exception as e:
            self.logger.error(f"ä»åŸå§‹æ–‡ä»¶åŠ è½½æ•°æ®æ—¶å‡ºé”™: {e}")
        
        return nodes

    async def _create_node_from_record(self, record: Dict[str, Any], platform: str, source_file: Path) -> Optional[TextNode]:
        """ä»è®°å½•åˆ›å»ºTextNode"""
        try:
            # åŸºæœ¬å­—æ®µæå–
            content = record.get('content', '')
            title = record.get('title', record.get('name', ''))
            
            # å†…å®¹éªŒè¯
            if not content or not content.strip():
                return None
            
            if not title:
                title = f"æ–‡æ¡£_{record.get('id', 'unknown')}"
            
            # åˆå¹¶æ ‡é¢˜å’Œå†…å®¹
            full_text = f"{title}\n{content}" if title else content
            
            # æå–å…¶ä»–å…ƒæ•°æ®
            author = record.get('author', record.get('nickname', ''))
            url = record.get('original_url', record.get('url', record.get('link', '')))
            publish_time = record.get('publish_time', record.get('create_time', record.get('time', '')))
            
            # ç”Ÿæˆå”¯ä¸€ID
            record_id = record.get('id', str(abs(hash(url + title)) % 1000000))
            source_id = f"{platform}_{record_id}"
            
            # æ„é€ URLï¼ˆå¦‚æœæ²¡æœ‰ï¼‰
            if not url:
                if platform == 'wxapp':
                    url = f"wxapp://post/{record_id}"
                else:
                    url = f"{platform}://item/{record_id}"
            
            # åˆ›å»ºTextNode
            node = TextNode(
                text=full_text,
                metadata={
                    'source_id': source_id,
                    'id': record_id,
                    'url': url,
                    'title': title,
                    'author': author,
                    'original_url': url,
                    'publish_time': str(publish_time),
                    'source': platform,
                    'platform': platform,
                    'pagerank_score': float(record.get('pagerank_score', 0.0)),
                    'source_file': str(source_file),
                    'data_source': 'raw_files'
                }
            )
            
            return node
            
        except Exception as e:
            self.logger.warning(f"åˆ›å»ºèŠ‚ç‚¹æ—¶å‡ºé”™: {e}")
            return None

    async def _load_and_chunk_nodes_from_raw_files(self, limit: int = None) -> List[BaseNode]:
        """ä»åŸå§‹æ–‡ä»¶åŠ è½½æ•°æ®å¹¶è¿›è¡Œæ–‡æœ¬åˆ†å—"""
        # é¦–å…ˆåŠ è½½åŸå§‹èŠ‚ç‚¹
        raw_nodes = await self._load_nodes_from_raw_files(limit)
        
        if not raw_nodes:
            return []
        
        # è¿›è¡Œæ–‡æœ¬åˆ†å—
        return await self._chunk_nodes_with_progress(raw_nodes)

    async def _load_and_chunk_nodes_from_mysql(self, limit: int = None) -> List[BaseNode]:
        """ä»MySQLåŠ è½½æ•°æ®å¹¶è¿›è¡Œæ–‡æœ¬åˆ†å—"""
        nodes = []
        
        try:
            # æ„å»ºSQLæŸ¥è¯¢
            sql = """
            SELECT id, original_url, title, content, publish_time, platform, pagerank_score, author
            FROM website_nku
            WHERE content IS NOT NULL AND content != ''
            ORDER BY id
            """
            
            if limit:
                sql += f" LIMIT {limit}"
            
            # æ‰§è¡ŒæŸ¥è¯¢
            records = await db_core.execute_query(sql, fetch=True)
            
            if not records:
                self.logger.warning("MySQLä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•è®°å½•")
                return nodes
            
            # å¤„ç†æ¯æ¡è®°å½•ï¼Œåˆ›å»ºæ–‡æ¡£èŠ‚ç‚¹
            doc_nodes = []
            for record in tqdm(records, desc="å¤„ç†MySQLè®°å½•", unit="æ¡"):
                try:
                    # æ„å»ºæ–‡æ¡£å†…å®¹
                    content = record.get('content', '')
                    title = record.get('title', '')
                    
                    # åˆå¹¶æ ‡é¢˜å’Œå†…å®¹
                    full_text = f"{title}\n{content}" if title else content
                    
                    # åˆ›å»ºæ–‡æ¡£èŠ‚ç‚¹
                    doc_node = TextNode(
                        text=full_text,
                                        metadata={
                    'source_id': record.get('id'),
                    'id': record.get('id'),
                    'url': record.get('original_url', ''),
                    'title': title,
                    'author': record.get('author', ''),
                    'original_url': record.get('original_url', ''),
                    'publish_time': str(record.get('publish_time', '')),
                    'source': record.get('platform', ''),
                    'platform': record.get('platform', ''),
                            'pagerank_score': float(record.get('pagerank_score', 0.0)),
                            'data_source': 'mysql'
                        }
                    )
                    
                    doc_nodes.append(doc_node)
                    
                except Exception as e:
                    self.logger.warning(f"å¤„ç†è®°å½•ID {record.get('id')} æ—¶å‡ºé”™: {e}")
                    continue
            
            # è¿›è¡Œæ–‡æœ¬åˆ†å—
            nodes = await self._chunk_nodes_with_progress(doc_nodes)
            
        except Exception as e:
            self.logger.error(f"ä»MySQLåŠ è½½æ•°æ®æ—¶å‡ºé”™: {e}")
        
        return nodes

    async def _chunk_nodes_with_progress(self, doc_nodes: List[BaseNode]) -> List[BaseNode]:
        """å¯¹æ–‡æ¡£èŠ‚ç‚¹è¿›è¡Œåˆ†å—å¹¶æ˜¾ç¤ºè¿›åº¦ï¼ˆä½¿ç”¨ç¼“å­˜ä¼˜åŒ–ï¼‰"""
        from etl.processors.chunk_cache import chunk_documents_cached
        
        self.logger.info(f"å¼€å§‹åˆ†å— {len(doc_nodes)} ä¸ªæ–‡æ¡£èŠ‚ç‚¹...")
        
        # ä½¿ç”¨ç¼“å­˜è¿›è¡Œåˆ†å—
        chunked_nodes = await chunk_documents_cached(
            doc_nodes=doc_nodes,
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            force_refresh=False,
            show_progress=True
        )
        
        self.logger.info(f"åˆ†å—å®Œæˆï¼Œæ€»è®¡ {len(chunked_nodes)} ä¸ªæ–‡æœ¬å—")
        return chunked_nodes

    async def _load_nodes_hybrid(self, limit: int = None) -> List[BaseNode]:
        """æ··åˆåŠ è½½æ–¹æ¡ˆï¼šä»åŸå§‹æ–‡ä»¶åŠ è½½åŸºç¡€æ•°æ®ï¼Œä»MySQLè¡¥å……PageRankåˆ†æ•°ï¼ˆæ¨èï¼‰"""
        try:
            # ç¬¬ä¸€æ­¥ï¼šä»åŸå§‹æ–‡ä»¶åŠ è½½å¹¶åˆ†å—
            self.logger.info("ğŸ“ ç¬¬ä¸€æ­¥ï¼šä»åŸå§‹JSONæ–‡ä»¶åŠ è½½å¹¶åˆ†å—æ•°æ®...")
            raw_nodes = await self._load_and_chunk_nodes_from_raw_files(limit)
            
            if not raw_nodes:
                self.logger.warning("åŸå§‹æ–‡ä»¶ä¸­æ²¡æœ‰æ•°æ®ï¼Œå›é€€åˆ°MySQLæ¨¡å¼")
                return await self._load_and_chunk_nodes_from_mysql(limit)
            
            # ç¬¬äºŒæ­¥ï¼šä»MySQLåŠ è½½PageRankåˆ†æ•°æ˜ å°„
            self.logger.info("ğŸ“Š ç¬¬äºŒæ­¥ï¼šä»MySQLåŠ è½½PageRankåˆ†æ•°...")
            pagerank_mapping = await self._load_pagerank_mapping()
            
            if pagerank_mapping:
                self.logger.info(f"æˆåŠŸåŠ è½½ {len(pagerank_mapping)} ä¸ªPageRankåˆ†æ•°")
                
                # ç¬¬ä¸‰æ­¥ï¼šä¸ºèŠ‚ç‚¹è¡¥å……PageRankåˆ†æ•°
                updated_count = 0
                with tqdm(raw_nodes, desc="è¡¥å……PageRankåˆ†æ•°", unit="èŠ‚ç‚¹") as pbar:
                    for node in pbar:
                        url = node.metadata.get('original_url', '')
                        if url in pagerank_mapping:
                            node.metadata['pagerank_score'] = float(pagerank_mapping[url])
                            updated_count += 1
                        pbar.set_postfix({'å·²æ›´æ–°': updated_count})
                
                self.logger.info(f"ä¸º {updated_count} ä¸ªèŠ‚ç‚¹è¡¥å……äº†PageRankåˆ†æ•°")
            else:
                self.logger.warning("æ²¡æœ‰æ‰¾åˆ°PageRankåˆ†æ•°ï¼Œæ‰€æœ‰èŠ‚ç‚¹å°†ä½¿ç”¨é»˜è®¤å€¼0.0")
            
            self.logger.info(f"æ··åˆåŠ è½½å®Œæˆï¼Œæ€»è®¡ {len(raw_nodes)} ä¸ªèŠ‚ç‚¹")
            return raw_nodes
            
        except Exception as e:
            self.logger.error(f"æ··åˆåŠ è½½å¤±è´¥: {e}")
            self.logger.info("å°è¯•å›é€€åˆ°MySQLæ¨¡å¼...")
            try:
                return await self._load_and_chunk_nodes_from_mysql(limit)
            except Exception as mysql_error:
                self.logger.error(f"MySQLå›é€€ä¹Ÿå¤±è´¥: {mysql_error}")
                return []

    async def _load_pagerank_mapping(self) -> Dict[str, float]:
        """ä»MySQLåŠ è½½PageRankåˆ†æ•°æ˜ å°„"""
        try:
            # é¦–å…ˆå°è¯•ä»website_nkuè¡¨è·å–ï¼ˆå·²æ•´åˆçš„PageRankåˆ†æ•°ï¼‰
            query = """
            SELECT original_url, pagerank_score 
            FROM website_nku 
            WHERE pagerank_score > 0
            """
            records = await db_core.execute_query(query, fetch=True)
            
            if records:
                mapping = {record['original_url']: float(record['pagerank_score']) for record in records}
                self.logger.info(f"ä»website_nkuè¡¨åŠ è½½äº† {len(mapping)} ä¸ªPageRankåˆ†æ•°")
                return mapping
            
            # å¦‚æœwebsite_nkuè¡¨æ²¡æœ‰æ•°æ®ï¼Œå°è¯•ä»pagerank_scoresè¡¨è·å–
            query = """
            SELECT url, pagerank_score 
            FROM pagerank_scores
            """
            records = await db_core.execute_query(query, fetch=True)
            
            if records:
                mapping = {record['url']: float(record['pagerank_score']) for record in records}
                self.logger.info(f"ä»pagerank_scoresè¡¨åŠ è½½äº† {len(mapping)} ä¸ªPageRankåˆ†æ•°")
                return mapping
            
            self.logger.warning("ä¸¤ä¸ªè¡¨ä¸­éƒ½æ²¡æœ‰æ‰¾åˆ°PageRankæ•°æ®")
            return {}
            
        except Exception as e:
            self.logger.warning(f"åŠ è½½PageRankåˆ†æ•°æ—¶å‡ºé”™: {e}")
            return {}

    async def validate_index(self) -> Dict[str, Any]:
        """å¼‚æ­¥éªŒè¯Qdrantç´¢å¼•æ˜¯å¦å­˜åœ¨ä¸”æœ‰æ•ˆ"""
        self.logger.info(f"å¼€å§‹éªŒè¯Qdrantç´¢å¼•: {self.collection_name}")
        client = AsyncQdrantClient(url=self.qdrant_url)
        
        try:
            # æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨
            collections_response = await client.get_collections()
            collection_names = [col.name for col in collections_response.collections]

            if self.collection_name not in collection_names:
                self.logger.warning(f"é›†åˆ '{self.collection_name}' ä¸å­˜åœ¨ã€‚")
                return {"status": "missing", "collection_name": self.collection_name}

            # è·å–é›†åˆä¿¡æ¯
            collection_info = await client.get_collection(collection_name=self.collection_name)
            vector_count = collection_info.vectors_count

            # è·å–ä¸€äº›ç¤ºä¾‹æ–‡æ¡£ (è¿™é‡Œç”¨ scroll æ¥å£)
            scroll_response = await client.scroll(
                collection_name=self.collection_name,
                limit=5,
                with_payload=True
            )
            sample_docs = [record.payload for record in scroll_response[0]]

            self.logger.info(f"é›†åˆ '{self.collection_name}' éªŒè¯æˆåŠŸï¼ŒåŒ…å« {vector_count} ä¸ªå‘é‡ã€‚")

            return {
                "status": "ok",
                "collection_name": self.collection_name,
                "vector_count": vector_count,
                "vector_size": collection_info.vectors_config.params.size,
                "distance_metric": collection_info.vectors_config.params.distance.name,
                "sample_documents": sample_docs,
            }
        except Exception as e:
            self.logger.error(f"éªŒè¯Qdrantç´¢å¼•æ—¶å‡ºé”™: {e}")
            return {"status": "error", "error_message": str(e)}
        finally:
            await client.close()

    async def _load_nodes_from_daily_files(self, limit: int = None) -> List[BaseNode]:
        """ä»æ¯æ—¥å¢é‡æ•°æ®ç›®å½•åŠ è½½èŠ‚ç‚¹"""
        self.logger.info(f"ä»æ¯æ—¥å¢é‡æ•°æ®ç›®å½•åŠ è½½èŠ‚ç‚¹: {DAILY_PATH}")
        if not await aiofiles.os.path.exists(DAILY_PATH):
            self.logger.warning(f"æ¯æ—¥å¢é‡ç›®å½•ä¸å­˜åœ¨: {DAILY_PATH}")
            return []

        all_nodes = []
        source_files = list(DAILY_PATH.rglob("*.json"))
        
        if not source_files:
            self.logger.warning(f"åœ¨ {DAILY_PATH} ä¸­æœªæ‰¾åˆ° .json æ–‡ä»¶")
            return []
            
        self.logger.info(f"æ‰¾åˆ° {len(source_files)} ä¸ªæ¯æ—¥å¢é‡æ•°æ®æ–‡ä»¶")

        for file_path in tqdm(source_files, desc="å¤„ç†æ¯æ—¥å¢é‡æ–‡ä»¶"):
            try:
                platform = file_path.stem  # ä½¿ç”¨æ–‡ä»¶åä½œä¸ºå¹³å°æ ‡è¯†
                async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                    record = json.loads(await f.read())
                
                node = await self._create_node_from_record(record, platform, file_path)
                if node:
                    all_nodes.append(node)
                
                if limit and len(all_nodes) >= limit:
                    break
            except Exception as e:
                self.logger.error(f"å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
            
            if limit and len(all_nodes) >= limit:
                self.logger.info(f"å·²è¾¾åˆ°è®°å½•æ•°é™åˆ¶: {limit}")
                break
        
        self.logger.info(f"æ€»å…±ä»æ¯æ—¥å¢é‡æ–‡ä»¶åŠ è½½äº† {len(all_nodes)} ä¸ªèŠ‚ç‚¹")
        return all_nodes

    async def _load_nodes_recursively(self, data_path: Path, limit: int = None) -> List[BaseNode]:
        """
        ä»æŒ‡å®šçš„ç›®å½•é€’å½’åŠ è½½æ‰€æœ‰.jsonæ–‡ä»¶ä½œä¸ºèŠ‚ç‚¹ã€‚
        
        Args:
            data_path: è¦æ‰«æçš„æ ¹ç›®å½•è·¯å¾„
            limit: æœ€å¤§åŠ è½½èŠ‚ç‚¹æ•°
            
        Returns:
            åŠ è½½çš„TextNodeåˆ—è¡¨
        """
        self.logger.info(f"ä»ç›®æ ‡æ•°æ®ç›®å½•é€’å½’åŠ è½½èŠ‚ç‚¹: {data_path}")
        
        if not await aiofiles.os.path.isdir(data_path):
            self.logger.warning(f"ç›®æ ‡æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_path}")
            return []

        nodes = []
        
        # ä½¿ç”¨Path.rglobè¿›è¡Œé€’å½’æœç´¢
        json_files = list(data_path.rglob("*.json"))
        
        if not json_files:
            self.logger.warning(f"åœ¨ç›®å½• {data_path} ä¸‹æœªæ‰¾åˆ°ä»»ä½• .json æ–‡ä»¶ã€‚")
            return []
            
        self.logger.info(f"åœ¨ {data_path} ä¸­æ‰¾åˆ° {len(json_files)} ä¸ª .json æ–‡ä»¶ï¼Œå¼€å§‹å¤„ç†...")

        with tqdm(total=len(json_files), desc=f"é€’å½’åŠ è½½ {data_path.name}", unit="file") as pbar:
            for file_path in json_files:
                if limit and len(nodes) >= limit:
                    self.logger.info(f"å·²è¾¾åˆ°å¤„ç†ä¸Šé™ {limit}ï¼Œåœæ­¢åŠ è½½ã€‚")
                    break
                
                try:
                    async with aiofiles.open(file_path, mode='r', encoding='utf-8') as f:
                        content = await f.read()
                        # è·³è¿‡ç©ºæ–‡ä»¶
                        if not content.strip():
                            continue
                        record = json.loads(content)
                        
                        platform = record.get("platform", "unknown")
                        node = await self._create_node_from_record(record, platform, file_path)
                        if node:
                            nodes.append(node)
                            
                except json.JSONDecodeError:
                    self.logger.warning(f"æ— æ³•è§£æJSONæ–‡ä»¶: {file_path}")
                except Exception as e:
                    self.logger.error(f"å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
                
                pbar.update(1)
                
        self.logger.info(f"ä»ç›®å½• {data_path} çš„ {len(json_files)} ä¸ªæ–‡ä»¶ä¸­åŠ è½½äº† {len(nodes)} ä¸ªèŠ‚ç‚¹")
        return nodes

    async def build_indexes_from_files(self,
                                     file_paths: List[Path],
                                     batch_size: int = 128,
                                     test_mode: bool = False) -> Dict[str, Any]:
        """
        ä»æŒ‡å®šçš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨æ„å»ºQdrantå‘é‡ç´¢å¼•ã€‚

        Args:
            file_paths: è¦å¤„ç†çš„JSONæ–‡ä»¶çš„è·¯å¾„åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°
            test_mode: æµ‹è¯•æ¨¡å¼ï¼Œä¸å®é™…åˆ›å»ºç´¢å¼•ï¼Œä»…åŠ è½½èŠ‚ç‚¹

        Returns:
            æ„å»ºç»“æœç»Ÿè®¡
        """
        self.logger.info(f"å¼€å§‹ä» {len(file_paths)} ä¸ªæ–‡ä»¶æ„å»ºQdrantç´¢å¼•ï¼Œé›†åˆ: {self.collection_name}")
        
        try:
            await self._ensure_collection_exists()

            # 1. ä»æ–‡ä»¶åˆ—è¡¨åŠ è½½å’Œåˆ†å—èŠ‚ç‚¹
            nodes = await self._load_nodes_from_file_list(file_paths)
            if not nodes:
                self.logger.warning("ä»æä¾›çš„æ–‡ä»¶åˆ—è¡¨ä¸­æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•èŠ‚ç‚¹ã€‚")
                return {"success": True, "nodes": [], "message": "æ²¡æœ‰åŠ è½½åˆ°èŠ‚ç‚¹"}

            # 2. å¦‚æœæ˜¯æµ‹è¯•æ¨¡å¼ï¼ŒåŠ è½½å®ŒèŠ‚ç‚¹åç›´æ¥è¿”å›
            if test_mode:
                self.logger.info("æµ‹è¯•æ¨¡å¼ï¼šå·²åŠ è½½èŠ‚ç‚¹ï¼Œè·³è¿‡ç´¢å¼•æ„å»ºã€‚")
                return {"success": True, "nodes": nodes}
            
            # 3. åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
            if self.embed_model is None:
                self.embed_model = await self._init_embedding_model()
                if self.embed_model is None:
                    return {"success": False, "error": "åµŒå…¥æ¨¡å‹åˆå§‹åŒ–å¤±è´¥"}

            # 4. æ„å»ºå‘é‡ç´¢å¼•
            vector_store = QdrantVectorStore(
                aclient=self.client,
                collection_name=self.collection_name
            )
            
            self.logger.info("ğŸ”® ç”Ÿæˆå‘é‡åµŒå…¥...")
            nodes = await self._generate_embeddings_with_progress(self.embed_model, nodes, batch_size)

            self.logger.info("ğŸ“¤ ä¸Šä¼ å‘é‡åˆ°Qdrant...")
            await self._upload_to_qdrant_with_progress(vector_store, nodes, batch_size)
            
            self.logger.info(f"Qdrantå‘é‡ç´¢å¼•æ„å»ºå®Œæˆï¼Œé›†åˆ: {self.collection_name}")
            
            return {
                "nodes": nodes,
                "total_nodes": len(nodes),
                "success": True,
                "collection_name": self.collection_name,
                "message": f"æˆåŠŸä» {len(file_paths)} ä¸ªæ–‡ä»¶æ„å»ºäº† {len(nodes)} ä¸ªå‘é‡"
            }
            
        except Exception as e:
            self.logger.exception(f"ä»æ–‡ä»¶åˆ—è¡¨æ„å»ºQdrantç´¢å¼•æ—¶å‡ºé”™: {e}")
            return {"success": False, "error": str(e)}

    async def _load_nodes_from_file_list(self, file_paths: List[Path]) -> List[BaseNode]:
        """
        ä»ä¸€ä¸ªæ–‡ä»¶è·¯å¾„åˆ—è¡¨åŠ è½½ã€è§£æå’Œåˆ†å—èŠ‚ç‚¹ã€‚
        
        Args:
            file_paths: JSONæ–‡ä»¶çš„è·¯å¾„åˆ—è¡¨ã€‚
            
        Returns:
            å¤„ç†å’Œåˆ†å—åçš„èŠ‚ç‚¹åˆ—è¡¨ã€‚
        """
        all_docs = []
        self.logger.info(f"å¼€å§‹ä» {len(file_paths)} ä¸ªæ–‡ä»¶åŠ è½½å†…å®¹...")
        
        for file_path in tqdm(file_paths, desc="è¯»å–æ–‡ä»¶", unit="file"):
            try:
                async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                    content = await f.read()
                    data = json.loads(content)
                    
                    # ç¡®ä¿æ˜¯æœ‰æ•ˆçš„å­—å…¸å¹¶ä¸”åŒ…å«å†…å®¹
                    if isinstance(data, dict) and data.get("content"):
                        node = self._create_node_from_record(data, file_path)
                        if node:
                            all_docs.append(node)
                            
            except json.JSONDecodeError:
                self.logger.warning(f"æ— æ³•è§£æJSONæ–‡ä»¶ï¼Œè·³è¿‡: {file_path}")
            except Exception as e:
                self.logger.error(f"è¯»å–æˆ–å¤„ç†æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        
        if not all_docs:
            self.logger.warning("æœªèƒ½ä»ä»»ä½•æ–‡ä»¶åˆ›å»ºæœ‰æ•ˆèŠ‚ç‚¹ã€‚")
            return []
            
        self.logger.info(f"æˆåŠŸä»æ–‡ä»¶åˆ—è¡¨åŠ è½½äº† {len(all_docs)} ç¯‡æ–‡æ¡£ï¼Œç°åœ¨å¼€å§‹åˆ†å—...")
        chunked_nodes = await self._chunk_nodes_with_progress(all_docs)
        self.logger.info(f"åˆ†å—å®Œæˆï¼Œå…±ç”Ÿæˆ {len(chunked_nodes)} ä¸ªèŠ‚ç‚¹ã€‚")
        
        return chunked_nodes


# å‘åå…¼å®¹çš„å‡½æ•°æ¥å£
async def build_qdrant_index(
    collection_name: str = None,
    embedding_model: str = None,
    qdrant_url: str = None,
    chunk_size: int = 512,
    chunk_overlap: int = 200,
    limit: int = None,
    batch_size: int = 100,
    data_source: str = "raw_files"
) -> Dict[str, Any]:
    """
    æ„å»ºQdrantå‘é‡ç´¢å¼•ï¼ˆå‘åå…¼å®¹æ¥å£ï¼‰
    
    Args:
        data_source: æ•°æ®æºç±»å‹ ("raw_files"=æ··åˆæ¨¡å¼, "mysql"=ä»…MySQL, "raw_only"=ä»…åŸå§‹æ–‡ä»¶)
    """
    config = Config()
    
    # è®¾ç½®é…ç½®å‚æ•°
    indexer_config = {
        'etl.data.qdrant.collection': collection_name or config.get('etl.data.qdrant.collection'),
        'etl.data.qdrant.url': qdrant_url or config.get('etl.data.qdrant.url'),
        'etl.embedding.name': embedding_model or config.get('etl.embedding.name'),
        'etl.embedding.chunking.chunk_size': chunk_size,
        'etl.embedding.chunking.chunk_overlap': chunk_overlap,
        'etl.data.base_path': config.get('etl.data.base_path'),
        'etl.data.models.path': config.get('etl.data.models.path'),
        'etl.data.qdrant.vector_size': config.get('etl.data.qdrant.vector_size')
    }
    
    # åˆ›å»ºç´¢å¼•æ„å»ºå™¨
    indexer = QdrantIndexer(indexer_config)
    
    # æ„å»ºç´¢å¼•
    return await indexer.build_indexes(limit=limit, batch_size=batch_size, data_source=data_source) 