"""
Elasticsearchç´¢å¼•æ„å»ºå™¨

è´Ÿè´£ä»MySQLæ•°æ®æ„å»ºElasticsearchå…¨æ–‡æ£€ç´¢ç´¢å¼•ã€‚
"""

import sys
import loguru
import asyncio
import json
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime
import aiofiles
import aiofiles.os
from tqdm.asyncio import tqdm
from elasticsearch import Elasticsearch, helpers, AsyncElasticsearch
import os
import loguru

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).resolve().parent.parent.parent))
from config import Config
from etl.load import db_core
# å¯¼å…¥ETLæ¨¡å—çš„ç»Ÿä¸€è·¯å¾„é…ç½®
from etl import (RAW_PATH, ES_INDEX_NAME, ES_HOST, ES_PORT, ES_ENABLE_CHUNKING, CHUNK_SIZE, CHUNK_OVERLAP)

class ElasticsearchIndexer:
    """Elasticsearchç´¢å¼•æ„å»ºå™¨
    
    è´Ÿè´£ä»MySQLæ•°æ®æ„å»ºElasticsearchå…¨æ–‡æ£€ç´¢ç´¢å¼•ï¼Œæ”¯æŒé€šé…ç¬¦æŸ¥è¯¢å’Œå¤æ‚æ–‡æœ¬åŒ¹é…ã€‚
    """
    
    def __init__(self, logger):
        self.logger = logger
        
        # ä»ETLæ¨¡å—ç»Ÿä¸€é…ç½®è·å–å‚æ•°
        self.index_name = ES_INDEX_NAME
        self.es_host = ES_HOST
        self.es_port = ES_PORT
        
        # åˆ†å—å‚æ•°
        self.enable_chunking = ES_ENABLE_CHUNKING
        self.chunk_size = CHUNK_SIZE
        self.chunk_overlap = CHUNK_OVERLAP
        
        self.es_client: Optional[AsyncElasticsearch] = None
        self.logger.info(f"ElasticsearchIndexer åˆå§‹åŒ–å®Œæˆã€‚è¿æ¥ç›®æ ‡: http://{self.es_host}:{self.es_port}")
        
    async def build_indexes(self, 
                     limit: int = None, 
                     batch_size: int = 50,
                     recreate_index: bool = True,
                     test_mode: bool = False,
                     data_source: str = "raw_files",
                     start_batch: int = 0,
                     max_batches: int = None) -> Dict[str, Any]:
        """
        æ„å»ºElasticsearchç´¢å¼•
        
        Args:
            limit: é™åˆ¶å¤„ç†çš„è®°å½•æ•°é‡ï¼ŒNoneè¡¨ç¤ºå¤„ç†æ‰€æœ‰
            batch_size: æ‰¹å¤„ç†å¤§å°
            recreate_index: æ˜¯å¦é‡æ–°åˆ›å»ºç´¢å¼•
            test_mode: æµ‹è¯•æ¨¡å¼ï¼Œä¸å®é™…åˆ›å»ºç´¢å¼•
            data_source: æ•°æ®æºç±»å‹ ("raw_files"=æ··åˆæ¨¡å¼, "mysql"=ä»…MySQL, "raw_only"=ä»…åŸå§‹æ–‡ä»¶)
            start_batch: ä»ç¬¬å‡ æ‰¹å¼€å§‹å¤„ç†ï¼ˆåˆ†æ‰¹æ„å»ºï¼‰
            max_batches: æœ€å¤§æ‰¹æ¬¡æ•°ï¼ŒNoneè¡¨ç¤ºå¤„ç†æ‰€æœ‰æ‰¹æ¬¡
            
        Returns:
            æ„å»ºç»“æœç»Ÿè®¡
        """
        self.logger.info(f"å¼€å§‹æ„å»ºElasticsearchç´¢å¼•: {self.index_name}")
        
        try:
            # åˆå§‹åŒ–Elasticsearchå®¢æˆ·ç«¯
            es_client = await self._init_es_client()
            if not es_client:
                if test_mode:
                    # æµ‹è¯•æ¨¡å¼ä¸‹ï¼Œæ— æ³•è¿æ¥ESæ—¶è¿”å›æˆåŠŸçš„æ¨¡æ‹Ÿç»“æœ
                    return {
                        "total_records": 0,
                        "success": True,
                        "indexed": 0,
                        "errors": 0,
                        "data_source": data_source,
                        "message": "æµ‹è¯•æ¨¡å¼ï¼šElasticsearchæœåŠ¡ä¸å¯ç”¨ï¼Œè·³è¿‡ç´¢å¼•æ„å»º"
                    }
                else:
                    return {
                        "success": False, 
                        "error": "æ— æ³•è¿æ¥åˆ°Elasticsearch", 
                        "message": "Elasticsearchè¿æ¥å¤±è´¥"
                    }
            
            # è®¾ç½®ç´¢å¼•
            if not test_mode and recreate_index:
                await self._setup_index(es_client)
            
            # æ ¹æ®æ•°æ®æºç±»å‹åŠ è½½æ•°æ®å¹¶å»ºç«‹ç´¢å¼•
            try:
                if not test_mode:
                    result = await self._index_documents_from_source(es_client, limit, batch_size, data_source)
                else:
                    # æµ‹è¯•æ¨¡å¼ï¼šåªåŠ è½½æ•°æ®ä¸ç´¢å¼•
                    result = await self._test_data_loading_from_source(limit, data_source)
                    
                result['data_source'] = data_source
                self.logger.info(f"Elasticsearchç´¢å¼•æ„å»ºå®Œæˆ: {result}")
                return result
            finally:
                # ç¡®ä¿å®¢æˆ·ç«¯è¿æ¥è¢«æ­£ç¡®å…³é—­
                if es_client:
                    try:
                        await es_client.close()
                    except Exception as e:
                        self.logger.debug(f"å…³é—­ESå®¢æˆ·ç«¯æ—¶å‡ºé”™: {e}")
            
        except Exception as e:
            print(f"âŒ æ„å»ºå¤±è´¥: {e}")
            self.logger.error(f"æ„å»ºElasticsearchç´¢å¼•æ—¶å‡ºé”™: {e}")
            return {
                "success": False, 
                "error": str(e),
                "message": f"Elasticsearchç´¢å¼•æ„å»ºå¤±è´¥: {str(e)}"
            }

    async def build_from_nodes(self, nodes: List[Dict[str, Any]], batch_size: int = 500) -> Dict[str, Any]:
        """
        ä»é¢„åŠ è½½çš„èŠ‚ç‚¹åˆ—è¡¨æ„å»ºElasticsearchç´¢å¼•ã€‚
        
        Args:
            nodes: ä»æ–‡ä»¶å¤„ç†é˜¶æ®µä¼ å…¥çš„TextNodeåˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°
            
        Returns:
            æ„å»ºç»“æœç»Ÿè®¡
        """
        self.logger.info(f"å¼€å§‹ä» {len(nodes)} ä¸ªé¢„åŠ è½½èŠ‚ç‚¹æ„å»ºElasticsearchç´¢å¼•...")
        es_client = await self._init_es_client()
        if not es_client:
            return {"success": False, "error": "æ— æ³•è¿æ¥åˆ°Elasticsearch"}

        try:
            # 1. å°† TextNode è½¬æ¢ä¸º Elasticsearch éœ€è¦çš„å­—å…¸æ ¼å¼
            actions = []
            for node in nodes:
                # ä» metadata æå–æ‰€éœ€å­—æ®µ
                doc = {
                    "url": node.metadata.get("url", ""),
                    "title": node.metadata.get("title", ""),
                    "content": node.text,  # content æ¥è‡ª node.text
                    "publish_time": node.metadata.get("publish_time"),
                    "platform": node.metadata.get("platform", ""),
                    "pagerank_score": float(node.metadata.get("pagerank_score", 0.0))
                }
                
                # ç§»é™¤ç©ºå€¼å­—æ®µï¼Œä»¥é¿å…ESç´¢å¼•é”™è¯¯
                doc = {k: v for k, v in doc.items() if v is not None}
                
                actions.append({
                    "_index": self.index_name,
                    "_id": node.metadata.get("source_id", node.id_),
                    "_source": doc
                })

            if not actions:
                self.logger.warning("æ²¡æœ‰å¯ä¾›ç´¢å¼•çš„æœ‰æ•ˆèŠ‚ç‚¹ã€‚")
                return {"success": True, "indexed": 0, "errors": 0}

            # 2. ä½¿ç”¨ helpers.async_bulk æ‰¹é‡ç´¢å¼•
            indexed, errors = await helpers.async_bulk(es_client, actions, chunk_size=batch_size)
            
            result = {
                "success": True,
                "indexed": indexed,
                "errors": len(errors),
                "message": f"æˆåŠŸç´¢å¼• {indexed} ä¸ªæ–‡æ¡£ï¼Œå¤±è´¥ {len(errors)} ä¸ªã€‚"
            }
            self.logger.info(f"Elasticsearchç´¢å¼•æ„å»ºå®Œæˆ: {result}")
            return result
            
        except Exception as e:
            self.logger.error(f"ä»èŠ‚ç‚¹æ„å»ºElasticsearchç´¢å¼•æ—¶å‡ºé”™: {e}")
            return {"success": False, "error": str(e)}
        finally:
            if es_client:
                await es_client.close()

    async def _init_es_client(self) -> Optional[AsyncElasticsearch]:
        """åˆå§‹åŒ–Elasticsearchå®¢æˆ·ç«¯"""
        if self.es_client and await self.es_client.ping():
            return self.es_client
            
        try:
            self.logger.info("ğŸ” æ­£åœ¨åˆå§‹åŒ–æˆ–é‡æ–°åˆå§‹åŒ–Elasticsearchå¼‚æ­¥å®¢æˆ·ç«¯...")
            
            # ä½¿ç”¨åœ¨ __init__ ä¸­å·²ç»è®¾ç½®å¥½çš„ä¸»æœºå’Œç«¯å£
            es_url = f"http://{self.es_host}:{self.es_port}"
            
            self.es_client = AsyncElasticsearch(
                hosts=[es_url],
                verify_certs=False,
                http_compress=True,
                request_timeout=30,
                max_retries=3, 
                retry_on_timeout=True
            )
            
           # æµ‹è¯•è¿æ¥ï¼ˆè®¾ç½®è¾ƒçŸ­è¶…æ—¶ï¼‰
            try:
                ping_result = await asyncio.wait_for(self.es_client.ping(), timeout=5.0)
                if not ping_result:
                    await self.es_client.close() 
                    self.logger.debug("ElasticsearchæœåŠ¡å™¨pingå¤±è´¥")
                    return None
            except asyncio.TimeoutError:
                await self.es_client.close()
                self.logger.debug("Elasticsearchè¿æ¥è¶…æ—¶")
                return None
            
            self.logger.info(f"æˆåŠŸè¿æ¥åˆ°Elasticsearch: {es_url}")
            return self.es_client
                
        except Exception as e:
            self.logger.error(f"âŒ åˆå§‹åŒ–Elasticsearchå®¢æˆ·ç«¯æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
            if self.es_client:
                await self.es_client.close()
            self.es_client = None
            return None

    async def _setup_index(self, es_client: AsyncElasticsearch):
        """è®¾ç½®Elasticsearchç´¢å¼•"""
        try:
            print("ğŸ—‚ï¸  è®¾ç½®Elasticsearchç´¢å¼•...")
            # åˆ é™¤ç°æœ‰ç´¢å¼•
            if await es_client.indices.exists(index=self.index_name):
                self.logger.info(f"åˆ é™¤ç°æœ‰ç´¢å¼•: {self.index_name}")
                await es_client.indices.delete(index=self.index_name)
            
            # ä½¿ç”¨IKåˆ†æå™¨è¿›è¡Œä¸­æ–‡åˆ†è¯
            self.logger.info("ä½¿ç”¨IKåˆ†æå™¨è¿›è¡Œä¸­æ–‡åˆ†è¯")
            
            mapping = {
                "mappings": {
                    "properties": {
                        "url": {
                            "type": "keyword",
                            "index": True
                        },
                        "title": {
                            "type": "text",
                            "analyzer": "ik_max_word",
                            "search_analyzer": "ik_smart"
                        },
                        "content": {
                            "type": "text",
                            "analyzer": "ik_max_word",
                            "search_analyzer": "ik_smart"
                        },
                        "publish_time": {
                            "type": "date",
                            "format": "yyyy-MM-dd||yyyy-MM-dd HH:mm:ss||epoch_millis"
                        },
                        "platform": {
                            "type": "keyword",
                            "index": True
                        },
                        "pagerank_score": {
                            "type": "float"
                        }
                    }
                },
                "settings": {
                    "number_of_shards": 1,
                    "number_of_replicas": 0,
                    "analysis": {
                        "analyzer": {
                            "default": {
                                "type": "ik_max_word"
                            }
                        }
                    }
                }
            }
            
            # åˆ›å»ºç´¢å¼•
            self.logger.info(f"åˆ›å»ºæ–°ç´¢å¼•: {self.index_name}")
            await es_client.indices.create(index=self.index_name, body=mapping)
            
        except Exception as e:
            self.logger.error(f"è®¾ç½®Elasticsearchç´¢å¼•æ—¶å‡ºé”™: {e}")
            raise

    async def _load_records_from_raw_files(self, limit: int = None) -> List[Dict[str, Any]]:
        """ä»åŸå§‹JSONæ–‡ä»¶åŠ è½½è®°å½•æ•°æ®"""
        records = []
        
        try:
            # å®šä¹‰æ•°æ®æºé…ç½®
            data_sources = [
                {
                    'name': 'website',
                    'path': RAW_PATH / 'website' / 'nku',
                    'platform': 'website'
                },
                {
                    'name': 'wechat', 
                    'path': RAW_PATH / 'wechat' / 'nku',
                    'platform': 'wechat'
                },
                {
                    'name': 'market',
                    'path': RAW_PATH / 'market' / 'nku', 
                    'platform': 'market'
                },
                {
                    'name': 'wxapp',
                    'path': RAW_PATH / 'wxapp',
                    'platform': 'wxapp'
                }
            ]
            
            # ç»Ÿè®¡æ€»æ–‡ä»¶æ•°
            total_files = 0
            source_file_counts = {}
            
            for source in data_sources:
                if source['path'].exists():
                    json_files = list(source['path'].rglob('*.json'))
                    # è¿‡æ»¤æ‰ç‰¹æ®Šæ–‡ä»¶
                    json_files = [f for f in json_files if not f.name.startswith(('scraped_', 'counter.', 'lock.'))]
                    source_file_counts[source['name']] = len(json_files)
                    total_files += len(json_files)
                else:
                    source_file_counts[source['name']] = 0
                    self.logger.debug(f"æ•°æ®æºè·¯å¾„ä¸å­˜åœ¨: {source['path']}")
            
            if limit:
                total_files = min(total_files, limit)
            
            self.logger.info(f"é¢„è®¡å¤„ç† {total_files} ä¸ªJSONæ–‡ä»¶")
            for name, count in source_file_counts.items():
                if count > 0:
                    self.logger.info(f"  {name}: {count} ä¸ªæ–‡ä»¶")
            
            processed_count = 0
            
            # ä½¿ç”¨æ€»ä½“è¿›åº¦æ¡
            with tqdm(total=total_files, desc="åŠ è½½åŸå§‹æ•°æ®", unit="æ–‡ä»¶") as pbar:
                for source in data_sources:
                    if not source['path'].exists():
                        continue
                        
                    # è·å–è¯¥æ•°æ®æºçš„JSONæ–‡ä»¶
                    json_files = list(source['path'].rglob('*.json'))
                    json_files = [f for f in json_files if not f.name.startswith(('scraped_', 'counter.', 'lock.'))]
                    
                    if not json_files:
                        self.logger.info(f"{source['name']} æ²¡æœ‰æ‰¾åˆ°JSONæ–‡ä»¶")
                        continue
                    
                    self.logger.info(f"å¤„ç† {source['name']} æ•°æ®æº: {len(json_files)} ä¸ªæ–‡ä»¶")
                    
                    # å¤„ç†è¯¥æ•°æ®æºçš„æ–‡ä»¶
                    source_desc = f"å¤„ç†{source['name']}"
                    for json_file in tqdm(json_files, desc=source_desc, unit="æ–‡ä»¶", leave=False):
                        if limit and processed_count >= limit:
                            break
                            
                        try:
                            # è¯»å–JSONæ–‡ä»¶
                            async with aiofiles.open(json_file, 'r', encoding='utf-8') as f:
                                content_str = await f.read()
                            
                            if not content_str.strip():
                                continue
                            
                            data = json.loads(content_str)
                            
                            # å¤„ç†ä¸åŒçš„æ•°æ®æ ¼å¼
                            if isinstance(data, list):
                                file_records = data
                            elif isinstance(data, dict) and 'data' in data:
                                file_records = data['data'] if isinstance(data['data'], list) else [data['data']]
                            else:
                                file_records = [data]
                            
                            # è½¬æ¢æ¯æ¡è®°å½•
                            for record in file_records:
                                try:
                                    converted_record = await self._convert_record_from_raw(record, source['platform'], json_file)
                                    if converted_record:
                                        records.append(converted_record)
                                        
                                except Exception as e:
                                    self.logger.warning(f"å¤„ç†è®°å½•æ—¶å‡ºé”™ {json_file}: {e}")
                                    continue
                            
                            processed_count += 1
                            pbar.update(1)
                            
                        except Exception as e:
                            self.logger.warning(f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™ {json_file}: {e}")
                            pbar.update(1)
                            continue
                    
                    if limit and processed_count >= limit:
                        break
            
            self.logger.info(f"æ€»è®¡ä»åŸå§‹æ–‡ä»¶åŠ è½½ {len(records)} æ¡è®°å½•")
            
        except Exception as e:
            self.logger.error(f"ä»åŸå§‹æ–‡ä»¶åŠ è½½æ•°æ®æ—¶å‡ºé”™: {e}")
        
        return records

    async def _convert_record_from_raw(self, record: Dict[str, Any], platform: str, source_file: Path) -> Optional[Dict[str, Any]]:
        """ä»åŸå§‹è®°å½•è½¬æ¢ä¸ºElasticsearchè®°å½•æ ¼å¼"""
        try:
            # åŸºæœ¬å­—æ®µæå–
            content = record.get('content', '')
            title = record.get('title', record.get('name', ''))
            
            # å†…å®¹éªŒè¯
            if not content or not content.strip():
                return None
            
            if not title:
                title = f"æ–‡æ¡£_{record.get('id', 'unknown')}"
            
            # æå–å…¶ä»–å…ƒæ•°æ®
            author = record.get('author', record.get('nickname', ''))
            url = record.get('original_url', record.get('url', record.get('link', '')))
            publish_time = record.get('publish_time', record.get('create_time', record.get('time', '')))
            
            # ç”Ÿæˆå”¯ä¸€ID
            record_id = record.get('id', str(abs(hash(url + title)) % 1000000))
            
            # æ„é€ URLï¼ˆå¦‚æœæ²¡æœ‰ï¼‰
            if not url:
                if platform == 'wxapp':
                    url = f"wxapp://post/{record_id}"
                else:
                    url = f"{platform}://item/{record_id}"
            
            # å¤„ç†å‘å¸ƒæ—¶é—´
            if publish_time and not self._is_valid_date(str(publish_time)):
                publish_time = None
            
            # åˆ›å»ºESè®°å½•
            es_record = {
                'id': record_id,
                'source_id': f"{platform}_{record_id}",
                'original_url': url,
                'title': title,
                'content': content,
                'author': author,
                'publish_time': publish_time,
                'platform': platform,
                'pagerank_score': float(record.get('pagerank_score', 0.0)),
                'source_file': str(source_file),
                'data_source': 'raw_files'
            }
            
            return es_record
            
        except Exception as e:
            self.logger.warning(f"è½¬æ¢è®°å½•æ—¶å‡ºé”™: {e}")
            return None

    async def _load_records_from_mysql(self, limit: int = None) -> List[Dict[str, Any]]:
        """ä»MySQLåŠ è½½è®°å½•æ•°æ®"""
        try:
            # æ„å»ºSQLæŸ¥è¯¢
            sql = """
            SELECT id, original_url, title, content, publish_time, platform, pagerank_score, author
            FROM website_nku
            WHERE content IS NOT NULL AND content != ''
            ORDER BY id
            """
            
            if limit:
                sql += f" LIMIT {limit}"
            
            # æ‰§è¡ŒæŸ¥è¯¢
            records = await db_core.execute_query(sql, fetch=True)
            
            if not records:
                self.logger.warning("MySQLä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•è®°å½•")
                return []
            
            # è½¬æ¢è®°å½•æ ¼å¼ï¼Œæ·»åŠ æ•°æ®æºæ ‡è®°
            converted_records = []
            for record in tqdm(records, desc="å¤„ç†MySQLè®°å½•", unit="æ¡"):
                try:
                    # å¤„ç†å‘å¸ƒæ—¶é—´
                    publish_time = record.get('publish_time')
                    if publish_time and not self._is_valid_date(str(publish_time)):
                        publish_time = None
                    
                    converted_record = {
                        'id': record.get('id'),
                        'source_id': record.get('id'),
                        'original_url': record.get('original_url', ''),
                        'title': record.get('title', ''),
                        'content': record.get('content', ''),
                        'author': record.get('author', ''),
                        'publish_time': publish_time,
                        'platform': record.get('platform', ''),
                        'pagerank_score': float(record.get('pagerank_score', 0.0)),
                        'data_source': 'mysql'
                    }
                    converted_records.append(converted_record)
                    
                except Exception as e:
                    self.logger.warning(f"å¤„ç†MySQLè®°å½•æ—¶å‡ºé”™: {e}")
                    continue
            
            self.logger.info(f"ä»MySQLåŠ è½½äº† {len(converted_records)} æ¡è®°å½•")
            return converted_records
            
        except Exception as e:
            self.logger.error(f"ä»MySQLåŠ è½½æ•°æ®æ—¶å‡ºé”™: {e}")
            return []

    async def _load_records_hybrid(self, limit: int = None) -> List[Dict[str, Any]]:
        """æ··åˆåŠ è½½æ–¹æ¡ˆï¼šä»åŸå§‹æ–‡ä»¶åŠ è½½åŸºç¡€æ•°æ®ï¼Œä»MySQLè¡¥å……PageRankåˆ†æ•°ï¼ˆæ¨èï¼‰"""
        try:
            # ç¬¬ä¸€æ­¥ï¼šä»åŸå§‹æ–‡ä»¶åŠ è½½åŸºç¡€æ•°æ®
            self.logger.info("ğŸ“ ç¬¬ä¸€æ­¥ï¼šä»åŸå§‹JSONæ–‡ä»¶åŠ è½½åŸºç¡€æ•°æ®...")
            raw_records = await self._load_records_from_raw_files(limit)
            
            if not raw_records:
                self.logger.debug("åŸå§‹æ–‡ä»¶ä¸­æ²¡æœ‰æ•°æ®ï¼Œå›é€€åˆ°MySQLæ¨¡å¼")
                return await self._load_records_from_mysql(limit)
            
            # ç¬¬äºŒæ­¥ï¼šä»MySQLåŠ è½½PageRankåˆ†æ•°æ˜ å°„
            self.logger.info("ğŸ“Š ç¬¬äºŒæ­¥ï¼šä»MySQLåŠ è½½PageRankåˆ†æ•°...")
            pagerank_mapping = await self._load_pagerank_mapping()
            
            if pagerank_mapping:
                self.logger.info(f"æˆåŠŸåŠ è½½ {len(pagerank_mapping)} ä¸ªPageRankåˆ†æ•°")
                
                # ç¬¬ä¸‰æ­¥ï¼šä¸ºè®°å½•è¡¥å……PageRankåˆ†æ•°
                updated_count = 0
                with tqdm(raw_records, desc="è¡¥å……PageRankåˆ†æ•°", unit="è®°å½•") as pbar:
                    for record in pbar:
                        url = record.get('original_url', '')
                        if url in pagerank_mapping:
                            record['pagerank_score'] = float(pagerank_mapping[url])
                            updated_count += 1
                        pbar.set_postfix({'å·²æ›´æ–°': updated_count})
                
                self.logger.info(f"ä¸º {updated_count} æ¡è®°å½•è¡¥å……äº†PageRankåˆ†æ•°")
            else:
                self.logger.warning("æ²¡æœ‰æ‰¾åˆ°PageRankåˆ†æ•°ï¼Œæ‰€æœ‰è®°å½•å°†ä½¿ç”¨é»˜è®¤å€¼0.0")
            
            self.logger.info(f"æ··åˆåŠ è½½å®Œæˆï¼Œæ€»è®¡ {len(raw_records)} æ¡è®°å½•")
            return raw_records
            
        except Exception as e:
            self.logger.error(f"æ··åˆåŠ è½½å¤±è´¥: {e}")
            self.logger.info("å°è¯•å›é€€åˆ°MySQLæ¨¡å¼...")
            try:
                return await self._load_records_from_mysql(limit)
            except Exception as mysql_error:
                self.logger.error(f"MySQLå›é€€ä¹Ÿå¤±è´¥: {mysql_error}")
                return []

    async def _load_pagerank_mapping(self) -> Dict[str, float]:
        """ä»MySQLåŠ è½½PageRankåˆ†æ•°"""
        pagerank_map = {}
        query = "SELECT original_url, pagerank_score FROM link_graph"
        try:
            # ä½¿ç”¨æ­£ç¡®çš„å‡½æ•°å execute_custom_query
            records = await db_core.execute_custom_query(query, fetch='all')
            if records:
                pagerank_map = {rec['original_url']: rec['pagerank_score'] for rec in records}
            self.logger.info(f"âœ… æˆåŠŸä»MySQLåŠ è½½ {len(pagerank_map)} æ¡PageRankè®°å½•")
            return pagerank_map
            
        except Exception as e:
            self.logger.warning(f"åŠ è½½PageRankåˆ†æ•°æ—¶å‡ºé”™: {e}")
            return {}

    async def _chunk_documents_if_enabled(self, records: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å¯é€‰çš„æ–‡æ¡£åˆ†å—åŠŸèƒ½"""
        if not self.enable_chunking or not records:
            return records
        
        print("ğŸ“„ è¿›è¡Œæ–‡æ¡£åˆ†å—...")
        self.logger.info(f"å¼€å§‹åˆ†å— {len(records)} ä¸ªæ–‡æ¡£...")
        
        from etl.processors.chunk_cache import chunk_documents_cached
        from llama_index.core.schema import TextNode
        
        # å°†è®°å½•è½¬æ¢ä¸ºTextNode
        doc_nodes = []
        for record in records:
            content = record.get('content', '')
            title = record.get('title', '')
            full_text = f"{title}\n{content}" if title else content
            
            node = TextNode(
                text=full_text,
                metadata=record
            )
            doc_nodes.append(node)
        
        # ä½¿ç”¨ç¼“å­˜è¿›è¡Œåˆ†å—
        chunked_nodes = await chunk_documents_cached(
            doc_nodes=doc_nodes,
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            force_refresh=False,
            show_progress=True
        )
        
        # å°†åˆ†å—èŠ‚ç‚¹è½¬æ¢å›è®°å½•æ ¼å¼
        chunked_records = []
        for i, node in enumerate(chunked_nodes):
            record = node.metadata.copy()
            record['content'] = node.text  # ä½¿ç”¨åˆ†å—åçš„æ–‡æœ¬
            # ä¸ºåˆ†å—åˆ›å»ºå”¯ä¸€ID
            original_id = record.get('id', i)
            record['id'] = f"{original_id}_chunk_{i}"
            record['chunk_id'] = i  # æ·»åŠ åˆ†å—æ ‡è¯†
            record['original_id'] = original_id  # ä¿ç•™åŸå§‹ID
            chunked_records.append(record)
        
        self.logger.info(f"åˆ†å—å®Œæˆï¼Œä» {len(records)} ä¸ªæ–‡æ¡£ç”Ÿæˆ {len(chunked_records)} ä¸ªåˆ†å—")
        return chunked_records

    async def _index_documents_from_source(self, es_client: AsyncElasticsearch, limit: int = None, batch_size: int = 1000, data_source: str = "raw_files") -> Dict[str, Any]:
        """æ ¹æ®æ•°æ®æºç´¢å¼•æ–‡æ¡£åˆ°Elasticsearch"""
        try:
            # æ ¹æ®æ•°æ®æºç±»å‹åŠ è½½æ•°æ®
            if data_source == "raw_files":
                print("ğŸ“ æ··åˆæ¨¡å¼ï¼šä»åŸå§‹æ–‡ä»¶+PageRankæ•°æ®...")
                records = await self._load_records_hybrid(limit)
            elif data_source == "mysql":
                print("ğŸ“Š ä»MySQLæ•°æ®åº“åŠ è½½æ•°æ®...")
                records = await self._load_records_from_mysql(limit)
            elif data_source == "raw_only":
                print("ğŸ“ ä»…ä»åŸå§‹JSONæ–‡ä»¶åŠ è½½æ•°æ®...")
                records = await self._load_records_from_raw_files(limit)
            else:
                print("ğŸ“ é»˜è®¤æ··åˆæ¨¡å¼ï¼šä»åŸå§‹æ–‡ä»¶+PageRankæ•°æ®...")
                records = await self._load_records_hybrid(limit)
            
            if not records:
                return {"total_records": 0, "success": True, "indexed": 0, "errors": 0, "message": "æ²¡æœ‰æ‰¾åˆ°æ•°æ®"}
            
            # å¯é€‰çš„æ–‡æ¡£åˆ†å—
            records = await self._chunk_documents_if_enabled(records)
            
            self.logger.info(f"å‡†å¤‡ç´¢å¼• {len(records)} æ¡è®°å½•")
            
            # æ‰¹é‡ç´¢å¼•ï¼ˆå¸¦è¿›åº¦æ¡ï¼‰
            print("ğŸ“¤ ç´¢å¼•æ–‡æ¡£åˆ°Elasticsearch...")
            success_count = 0
            error_count = 0
            
            async def async_doc_generator():
                """å¼‚æ­¥æ–‡æ¡£ç”Ÿæˆå™¨ï¼Œç”¨äºæ‰¹é‡ç´¢å¼•"""
                for record in records:
                    try:
                        doc = {
                            "_index": self.index_name,
                            "_id": record.get('id'),
                            "_source": {
                                "source_id": record.get('source_id'),
                                "id": record.get('id'),
                                "original_url": record.get('original_url', ''),
                                "title": record.get('title', ''),
                                "content": record.get('content', ''),
                                "author": record.get('author', ''),
                                "publish_time": record.get('publish_time'),
                                "platform": record.get('platform', ''),
                                "pagerank_score": record.get('pagerank_score', 0.0)
                            }
                        }
                        yield doc
                        
                    except Exception as e:
                        self.logger.warning(f"å¤„ç†è®°å½•æ—¶å‡ºé”™: {e}")
                        continue
            
            # æ‰§è¡Œå¼‚æ­¥æ‰¹é‡ç´¢å¼•ï¼ˆå¸¦è¿›åº¦æ¡å’Œæ€§èƒ½ä¼˜åŒ–ï¼‰
            with tqdm(total=len(records), desc="ç´¢å¼•åˆ°ES", unit="æ–‡æ¡£") as pbar:
                async for ok, action_result in helpers.async_streaming_bulk(
                    es_client, 
                    async_doc_generator(), 
                    chunk_size=min(batch_size, 100),  # é™åˆ¶æ‰¹å¤§å°ï¼Œé¿å…è¿‡è½½
                    max_chunk_bytes=10*1024*1024,     # 10MBæœ€å¤§å—å¤§å°
                    request_timeout=60,                # å¢åŠ è¯·æ±‚è¶…æ—¶
                    max_retries=3,                     # å¢åŠ é‡è¯•æ¬¡æ•°
                    initial_backoff=1,                 # åˆå§‹é€€é¿æ—¶é—´
                    max_backoff=60,                    # æœ€å¤§é€€é¿æ—¶é—´
                    raise_on_error=False,
                    raise_on_exception=False
                ):
                    if ok:
                        success_count += 1
                    else:
                        error_count += 1
                        # è®°å½•å…·ä½“é”™è¯¯ä¿¡æ¯ï¼ˆä½†ä¸è®°å½•å¤ªå¤šï¼‰
                        if error_count <= 10:
                            self.logger.debug(f"ç´¢å¼•é”™è¯¯: {action_result}")
                    pbar.update(1)
                    pbar.set_postfix({'æˆåŠŸ': success_count, 'å¤±è´¥': error_count})
                    
                    # æ·»åŠ çŸ­æš‚å»¶è¿Ÿä»¥å‡å°‘ESå‹åŠ›
                    if (success_count + error_count) % 1000 == 0:
                        await asyncio.sleep(0.1)

            self.logger.info(f"ç´¢å¼•å®Œæˆ: {success_count} æˆåŠŸ, {error_count} å¤±è´¥")
            print("âœ… Elasticsearchç´¢å¼•æ„å»ºå®Œæˆ!")
            
            return {
                "total_records": len(records),
                "success": True,
                "indexed": success_count,
                "errors": error_count,
                "message": f"æˆåŠŸç´¢å¼• {success_count} æ¡è®°å½•, {error_count} æ¡å¤±è´¥"
            }

        except Exception as e:
            self.logger.error(f"ç´¢å¼•æ–‡æ¡£åˆ°Elasticsearchæ—¶å‡ºé”™: {e}")
            return {
                "total_records": 0, 
                "success": False, 
                "indexed": 0,
                "errors": 0,
                "error": str(e),
                "message": f"ç´¢å¼•æ–‡æ¡£å¤±è´¥: {str(e)}"
            }

    async def _test_data_loading_from_source(self, limit: int = None, data_source: str = "raw_files") -> Dict[str, Any]:
        """æµ‹è¯•æ¨¡å¼ä¸‹æ ¹æ®æ•°æ®æºä»…åŠ è½½æ•°æ®ï¼Œä¸è¿›è¡Œç´¢å¼•"""
        try:
            # æ ¹æ®æ•°æ®æºç±»å‹åŠ è½½æ•°æ®
            if data_source == "raw_files":
                records = await self._load_records_hybrid(limit)
            elif data_source == "mysql":
                records = await self._load_records_from_mysql(limit)
            elif data_source == "raw_only":
                records = await self._load_records_from_raw_files(limit)
            else:
                records = await self._load_records_hybrid(limit)
            
            # å¯é€‰çš„æ–‡æ¡£åˆ†å—
            records = await self._chunk_documents_if_enabled(records)
            
            self.logger.info(f"æµ‹è¯•æ¨¡å¼ï¼šæˆåŠŸä»{data_source}åŠ è½½ {len(records)} æ¡è®°å½•")
            
            return {
                "total_records": len(records),
                "success": True,
                "indexed": 0,
                "errors": 0,
                "message": f"æµ‹è¯•æ¨¡å¼ï¼šæˆåŠŸåŠ è½½ {len(records)} æ¡è®°å½•ï¼Œè·³è¿‡ç´¢å¼•"
            }
            
        except Exception as e:
            self.logger.error(f"æµ‹è¯•æ¨¡å¼ä¸‹åŠ è½½æ•°æ®å¤±è´¥: {e}")
            return {"total_records": 0, "success": False, "message": f"æµ‹è¯•æ¨¡å¼ä¸‹åŠ è½½æ•°æ®å¤±è´¥: {e}"}

    async def _index_documents(self, es_client: AsyncElasticsearch, limit: int = None, batch_size: int = 1000) -> Dict[str, Any]:
        """ç´¢å¼•æ–‡æ¡£åˆ°Elasticsearchï¼ˆå‘åå…¼å®¹æ–¹æ³•ï¼‰"""
        return await self._index_documents_from_source(es_client, limit, batch_size, "mysql")

    async def _test_data_loading(self, limit: int = None) -> Dict[str, Any]:
        """æµ‹è¯•æ¨¡å¼ä¸‹ä»…åŠ è½½æ•°æ®ï¼Œä¸è¿›è¡Œç´¢å¼•ï¼ˆå‘åå…¼å®¹æ–¹æ³•ï¼‰"""
        return await self._test_data_loading_from_source(limit, "mysql")

    def _is_valid_date(self, date_str) -> bool:
        """éªŒè¯æ—¥æœŸå­—ç¬¦ä¸²æ˜¯å¦æœ‰æ•ˆ"""
        if not date_str or not isinstance(date_str, str):
            return False
        try:
            # å°è¯•è§£æå¤šç§æ ¼å¼
            for fmt in ["%Y-%m-%d %H:%M:%S", "%Y-%m-%d"]:
                datetime.strptime(date_str, fmt)
            return True
        except ValueError:
            return False

    async def validate_index(self) -> Dict[str, Any]:
        """
        å¼‚æ­¥éªŒè¯ç´¢å¼•çš„å¥åº·çŠ¶å†µå’Œå†…å®¹ã€‚

        Returns:
            Dict[str, Any]: åŒ…å«ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯å’Œç¤ºä¾‹æ–‡æ¡£çš„å­—å…¸ã€‚
        """
        self.logger.info(f"å¼€å§‹éªŒè¯Elasticsearchç´¢å¼•: {self.index_name}")
        es_client = await self._init_es_client()
        if not es_client:
            return {"error": "æ— æ³•è¿æ¥åˆ°Elasticsearch"}

        try:
            # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
            if not await es_client.indices.exists(index=self.index_name):
                self.logger.warning(f"ç´¢å¼• '{self.index_name}' ä¸å­˜åœ¨ã€‚")
                return {"status": "missing", "index_name": self.index_name}

            # è·å–æ–‡æ¡£æ€»æ•°
            count_response = await es_client.count(index=self.index_name)
            doc_count = count_response.get("count", 0)

            # è·å–ä¸€äº›ç¤ºä¾‹æ–‡æ¡£
            search_response = await es_client.search(
                index=self.index_name,
                body={"query": {"match_all": {}}, "size": 5}
            )
            sample_docs = [hit["_source"] for hit in search_response["hits"]["hits"]]

            self.logger.info(f"ç´¢å¼• '{self.index_name}' éªŒè¯æˆåŠŸï¼ŒåŒ…å« {doc_count} ä¸ªæ–‡æ¡£ã€‚")
            
            return {
                "status": "ok",
                "index_name": self.index_name,
                "document_count": doc_count,
                "sample_documents": sample_docs,
            }
        except Exception as e:
            self.logger.error(f"éªŒè¯ç´¢å¼•æ—¶å‡ºé”™: {e}")
            return {"status": "error", "error_message": str(e)}
        finally:
            # ç¡®ä¿å®¢æˆ·ç«¯è¿æ¥è¢«æ­£ç¡®å…³é—­
            if es_client:
                try:
                    await es_client.close()
                except Exception as close_error:
                    self.logger.debug(f"å…³é—­ESå®¢æˆ·ç«¯æ—¶å‡ºé”™: {close_error}")


# å‘åå…¼å®¹çš„å‡½æ•°æ¥å£
async def build_elasticsearch_index(
    index_name: str = None,
    es_host: str = None,
    es_port: int = None,
    limit: int = None,
    batch_size: int = 1000,
    recreate_index: bool = True,
    data_source: str = "raw_files"
) -> Dict[str, Any]:
    """
    æ„å»ºElasticsearchç´¢å¼•ï¼ˆå‘åå…¼å®¹æ¥å£ï¼‰
    
    Args:
        data_source: æ•°æ®æºç±»å‹ ("raw_files"=æ··åˆæ¨¡å¼, "mysql"=ä»…MySQL, "raw_only"=ä»…åŸå§‹æ–‡ä»¶)
    """
    config = Config()
    
    # è®¾ç½®é…ç½®å‚æ•°
    indexer_config = {
        'etl.data.elasticsearch.index': index_name or config.get('etl.data.elasticsearch.index'),
        'etl.data.elasticsearch.host': es_host or config.get('etl.data.elasticsearch.host'),
        'etl.data.elasticsearch.port': es_port or config.get('etl.data.elasticsearch.port')
    }
    
    # åˆ›å»ºç´¢å¼•æ„å»ºå™¨
    indexer = ElasticsearchIndexer(indexer_config)
    
    # æ„å»ºç´¢å¼•
    return await indexer.build_indexes(limit=limit, batch_size=batch_size, recreate_index=recreate_index, data_source=data_source) 