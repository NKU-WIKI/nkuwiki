import sys
from pathlib import Path
sys.path.append(str(Path(__file__).resolve().parent.parent.parent))
from etl.crawler import (
    crawler_logger, config, proxy_pool, default_user_agents, 
    default_timezone, default_locale, RAW_PATH
)
from etl.utils.const import domain_source_map, nankai_url_maps, nankai_start_urls
from etl.crawler.base_crawler import BaseCrawler
from etl.utils.date import parse_date
from etl.utils.file import clean_filename
from tqdm import tqdm
from typing import Any, List, Dict, Set
from pathlib import Path
import asyncio
import json
import time
import re
import hashlib
from datetime import datetime
from urllib.parse import urlparse, urljoin
try:
    import requests
    import aiohttp
    from bs4 import BeautifulSoup
except ImportError as e:
    print(f"ç¼ºå°‘ä¾èµ–åŒ…: {e}")
    print("è¯·è¿è¡Œ: pip install requests aiohttp beautifulsoup4 lxml")
    sys.exit(1)

class WebpageCrawler(BaseCrawler):
    """ç½‘é¡µçˆ¬è™«ï¼Œç»§æ‰¿BaseCrawler
    
    Attributes:
        start_urls: èµ·å§‹URLåˆ—è¡¨
        debug: è°ƒè¯•æ¨¡å¼å¼€å…³
        headless: æ˜¯å¦ä½¿ç”¨æ— å¤´æµè§ˆå™¨æ¨¡å¼
        use_proxy: æ˜¯å¦ä½¿ç”¨ä»£ç†
        max_depth: æœ€å¤§çˆ¬å–æ·±åº¦
        max_pages: æœ€å¤§çˆ¬å–é¡µé¢æ•°
    """
    def __init__(self, start_urls: List[str], tag: str = "nku", debug: bool = False, 
                 headless: bool = True, use_proxy: bool = False, max_depth: int = 3, max_pages: int = 1000) -> None:
        """åˆå§‹åŒ–ç½‘é¡µçˆ¬è™«
        
        Args:
            start_urls: èµ·å§‹URLåˆ—è¡¨
            tag: æ ‡ç­¾
            debug: è°ƒè¯•æ¨¡å¼å¼€å…³
            headless: æ˜¯å¦ä½¿ç”¨æ— å¤´æµè§ˆå™¨æ¨¡å¼
            use_proxy: æ˜¯å¦ä½¿ç”¨ä»£ç†
            max_depth: æœ€å¤§çˆ¬å–æ·±åº¦
            max_pages: æœ€å¤§çˆ¬å–é¡µé¢æ•°
        """
        self.platform = "website"  # ä¿®æ”¹ä¸ºwebsiteä»¥å…¼å®¹åŸæœ‰ç›®å½•ç»“æ„
        self.tag = tag
        self.content_type = "webpage"
        super().__init__(debug, headless, use_proxy, tag)
        
        if not start_urls:
            self.logger.error("èµ·å§‹URLåˆ—è¡¨ä¸èƒ½ä¸ºç©º")
            raise ValueError("èµ·å§‹URLåˆ—è¡¨ä¸èƒ½ä¸ºç©º")
            
        self.start_urls = start_urls
        self.max_depth = max_depth
        self.max_pages = max_pages
        self.visited_urls: Set[str] = set()
        self.crawled_count = 0
        
        # å…è®¸çš„åŸŸååˆ—è¡¨ï¼ˆæå–è‡ªèµ·å§‹URLå¹¶æ‰©å±•ç›¸å…³åŸŸåï¼‰
        self.allowed_domains = set()
        for url in start_urls:
            parsed = urlparse(url)
            if parsed.netloc:
                self.allowed_domains.add(parsed.netloc)
                
        # æ·»åŠ å—å¼€å¤§å­¦ç›¸å…³çš„æ‰€æœ‰åŸŸå
        additional_domains = {
            'nankai.edu.cn', 'www.nankai.edu.cn', 'news.nankai.edu.cn',
            'jwc.nankai.edu.cn', 'graduate.nankai.edu.cn', 'career.nankai.edu.cn',
            'std.nankai.edu.cn', 'rsc.nankai.edu.cn', 'international.nankai.edu.cn',
            'lib.nankai.edu.cn', 'hq.nankai.edu.cn', 'cwc.nankai.edu.cn',
            'zzb.nankai.edu.cn', 'gh.nankai.edu.cn', 'guard.nankai.edu.cn',
            'museum.nankai.edu.cn', 'nkuaa.nankai.edu.cn', 'teda.nankai.edu.cn',
            'en.nankai.edu.cn', 'yzb.nankai.edu.cn', 'tyb.nankai.edu.cn',
            'fy.nankai.edu.cn', 'jsfz.nankai.edu.cn', 'nkuef.nankai.edu.cn',
            'nkzbb.nankai.edu.cn', 'nkjd.nankai.edu.cn', 'kexie.nankai.edu.cn',
        }
        self.allowed_domains.update(additional_domains)
        
        # ç§»é™¤ç‹¬ç«‹çš„å¿«ç…§ç›®å½•ï¼Œå¿«ç…§å°†ä¿å­˜åœ¨å„æ–‡ç« ç›®å½•ä¸‹
        
        # é“¾æ¥å›¾æ•°æ®åº“å°†åœ¨crawlæ–¹æ³•ä¸­å¼‚æ­¥åˆå§‹åŒ–
        
        depth_info = "æ— é™åˆ¶" if self.max_depth == -1 else str(self.max_depth)
        self.logger.info(f"åˆå§‹åŒ–ç½‘é¡µçˆ¬è™«ï¼Œèµ·å§‹URLæ•°é‡: {len(self.start_urls)}, æœ€å¤§æ·±åº¦: {depth_info}, å…è®¸åŸŸå: {self.allowed_domains}")

    def is_valid_url(self, url: str) -> bool:
        """æ£€æŸ¥URLæ˜¯å¦æœ‰æ•ˆä¸”åœ¨å…è®¸çš„åŸŸåå†…"""
        try:
            parsed_url = urlparse(url)
            
            # åŸºæœ¬URLæ ¼å¼æ£€æŸ¥
            if not (parsed_url.scheme in ['http', 'https'] and parsed_url.netloc):
                return False
                
            # è·³è¿‡ç‰¹æ®Šåè®®
            if url.startswith(('javascript:', 'mailto:', 'tel:', 'ftp:')):
                return False
                
            # åŸŸåæ£€æŸ¥ - å…è®¸å­åŸŸå
            netloc = parsed_url.netloc
            domain_allowed = False
            for allowed_domain in self.allowed_domains:
                if netloc == allowed_domain or netloc.endswith('.' + allowed_domain):
                    domain_allowed = True
                    break
                    
            if not domain_allowed:
                return False
                
            # æ£€æŸ¥æ˜¯å¦å·²è®¿é—®ï¼ˆç”¨äºé¿å…é‡å¤ï¼‰
            if url in self.visited_urls:
                return False
                
            # æ£€æŸ¥URLè·¯å¾„æ˜¯å¦åŒ…å«æ˜æ˜¾çš„éå†…å®¹æ ‡è¯†ï¼ˆæ”¾å®½é™åˆ¶ï¼‰
            path = parsed_url.path.lower()
            exclude_patterns = ['/admin', '/login', '/logout', '/api/']
            if any(exclude in path for exclude in exclude_patterns):
                return False
                
            return True
        except:
            return False

    def is_content_url(self, url: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºå†…å®¹é¡µé¢URL - å¤§å¹…ä¼˜åŒ–è¯†åˆ«é€»è¾‘ï¼Œæé«˜å†…å®¹å‘ç°ç‡"""
        # æ–‡æ¡£æ–‡ä»¶ï¼ˆæ‰©å±•æ›´å¤šæ ¼å¼ï¼‰
        doc_extensions = ['.pdf', '.docx', '.doc', '.xlsx', '.xls', '.ppt', '.pptx', 
                         '.txt', '.rtf', '.zip', '.rar', '.7z']
        if any(url.lower().endswith(ext) for ext in doc_extensions):
            return True
            
        # ç‰¹å®šçš„æ‹›è˜å’Œä¼ä¸šä¿¡æ¯URLï¼ˆåŸscrapyçˆ¬è™«é€»è¾‘ï¼‰
        if 'https://career.nankai.edu.cn/company/index/id' in url:
            return True
            
        # æ‰©å±•ç‰¹æ®ŠURLåˆ—è¡¨
        special_url_patterns = [
            r'career\.nankai\.edu\.cn/download/',
            r'career\.nankai\.edu\.cn/zpxx/',
            r'career\.nankai\.edu\.cn/jydt/',
            r'career\.nankai\.edu\.cn/jyzd/',
            r'jwc\.nankai\.edu\.cn/info/',
            r'graduate\.nankai\.edu\.cn/info/',
        ]
        for pattern in special_url_patterns:
            if re.search(pattern, url):
                return True
            
        # ä¸¥æ ¼æ’é™¤æ˜ç¡®çš„éå†…å®¹é¡µé¢
        strict_exclude_patterns = [
            r'/search[/?]',         # æœç´¢é¡µé¢
            r'\?page=\d+&',         # åˆ†é¡µå‚æ•°ä¸­çš„ä¸€éƒ¨åˆ†
            r'/(login|logout|admin|register)/?$', # ç®¡ç†å’Œç™»å½•é¡µé¢
            r'/sitemap\.xml',       # ç«™ç‚¹åœ°å›¾
            r'/robots\.txt',        # robotsæ–‡ä»¶
            r'\.css$',              # CSSæ–‡ä»¶
            r'\.js$',               # JavaScriptæ–‡ä»¶
            r'\.png$|\.jpg$|\.jpeg$|\.gif$|\.bmp$|\.svg$', # å›¾ç‰‡æ–‡ä»¶
        ]
        
        for pattern in strict_exclude_patterns:
            if re.search(pattern, url, re.IGNORECASE):
                return False
        
        # å¤§å¹…æ‰©å±•å†…å®¹é¡µé¢è¯†åˆ«æ¨¡å¼
        content_indicators = [
            # æ–‡ä»¶æ‰©å±•å
            r'\.html?$',                      # HTMLæ–‡ä»¶
            r'\.htm$',                        # HTMæ–‡ä»¶  
            r'\.shtml$',                      # SHTMLæ–‡ä»¶
            r'\.php(\?|$)',                   # PHPæ–‡ä»¶
            r'\.asp(\?|$)',                   # ASPæ–‡ä»¶
            r'\.jsp(\?|$)',                   # JSPæ–‡ä»¶
            
            # æ•°å­—IDæ¨¡å¼ï¼ˆæ›´å®½æ³›ï¼‰
            r'/\d+',                          # ä»»ä½•åŒ…å«æ•°å­—çš„è·¯å¾„
            r'\d{3,}',                        # 3ä½ä»¥ä¸Šæ•°å­—
            r'id=\d+',                        # URLå‚æ•°ä¸­çš„ID
            r'articleid=\d+',                 # æ–‡ç« ID
            r'newsid=\d+',                    # æ–°é—»ID
            
            # ç›®å½•ç»“æ„æ¨¡å¼
            r'/info/',                        # infoç›®å½•
            r'/news/',                        # newsç›®å½•
            r'/notice/',                      # noticeç›®å½•
            r'/article/',                     # articleç›®å½•
            r'/post/',                        # postç›®å½•
            r'/content/',                     # contentç›®å½•
            r'/detail/',                      # detailç›®å½•
            r'/view/',                        # viewç›®å½•
            r'/show/',                        # showç›®å½•
            r'/read/',                        # readç›®å½•
            r'/page/',                        # pageç›®å½•
            r'/item/',                        # itemç›®å½•
            
            # å—å¼€å¤§å­¦ç‰¹æœ‰æ¨¡å¼
            r'page\.htm',                     # page.htm
            r'news-detail',                   # æ–°é—»è¯¦æƒ…
            r'/c\d+a\d+/',                    # å—å¼€ç‰¹æœ‰çš„c+aæ¨¡å¼
            r'/_upload/',                     # ä¸Šä¼ æ–‡ä»¶è·¯å¾„
            r'/system/',                      # ç³»ç»Ÿè·¯å¾„
            
            # å¤–éƒ¨é“¾æ¥
            r'mp\.weixin\.qq\.com',           # å¾®ä¿¡å…¬ä¼—å·é“¾æ¥
            
            # å†…å®¹å…³é”®è¯
            r'/(announcement|bulletin|notification|notice)/', # å…¬å‘Šã€é€šçŸ¥
            r'/(academic|research|science|study)/',           # å­¦æœ¯ã€ç ”ç©¶
            r'/(student|teacher|faculty|professor)/',         # å­¦ç”Ÿã€æ•™å¸ˆ
            r'/(event|activity|conference|seminar)/',         # äº‹ä»¶ã€æ´»åŠ¨ã€ä¼šè®®
            r'/(publication|paper|journal)/',                 # å‡ºç‰ˆç‰©ã€è®ºæ–‡
            r'/(course|curriculum|education)/',               # è¯¾ç¨‹ã€æ•™è‚²
            r'/(lab|laboratory|center|institute)/',           # å®éªŒå®¤ã€ä¸­å¿ƒã€ç ”ç©¶æ‰€
            r'/(download|file|document)/',                    # ä¸‹è½½ã€æ–‡ä»¶ã€æ–‡æ¡£
            
            # ä¸­æ–‡å…³é”®è¯æ‹¼éŸ³
            r'/(xinwen|tongzhi|gonggao|xuesheng|jiaoshi)/',   # æ–°é—»ã€é€šçŸ¥ã€å…¬å‘Šã€å­¦ç”Ÿã€æ•™å¸ˆ
            r'/(keyan|xueshu|huodong|huiyi)/',                # ç§‘ç ”ã€å­¦æœ¯ã€æ´»åŠ¨ã€ä¼šè®®
            r'/(zhaosheng|jiuye|jiaoxue)/',                   # æ‹›ç”Ÿã€å°±ä¸šã€æ•™å­¦
            
            # å—å¼€ç½‘ç«™å¸¸è§æ ç›®ç¼©å†™
            r'/(xwzx|tzgg|xwdt|kydt|xsdt)/',                  # æ–°é—»ä¸­å¿ƒã€é€šçŸ¥å…¬å‘Šã€æ–°é—»åŠ¨æ€ã€ç§‘ç ”åŠ¨æ€ã€å­¦æœ¯åŠ¨æ€
            r'/(ywsd|mtjj|zhxw|tbft)/',                       # è¦é—»é€Ÿé€’ã€åª’ä½“èšç„¦ã€ç»¼åˆæ–°é—»ã€å›¾ç‰‡æŠ¥é“
            r'/(nkyw|xshd|rcpy|kxyj)/',                       # å—å¼€è¦é—»ã€å­¦æœ¯æ´»åŠ¨ã€äººæ‰åŸ¹å…»ã€ç§‘å­¦ç ”ç©¶
        ]
        
        # å¦‚æœURLåŒ¹é…ä»»ä½•å†…å®¹æŒ‡ç¤ºå™¨ï¼Œè®¤ä¸ºæ˜¯å†…å®¹é¡µé¢
        for pattern in content_indicators:
            if re.search(pattern, url, re.IGNORECASE):
                return True
        
        # è·¯å¾„ç»“æ„åˆ†æ - æ›´å®½æ¾çš„æ¡ä»¶
        parsed_url = urlparse(url)
        path = parsed_url.path
        query = parsed_url.query
        
        if path and len(path) > 1:
            # æ’é™¤æ˜ç¡®çš„ç›®å½•é¡µé¢
            directory_patterns = [
                r'^/(index|home|main|default)/?$',
                r'^/(list|category|tag|archive)/?$',
                r'^/[^/]+/?$',  # åªæœ‰ä¸€çº§è·¯å¾„çš„å¯èƒ½æ˜¯ç›®å½•
            ]
            
            is_directory = False
            for pattern in directory_patterns:
                if re.search(pattern, path, re.IGNORECASE):
                    is_directory = True
                    break
            
            if not is_directory:
                # å¦‚æœè·¯å¾„åŒ…å«å¤šä¸ªæ®µæˆ–æœ‰æŸ¥è¯¢å‚æ•°ï¼Œå¾ˆå¯èƒ½æ˜¯å†…å®¹é¡µé¢
                path_segments = [seg for seg in path.split('/') if seg]
                if len(path_segments) >= 2 or query:
                    return True
                    
                # å¦‚æœè·¯å¾„åŒ…å«ç‰¹æ®Šå­—ç¬¦æˆ–æ•°å­—ï¼Œä¹Ÿå¯èƒ½æ˜¯å†…å®¹é¡µé¢
                if re.search(r'[_\-\d]', path):
                    return True
        
        # æŸ¥è¯¢å‚æ•°åˆ†æ
        if query:
            # å¸¸è§çš„å†…å®¹é¡µé¢æŸ¥è¯¢å‚æ•°
            content_params = ['id', 'aid', 'pid', 'nid', 'cid', 'articleid', 'newsid']
            for param in content_params:
                if param in query.lower():
                    return True
        
        return False

    def extract_links(self, html_content: str, base_url: str) -> List[str]:
        """ä»HTMLå†…å®¹ä¸­æå–é“¾æ¥ - ä¼˜åŒ–é“¾æ¥å‘ç°ç­–ç•¥"""
        soup = BeautifulSoup(html_content, 'html.parser')
        links = []
        
        # æå–aæ ‡ç­¾é“¾æ¥
        for a_tag in soup.find_all('a', href=True):
            href = a_tag['href'].strip()
            
            # è·³è¿‡æ— æ•ˆé“¾æ¥
            if not href or href == '#' or href.startswith('javascript:') or href.startswith('mailto:'):
                continue
                
            # è½¬æ¢ä¸ºç»å¯¹URL
            if not href.startswith('http'):
                href = urljoin(base_url, href)
                
            if self.is_valid_url(href):
                links.append(href)
        
        # æå–frameå’Œiframeä¸­çš„é“¾æ¥
        for frame_tag in soup.find_all(['frame', 'iframe'], src=True):
            src = frame_tag['src'].strip()
            if src and not src.startswith('javascript:'):
                if not src.startswith('http'):
                    src = urljoin(base_url, src)
                if self.is_valid_url(src):
                    links.append(src)
        
        # æå–é¡µé¢ä¸­çš„åˆ†é¡µé“¾æ¥ï¼ˆå¸¸è§æ¨¡å¼ï¼‰
        page_patterns = [
            r'href=["\']([^"\']+page[^"\']*)["\']',
            r'href=["\']([^"\']+\d+\.html?)["\']',
            r'href=["\']([^"\']+list[^"\']*)["\']',
        ]
        
        for pattern in page_patterns:
            matches = re.findall(pattern, html_content, re.IGNORECASE)
            for match in matches:
                if not match.startswith('http'):
                    match = urljoin(base_url, match)
                if self.is_valid_url(match):
                    links.append(match)
        
        # ç‰¹åˆ«æœç´¢å—å¼€ç½‘ç«™å¸¸è§çš„é“¾æ¥æ¨¡å¼
        nankai_patterns = [
            r'href=["\']([^"\']*nankai\.edu\.cn[^"\']*)["\']',
            r'href=["\']([^"\']*info/\d+[^"\']*)["\']',
            r'href=["\']([^"\']*news/\d+[^"\']*)["\']',
            r'href=["\']([^"\']*article/\d+[^"\']*)["\']',
        ]
        
        for pattern in nankai_patterns:
            matches = re.findall(pattern, html_content, re.IGNORECASE)
            for match in matches:
                if not match.startswith('http'):
                    match = urljoin(base_url, match)
                if self.is_valid_url(match):
                    links.append(match)
        
        # ä»JavaScriptä»£ç ä¸­æå–é“¾æ¥ï¼ˆå¸¸è§äºåŠ¨æ€é¡µé¢ï¼‰
        js_link_patterns = [
            r'window\.open\(["\']([^"\']+)["\']',
            r'location\.href\s*=\s*["\']([^"\']+)["\']',
            r'url:\s*["\']([^"\']+)["\']',
        ]
        
        for pattern in js_link_patterns:
            matches = re.findall(pattern, html_content, re.IGNORECASE)
            for match in matches:
                if not match.startswith('http'):
                    match = urljoin(base_url, match)
                if self.is_valid_url(match):
                    links.append(match)
        
        # æå–meta refreshé‡å®šå‘é“¾æ¥
        meta_refresh = soup.find('meta', attrs={'http-equiv': re.compile(r'^refresh$', re.I)})
        if meta_refresh and meta_refresh.get('content'):
            content = meta_refresh['content']
            url_match = re.search(r'url=(.+)', content, re.IGNORECASE)
            if url_match:
                refresh_url = url_match.group(1).strip('\'"')
                if not refresh_url.startswith('http'):
                    refresh_url = urljoin(base_url, refresh_url)
                if self.is_valid_url(refresh_url):
                    links.append(refresh_url)
        
        # å»é‡å¹¶è¿”å›
        return list(set(links))

    def get_source_name(self, url: str) -> str:
        """æ ¹æ®URLç¡®å®šæ¥æºï¼ˆå­¦é™¢åç§°ï¼‰ - ä¸scrapyçˆ¬è™«é€»è¾‘ä¸€è‡´"""
        parsed_url = urlparse(url)
        netloc = parsed_url.netloc
        
        # æŸ¥æ‰¾åŒ¹é…çš„å­¦é™¢åç§°
        for college_name, college_url in nankai_url_maps.items():
            college_parsed = urlparse(college_url)
            if netloc == college_parsed.netloc:
                return college_name
                
        # ä½¿ç”¨å¯¼å…¥çš„åŸŸåæ˜ å°„å­—å…¸
        if netloc in domain_source_map:
            return domain_source_map[netloc]
                
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œè¿”å›åŸŸå
        return netloc

    def extract_content(self, html_content: str, url: str) -> Dict[str, Any]:
        """ä»HTMLå†…å®¹ä¸­æå–æ–‡ç« ä¿¡æ¯"""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # æå–æ ‡é¢˜
        title = ""
        if soup.title:
            title = soup.title.get_text().strip()
        elif soup.find('h1'):
            title = soup.find('h1').get_text().strip()
            
        # æå–å‘å¸ƒæ—¶é—´ - æ›´ç²¾ç¡®çš„æ—¶é—´æå–
        publish_time = ""
        
        # é¦–å…ˆä»URLä¸­æå–æ—¶é—´ï¼ˆå¦‚åŸscrapyçˆ¬è™«çš„get_datestr_from_urlï¼‰
        # åªåŒ¹é…çœŸæ­£çš„æ—¥æœŸæ ¼å¼ï¼š2024/01/01
        date_match = re.search(r"(\d{4})/(\d{2})/(\d{2})", url)
        if date_match:
            year, month, day = date_match.groups()
            # ä¸¥æ ¼éªŒè¯æ—¥æœŸåˆç†æ€§
            try:
                year_int = int(year)
                month_int = int(month)
                day_int = int(day)
                if (2000 <= year_int <= 2030 and 
                    1 <= month_int <= 12 and 
                    1 <= day_int <= 31):
                    publish_time = f"{year}-{month}-{day}"
            except ValueError:
                pass
        
        if not publish_time:
            # åŒ¹é…ç´§å‡‘æ ¼å¼ï¼š2024/0312ï¼Œä½†è¦éªŒè¯æœˆæ—¥
            date_match = re.search(r"(\d{4})/(\d{2})(\d{2})", url)
            if date_match:
                year, month, day = date_match.groups()
                try:
                    year_int = int(year)
                    month_int = int(month)
                    day_int = int(day)
                    if (2000 <= year_int <= 2030 and 
                        1 <= month_int <= 12 and 
                        1 <= day_int <= 31):
                        publish_time = f"{year}-{month}-{day}"
                except ValueError:
                    pass
        
        # å¦‚æœURLä¸­æ²¡æœ‰æ—¶é—´ï¼Œä»é¡µé¢å†…å®¹ä¸­æå–
        if not publish_time:
            time_patterns = [
                r'(\d{4})[-/](\d{1,2})[-/](\d{1,2})',
                r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥',
                r'å‘å¸ƒæ—¶é—´[ï¼š:]?\s*(\d{4})[-/](\d{1,2})[-/](\d{1,2})',
                r'æ—¶é—´[ï¼š:]?\s*(\d{4})[-/](\d{1,2})[-/](\d{1,2})',
                r'æ—¥æœŸ[ï¼š:]?\s*(\d{4})[-/](\d{1,2})[-/](\d{1,2})'
            ]
            
            page_text = soup.get_text()
            for pattern in time_patterns:
                match = re.search(pattern, page_text)
                if match:
                    groups = match.groups()
                    if len(groups) >= 3:
                        year, month, day = groups[:3]
                        # éªŒè¯æ—¥æœŸçš„åˆç†æ€§
                        try:
                            month = int(month)
                            day = int(day)
                            year = int(year)
                            if 1990 <= year <= 2030 and 1 <= month <= 12 and 1 <= day <= 31:
                                publish_time = f"{year}-{month:02d}-{day:02d}"
                                break
                        except ValueError:
                            continue
                    else:
                        publish_time = match.group(0)
                        break
        
        # æå–æ­£æ–‡å†…å®¹ - æ›´ç²¾ç¡®çš„å†…å®¹æå–
        content = ""
        # ç§»é™¤è„šæœ¬å’Œæ ·å¼æ ‡ç­¾
        for tag in soup(["script", "style", "nav", "header", "footer"]):
            tag.decompose()
            
        # å°è¯•æ‰¾åˆ°ä¸»è¦å†…å®¹åŒºåŸŸï¼ˆä¼˜å…ˆæŸ¥æ‰¾wp_articlecontentç±»ï¼Œè¿™æ˜¯å—å¼€ç½‘ç«™å¸¸ç”¨çš„ï¼‰
        main_content = (
            soup.find('div', class_='wp_articlecontent') or
            soup.find('div', class_=re.compile(r'content|article|main', re.I)) or
            soup.find('article') or
            soup.find('main') or
            soup.body
        )
        
        if main_content:
            content = main_content.get_text(separator='\n', strip=True)
            # æ¸…ç†å†…å®¹
            content = re.sub(r'\n\s*\n', '\n', content).strip()
            content = content.replace('\xa0', ' ')  # æ›¿æ¢ä¸é—´æ–­ç©ºæ ¼
            
        # ç¡®å®šæ¥æº - ä½¿ç”¨æ˜ å°„è¡¨
        source = self.get_source_name(url)
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºPDFç­‰æ–‡æ¡£
        file_url = None
        if any(url.lower().endswith(ext) for ext in ['.pdf', '.docx', '.doc', '.xlsx', '.xls']):
            file_url = url
            
        # æ£€æŸ¥é¡µé¢ä¸­æ˜¯å¦æœ‰PDFæ’­æ”¾å™¨ï¼ˆå—å¼€ç½‘ç«™ç‰¹æœ‰ï¼‰
        if not file_url and not content.replace(' ', ''):
            pdf_player = soup.find('div', class_='wp_pdf_player')
            if pdf_player and pdf_player.get('pdfsrc'):
                pdf_src = pdf_player.get('pdfsrc')
                if not pdf_src.startswith('http'):
                    parsed_url = urlparse(url)
                    base_url = f"{parsed_url.scheme}://{parsed_url.netloc}"
                    pdf_src = base_url + pdf_src
                file_url = pdf_src
        
        return {
            'title': title,
            'publish_time': publish_time,
            'content': content,
            'source': source,
            'url': url,
            'file_url': file_url
        }

    def save_snapshot(self, html_content: str, url: str, article_dir: Path) -> None:
        """ä¿å­˜ç½‘é¡µå¿«ç…§åˆ°æ–‡ç« ç›®å½•ä¸‹
        
        Args:
            html_content: HTMLå†…å®¹
            url: åŸå§‹URL
            article_dir: æ–‡ç« ç›®å½•è·¯å¾„
        """
        try:
            # ä¿å­˜ä¸ºåŸå§‹æ–‡ä»¶å.html
            snapshot_file = article_dir / "original.html"
            
            if isinstance(html_content, str):
                html_content = html_content.encode('utf-8')
                
            with open(snapshot_file, 'wb') as f:
                f.write(html_content)
                
            self.logger.debug(f"ç½‘é¡µå¿«ç…§å·²ä¿å­˜: {snapshot_file}")
        except Exception as e:
            self.logger.error(f"ä¿å­˜ç½‘é¡µå¿«ç…§å¤±è´¥: {e}")

    async def init_link_graph_db(self) -> None:
        """åˆå§‹åŒ–é“¾æ¥å›¾æ•°æ®åº“ï¼ˆä½¿ç”¨MySQLï¼‰"""
        try:
            from etl.load.db_core import execute_query
            
            # åˆ›å»ºé“¾æ¥å›¾è¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰- ä½¿ç”¨å·²åˆ›å»ºçš„è¡¨ç»“æ„
            await execute_query('''
                CREATE TABLE IF NOT EXISTS link_graph (
                    id bigint(20) NOT NULL AUTO_INCREMENT,
                    source_url varchar(255) NOT NULL COMMENT 'æºé¡µé¢URL',
                    target_url varchar(255) NOT NULL COMMENT 'ç›®æ ‡é¡µé¢URL',
                    anchor_text varchar(255) DEFAULT NULL COMMENT 'é“¾æ¥é”šæ–‡æœ¬',
                    link_type varchar(20) DEFAULT 'internal' COMMENT 'é“¾æ¥ç±»å‹ï¼šinternal-å†…éƒ¨é“¾æ¥, external-å¤–éƒ¨é“¾æ¥',
                    create_time datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT 'åˆ›å»ºæ—¶é—´',
                    update_time datetime NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT 'æ›´æ–°æ—¶é—´',
                    PRIMARY KEY (id),
                    UNIQUE KEY uk_source_target (source_url, target_url),
                    KEY idx_source_url (source_url),
                    KEY idx_target_url (target_url),
                    KEY idx_link_type (link_type)
                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='é¡µé¢é“¾æ¥å…³ç³»è¡¨'
            ''', fetch=False)
            
            # åˆ›å»ºPageRankåˆ†æ•°è¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰- ä½¿ç”¨å·²åˆ›å»ºçš„è¡¨ç»“æ„
            await execute_query('''
                CREATE TABLE IF NOT EXISTS pagerank_scores (
                    id bigint(20) NOT NULL AUTO_INCREMENT,
                    url varchar(255) NOT NULL COMMENT 'é¡µé¢URL',
                    pagerank_score double NOT NULL DEFAULT '0' COMMENT 'PageRankåˆ†æ•°',
                    in_degree int(11) NOT NULL DEFAULT '0' COMMENT 'å…¥åº¦ï¼ˆæŒ‡å‘è¯¥é¡µé¢çš„é“¾æ¥æ•°ï¼‰',
                    out_degree int(11) NOT NULL DEFAULT '0' COMMENT 'å‡ºåº¦ï¼ˆè¯¥é¡µé¢æŒ‡å‘å…¶ä»–é¡µé¢çš„é“¾æ¥æ•°ï¼‰',
                    calculation_date datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT 'PageRankè®¡ç®—æ—¶é—´',
                    create_time datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT 'åˆ›å»ºæ—¶é—´',
                    update_time datetime NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT 'æ›´æ–°æ—¶é—´',
                    PRIMARY KEY (id),
                    UNIQUE KEY uk_url (url),
                    KEY idx_pagerank_score (pagerank_score DESC),
                    KEY idx_calculation_date (calculation_date)
                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='PageRankåˆ†æ•°è¡¨'
            ''', fetch=False)
            
            self.logger.info("é“¾æ¥å›¾MySQLæ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"åˆå§‹åŒ–é“¾æ¥å›¾MySQLæ•°æ®åº“å¤±è´¥: {e}")

    async def save_link_graph(self, source_url: str, target_urls: List[str]) -> None:
        """ä¿å­˜é“¾æ¥å…³ç³»åˆ°MySQLæ•°æ®åº“
        
        Args:
            source_url: æºURL
            target_urls: ç›®æ ‡URLåˆ—è¡¨
        """
        if not target_urls:
            return
            
        try:
            from etl.load.db_core import execute_query
            
            # æ‰¹é‡æ’å…¥é“¾æ¥å…³ç³» - é€ä¸ªæ’å…¥ä»¥é¿å…é‡å¤
            for target_url in target_urls:
                try:
                    # ä½¿ç”¨INSERT IGNOREæ¥é¿å…é‡å¤æ’å…¥
                    await execute_query('''
                        INSERT IGNORE INTO link_graph (source_url, target_url, link_type)
                        VALUES (%s, %s, %s)
                    ''', [source_url, target_url, 'internal'], fetch=False)
                except Exception as insert_error:
                    self.logger.warning(f"æ’å…¥é“¾æ¥å…³ç³»å¤±è´¥: {source_url} -> {target_url}, é”™è¯¯: {insert_error}")
                    continue
            
            self.logger.debug(f"ä¿å­˜äº† {len(target_urls)} ä¸ªé“¾æ¥å…³ç³»åˆ°MySQLï¼ŒæºURL: {source_url[:50]}...")
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜é“¾æ¥å…³ç³»åˆ°MySQLå¤±è´¥: {e}")

    async def crawl_url(self, url: str, depth: int, session: 'aiohttp.ClientSession' = None) -> Dict[str, Any]:
        """çˆ¬å–å•ä¸ªURLï¼ˆå¹¶å‘ç‰ˆæœ¬ï¼‰"""
        # æ£€æŸ¥æ·±åº¦é™åˆ¶ï¼ˆ-1è¡¨ç¤ºæ— é™åˆ¶ï¼‰å’Œé¡µé¢æ•°é‡é™åˆ¶
        if ((self.max_depth != -1 and depth > self.max_depth) or 
            self.crawled_count >= self.max_pages):
            return {"url": url, "links": [], "article_data": None, "success": False}
            
        if url in self.visited_urls:
            return {"url": url, "links": [], "article_data": None, "success": False}
            
        self.visited_urls.add(url)
        
        try:
            if session:
                # ä½¿ç”¨aiohttpè¿›è¡Œæ›´å¿«çš„HTTPè¯·æ±‚
                async with session.get(url, timeout=15) as response:
                    if response.status == 200:
                        html_content = await response.text()
                    else:
                        raise Exception(f"HTTP {response.status}")
            else:
                # å›é€€åˆ°playwright
                await self.page.goto(url, wait_until='domcontentloaded', timeout=15000)
                html_content = await self.page.content()
            
            # æå–é“¾æ¥ç”¨äºè¿›ä¸€æ­¥çˆ¬å–
            links = self.extract_links(html_content, url)
            
            # æå–å†…å®¹
            article_data = None
            if self.is_content_url(url):
                article_data = self.extract_content(html_content, url)
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆå†…å®¹
                if article_data['title'] or article_data['content']:
                    article_data['html_content'] = html_content  # ä¿å­˜HTMLç”¨äºæ‰¹é‡å¤„ç†
                    self.crawled_count += 1
                    self.counter['scrape'] += 1
                    
            self.counter['visit'] += 1
            
            return {
                "url": url, 
                "links": links, 
                "article_data": article_data, 
                "success": True
            }
            
        except Exception as e:
            self.logger.debug(f"çˆ¬å–URLå¤±è´¥: {url}, é”™è¯¯: {e}")
            self.counter['error'] += 1
            return {"url": url, "links": [], "article_data": None, "success": False}

    def save_article(self, article_data: Dict[str, Any], html_content: str = None) -> None:
        """ä¿å­˜æ–‡ç« æ•°æ®ï¼ˆåŒ…æ‹¬HTMLå¿«ç…§ï¼‰
        
        Args:
            article_data: æ–‡ç« æ•°æ®å­—å…¸
            html_content: åŸå§‹HTMLå†…å®¹ï¼Œå¦‚æœæä¾›åˆ™ä¿å­˜ä¸ºå¿«ç…§
        """
        try:
            title = article_data.get('title', 'æœªçŸ¥æ ‡é¢˜')
            clean_title = clean_filename(title)
            
            # æ ¹æ®å‘å¸ƒæ—¶é—´ç»„ç»‡ç›®å½•ç»“æ„
            publish_time = article_data.get('publish_time', '')
            if publish_time:
                try:
                    # å°è¯•å¤šç§æ—¥æœŸæ ¼å¼
                    date_match = re.search(r'(\d{4})[-/](\d{1,2})[-/]\d{1,2}', publish_time)
                    if date_match:
                        year = date_match.group(1)
                        month = date_match.group(2).zfill(2)  # è¡¥é›¶åˆ°ä¸¤ä½æ•°
                        year_month = f"{year}{month}"
                    else:
                        # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°å®Œæ•´æ—¥æœŸï¼Œå°è¯•åŒ¹é…å¹´æœˆ
                        year_month_match = re.search(r'(\d{4})[-/](\d{1,2})', publish_time)
                        if year_month_match:
                            year = year_month_match.group(1)
                            month = year_month_match.group(2).zfill(2)
                            year_month = f"{year}{month}"
                        else:
                            year_month = datetime.now().strftime("%Y%m")
                except:
                    year_month = datetime.now().strftime("%Y%m")
            else:
                year_month = datetime.now().strftime("%Y%m")
                
            save_dir = self.data_dir / year_month
            save_dir.mkdir(exist_ok=True, parents=True)
            
            # åˆ›å»ºæ–‡ç« ä¸“å±ç›®å½•
            article_dir = save_dir / clean_title[:50]  # é™åˆ¶ç›®å½•åé•¿åº¦
            article_dir.mkdir(exist_ok=True, parents=True)
            
            # ä¿å­˜æ–‡ç« å…ƒæ•°æ®
            meta = {
                "platform": self.platform,
                "original_url": article_data.get('url', ''),
                "title": title,
                "publish_time": publish_time,
                "scrape_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "content_type": self.content_type,
                "content": article_data.get('content', ''),
                "file_url": article_data.get('file_url', '')
            }
            
            with open(article_dir / f"{clean_title[:50]}.json", 'w', encoding='utf-8') as f:
                json.dump(meta, f, ensure_ascii=False, indent=2)
                
            # ä¿å­˜å†…å®¹åˆ°markdownæ–‡ä»¶
            content = article_data.get('content', '')
            if content:
                with open(article_dir / f"{clean_title[:50]}.md", 'w', encoding='utf-8') as f:
                    f.write(f"# {title}\n\n")
                    f.write(f"å‘å¸ƒæ—¶é—´: {publish_time}\n\n")
                    f.write(f"æ¥æº: {article_data.get('platform', '')}\n\n")
                    f.write(f"é“¾æ¥: {article_data.get('url', '')}\n\n")
                    f.write("---\n\n")
                    f.write(content)
            
            # ä¿å­˜HTMLå¿«ç…§ï¼ˆå¦‚æœæä¾›ï¼‰
            if html_content:
                self.save_snapshot(html_content, article_data.get('url', ''), article_dir)
                    
            self.logger.debug(f"æ–‡ç« å·²ä¿å­˜: {clean_title}")
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜æ–‡ç« å¤±è´¥: {e}")
            self.counter['error'] += 1

    async def batch_save_articles(self, article_batch: List[Dict[str, Any]]) -> None:
        """æ‰¹é‡ä¿å­˜æ–‡ç« æ•°æ®"""
        if not article_batch:
            return
            
        link_relations = []  # æ”¶é›†é“¾æ¥å…³ç³»ç”¨äºæ‰¹é‡ä¿å­˜
        
        for item in article_batch:
            try:
                article_data = item['article_data']
                if not article_data:
                    continue
                    
                # ä¿å­˜æ–‡ç« 
                html_content = article_data.pop('html_content', None)
                self.save_article(article_data, html_content)
                
                # æ”¶é›†é“¾æ¥å…³ç³»
                url = item['url']
                links = item['links']
                if links:
                    link_relations.append((url, links))
                    
            except Exception as e:
                self.logger.error(f"æ‰¹é‡ä¿å­˜æ–‡ç« å¤±è´¥: {e}")
        
        # æ‰¹é‡ä¿å­˜é“¾æ¥å…³ç³»
        if link_relations:
            await self.batch_save_link_graph(link_relations)

    async def batch_save_link_graph(self, link_relations: List[tuple]) -> None:
        """æ‰¹é‡ä¿å­˜é“¾æ¥å…³ç³»"""
        try:
            from etl.load.db_core import execute_query
            
            # å‡†å¤‡æ‰¹é‡æ’å…¥æ•°æ®
            insert_data = []
            for source_url, target_urls in link_relations:
                for target_url in target_urls:
                    insert_data.append((source_url, target_url, 'internal'))
            
            if insert_data:
                # æ‰¹é‡æ’å…¥ï¼Œæ¯æ¬¡æœ€å¤š500æ¡
                batch_size = 500
                for i in range(0, len(insert_data), batch_size):
                    batch = insert_data[i:i + batch_size]
                    
                    placeholders = ','.join(['(%s, %s, %s)'] * len(batch))
                    sql = f'''
                        INSERT IGNORE INTO link_graph (source_url, target_url, link_type)
                        VALUES {placeholders}
                    '''
                    
                    flat_data = [item for row in batch for item in row]
                    await execute_query(sql, flat_data, fetch=False)
                    
                self.logger.debug(f"æ‰¹é‡ä¿å­˜äº† {len(insert_data)} ä¸ªé“¾æ¥å…³ç³»")
                
        except Exception as e:
            self.logger.error(f"æ‰¹é‡ä¿å­˜é“¾æ¥å…³ç³»å¤±è´¥: {e}")

    async def crawl(self, concurrent_limit: int = 20, batch_size: int = 50, fresh_start: bool = False) -> None:
        """æ‰§è¡Œçˆ¬å–ä»»åŠ¡ï¼ˆå¹¶å‘ä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        try:
            import aiohttp
            
            # åˆå§‹åŒ–é“¾æ¥å›¾æ•°æ®åº“
            await self.init_link_graph_db()
            
            # å…ˆè·å–å·²ç»æŠ“å–çš„é“¾æ¥ï¼ˆé™¤éæ˜¯å…¨æ–°å¼€å§‹ï¼‰
            if not fresh_start:
                scraped_original_urls = self.get_scraped_original_urls()
                self.visited_urls.update(scraped_original_urls)
                self.logger.info(f"å·²åŠ è½½ {len(scraped_original_urls)} ä¸ªå†å²URLè®°å½•")
            else:
                self.logger.info("å…¨æ–°å¼€å§‹çˆ¬å–ï¼Œå¿½ç•¥å†å²è®°å½•")
            
            start_time = time.time()
            
            if not self.debug:
                lock_path = self.base_dir / self.lock_file
                lock_path.write_text(str(int(start_time)))
                
            # ä½¿ç”¨é˜Ÿåˆ—ç®¡ç†URL
            url_queue = list(self.start_urls)
            depth_map = {url: 0 for url in self.start_urls}
            
            self.logger.info(f"åˆå§‹URLé˜Ÿåˆ—å¤§å°: {len(url_queue)}")
            self.logger.info(f"å·²è®¿é—®URLæ•°é‡: {len(self.visited_urls)}")
            
            # è¿‡æ»¤æ‰å·²è®¿é—®çš„URL
            original_queue_size = len(url_queue)
            url_queue = [url for url in url_queue if url not in self.visited_urls]
            self.logger.info(f"è¿‡æ»¤åURLé˜Ÿåˆ—å¤§å°: {len(url_queue)} (è¿‡æ»¤æ‰ {original_queue_size - len(url_queue)} ä¸ªå·²è®¿é—®URL)")
            
            # å¹¶å‘å’Œæ‰¹å¤„ç†é…ç½®
            save_batch_size = max(10, batch_size // 3)  # æ¯æ¬¡ä¿å­˜çš„æ–‡ç« æ•°é‡
            
            # åˆ›å»ºaiohttpä¼šè¯ï¼Œç”¨äºå¿«é€ŸHTTPè¯·æ±‚
            # å¤„ç†æ— é™åˆ¶å¹¶å‘çš„æƒ…å†µ
            if concurrent_limit is None:
                connector = aiohttp.TCPConnector(
                    limit=None,  # æ— é™åˆ¶
                    limit_per_host=None,  # æ¯ä¸ªä¸»æœºä¹Ÿæ— é™åˆ¶
                    ttl_dns_cache=300,
                    use_dns_cache=True,
                )
            else:
                connector = aiohttp.TCPConnector(
                    limit=concurrent_limit,
                    limit_per_host=min(10, concurrent_limit // 2),  # æ¯ä¸ªä¸»æœºé™åˆ¶ä¸ºæ€»é™åˆ¶çš„ä¸€åŠ
                    ttl_dns_cache=300,
                    use_dns_cache=True,
                )
            
            timeout = aiohttp.ClientTimeout(total=15, connect=5)
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            article_buffer = []  # æ–‡ç« ç¼“å†²åŒº
            
            async with aiohttp.ClientSession(
                connector=connector, 
                timeout=timeout,
                headers=headers
            ) as session:
                
                # åˆ›å»ºè¿›åº¦æ¡
                with tqdm(total=self.max_pages, desc="ğŸ•·ï¸ ç½‘é¡µçˆ¬å–è¿›åº¦", unit="é¡µ") as pbar:
                    batch_count = 0
                    
                    while url_queue and self.crawled_count < self.max_pages:
                        # è·å–å½“å‰æ‰¹æ¬¡çš„URL
                        current_batch = []
                        batch_urls = []
                        batch_depths = []
                        
                        for _ in range(min(batch_size, len(url_queue), self.max_pages - self.crawled_count)):
                            if url_queue:
                                url = url_queue.pop(0)
                                depth = depth_map.get(url, 0)
                                # æ£€æŸ¥æ·±åº¦é™åˆ¶ï¼ˆ-1è¡¨ç¤ºæ— é™åˆ¶ï¼‰
                                if self.max_depth == -1 or depth <= self.max_depth:
                                    current_batch.append(url)
                                    batch_urls.append(url)
                                    batch_depths.append(depth)
                        
                        if not current_batch:
                            break
                            
                        batch_count += 1
                        
                        # å¹¶å‘çˆ¬å–å½“å‰æ‰¹æ¬¡
                        tasks = []
                        for url, depth in zip(batch_urls, batch_depths):
                            task = self.crawl_url(url, depth, session)
                            tasks.append(task)
                        
                        # æ‰§è¡Œå¹¶å‘ä»»åŠ¡
                        results = await asyncio.gather(*tasks, return_exceptions=True)
                        
                        # å¤„ç†ç»“æœ
                        new_articles = []
                        total_new_links = 0
                        
                        for result in results:
                            if isinstance(result, dict) and result['success']:
                                # æ·»åŠ æ–°é“¾æ¥åˆ°é˜Ÿåˆ— - ä¼˜åŒ–é˜Ÿåˆ—ç®¡ç†
                                current_depth = depth_map.get(result['url'], 0)
                                for link in result['links']:
                                    if (link not in self.visited_urls and 
                                        link not in depth_map and 
                                        len(url_queue) < 200000 and  # å¢åŠ é˜Ÿåˆ—å¤§å°é™åˆ¶åˆ°20ä¸‡
                                        (self.max_depth == -1 or current_depth < self.max_depth)):  # æ£€æŸ¥æ·±åº¦é™åˆ¶ï¼ˆ-1è¡¨ç¤ºæ— é™åˆ¶ï¼‰
                                        
                                        # ä¼˜å…ˆæ·»åŠ å†…å®¹é¡µé¢åˆ°é˜Ÿåˆ—å‰éƒ¨
                                        if self.is_content_url(link):
                                            url_queue.insert(0, link)  # å†…å®¹é¡µé¢ä¼˜å…ˆå¤„ç†
                                        else:
                                            url_queue.append(link)  # æ™®é€šé¡µé¢æ”¾åœ¨åé¢
                                            
                                        depth_map[link] = current_depth + 1
                                        total_new_links += 1
                                
                                # æ”¶é›†æ–‡ç« æ•°æ®
                                if result['article_data']:
                                    new_articles.append(result)
                        
                        # æ·»åŠ åˆ°ç¼“å†²åŒº
                        article_buffer.extend(new_articles)
                        
                        # è¾¾åˆ°ä¿å­˜æ‰¹æ¬¡å¤§å°æ—¶è¿›è¡Œæ‰¹é‡ä¿å­˜
                        if len(article_buffer) >= save_batch_size:
                            await self.batch_save_articles(article_buffer[:save_batch_size])
                            article_buffer = article_buffer[save_batch_size:]
                        
                        # æ›´æ–°è¿›åº¦æ¡
                        pbar.set_postfix({
                            "æ‰¹æ¬¡": batch_count,
                            "å¹¶å‘": len(current_batch),
                            "æˆåŠŸ": len([r for r in results if isinstance(r, dict) and r['success']]),
                            "æ–°æ–‡ç« ": len(new_articles),
                            "ç¼“å†²": len(article_buffer),
                            "é˜Ÿåˆ—": len(url_queue),
                            "æ–°é“¾æ¥": total_new_links,
                            "å·²è®¿é—®": len(self.visited_urls),
                            "æ·±åº¦å›¾": len(depth_map),
                            "æˆåŠŸç‡": f"{(self.counter['visit'] - self.counter['error']) / max(1, self.counter['visit']) * 100:.1f}%"
                        })
                        
                        if self.crawled_count > pbar.n:
                            pbar.update(self.crawled_count - pbar.n)
                        
                        # æ¯100ä¸ªæ‰¹æ¬¡è¾“å‡ºè¯¦ç»†ç»Ÿè®¡
                        if batch_count % 100 == 0:
                            self.logger.info(f"æ‰¹æ¬¡ {batch_count}: é˜Ÿåˆ—={len(url_queue)}, å·²è®¿é—®={len(self.visited_urls)}, çˆ¬å–æ–‡ç« ={self.crawled_count}")
                            # ç»Ÿè®¡é˜Ÿåˆ—ä¸­å„ç§æ·±åº¦çš„URLæ•°é‡
                            depth_stats = {}
                            for url in url_queue[:1000]:  # åªç»Ÿè®¡å‰1000ä¸ªé¿å…æ€§èƒ½é—®é¢˜
                                depth = depth_map.get(url, 0)
                                depth_stats[depth] = depth_stats.get(depth, 0) + 1
                            self.logger.info(f"æ·±åº¦åˆ†å¸ƒ: {depth_stats}")
                        
                        # çŸ­æš‚ä¼‘æ¯é¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚
                        if not self.debug:
                            await asyncio.sleep(0.1)
                
                # ä¿å­˜å‰©ä½™çš„æ–‡ç« 
                if article_buffer:
                    await self.batch_save_articles(article_buffer)
                    
                # è®°å½•é€€å‡ºåŸå› 
                if not url_queue:
                    self.logger.info(f"çˆ¬å–ç»“æŸï¼šURLé˜Ÿåˆ—ä¸ºç©º")
                elif self.crawled_count >= self.max_pages:
                    self.logger.info(f"çˆ¬å–ç»“æŸï¼šè¾¾åˆ°æœ€å¤§é¡µé¢æ•°é™åˆ¶ {self.max_pages}")
                else:
                    self.logger.info(f"çˆ¬å–ç»“æŸï¼šæœªçŸ¥åŸå› ï¼Œé˜Ÿåˆ—å¤§å°={len(url_queue)}, çˆ¬å–æ•°é‡={self.crawled_count}")
                        
            # ä¿å­˜å·²çˆ¬å–çš„URLåˆ—è¡¨
            all_visited = list(self.visited_urls)
            self.update_scraped_articles(all_visited, [])
            
            if not self.debug:
                lock_path.unlink()
                
            elapsed_time = time.time() - start_time
            self.save_counter(start_time)
            
            self.logger.info(f"ğŸ‰ çˆ¬å–å®Œæˆï¼ç»Ÿè®¡ä¿¡æ¯ï¼š")
            self.logger.info(f"  - å…±çˆ¬å–é¡µé¢: {self.crawled_count}")
            self.logger.info(f"  - è®¿é—®URLæ•°: {self.counter['visit']}")
            self.logger.info(f"  - å‘ç°é“¾æ¥æ•°: {len(self.visited_urls)}")
            self.logger.info(f"  - é”™è¯¯æ¬¡æ•°: {self.counter['error']}")
            self.logger.info(f"  - æˆåŠŸç‡: {(self.counter['visit'] - self.counter['error']) / max(1, self.counter['visit']) * 100:.1f}%")
            self.logger.info(f"  - å¹³å‡é€Ÿåº¦: {self.counter['visit'] / elapsed_time:.1f} é¡µ/ç§’")
            self.logger.info(f"  - ç”¨æ—¶: {elapsed_time:.1f} ç§’")
            
        except Exception as e:
            self.logger.error(f"çˆ¬å–å‡ºé”™: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
        finally:
            if hasattr(self, 'update_f') and self.update_f:
                self.update_f.close()

    async def download(self, time_range: tuple = None):
        """ä¸‹è½½åŠŸèƒ½ï¼ˆç½‘é¡µçˆ¬è™«ä¸­å®é™…ä¸Šæ˜¯åœ¨crawlä¸­å®Œæˆçš„ï¼‰"""
        self.logger.info("ç½‘é¡µçˆ¬è™«çš„ä¸‹è½½åŠŸèƒ½å·²é›†æˆåœ¨crawlæ–¹æ³•ä¸­")

    def import_from_scrapy_db(self, db_path: str = None) -> int:
        """ä»scrapyçš„sqliteæ•°æ®åº“å¯¼å…¥æ•°æ®åˆ°ç»Ÿä¸€æ ¼å¼
        
        Args:
            db_path: sqliteæ•°æ®åº“è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
            
        Returns:
            å¯¼å…¥çš„è®°å½•æ•°é‡
        """
        import sqlite3
        
        if db_path is None:
            # ä½¿ç”¨é»˜è®¤çš„scrapyæ•°æ®åº“è·¯å¾„
            scrapy_dir = Path(__file__).parent / "webpage_spider" / "counselor"
            db_path = scrapy_dir / "nk_database.db"
            
        if not Path(db_path).exists():
            self.logger.error(f"æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨: {db_path}")
            return 0
            
        try:
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            
            # æŸ¥è¯¢æ‰€æœ‰è®°å½•
            cursor.execute("""
                SELECT title, push_time, url, content, file_url, source, push_time_date 
                FROM entries 
                ORDER BY id
            """)
            
            rows = cursor.fetchall()
            imported_count = 0
            
            with tqdm(total=len(rows), desc="å¯¼å…¥scrapyæ•°æ®", unit="æ¡") as pbar:
                for row in rows:
                    title, push_time, url, content, file_url, source, push_time_date = row
                    
                    # è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼
                    article_data = {
                        'title': title or 'æœªçŸ¥æ ‡é¢˜',
                        'publish_time': push_time or '',
                        'content': content or '',
                        'source': source or 'unknown',
                        'url': url,
                        'file_url': file_url or None
                    }
                    
                    # ä¿å­˜æ–‡ç« æ•°æ®
                    try:
                        self.save_article(article_data)
                        imported_count += 1
                    except Exception as e:
                        self.logger.error(f"å¯¼å…¥è®°å½•å¤±è´¥: {url}, é”™è¯¯: {e}")
                        
                    pbar.update(1)
                    pbar.set_postfix({"å·²å¯¼å…¥": imported_count})
                    
            conn.close()
            self.logger.info(f"ä»scrapyæ•°æ®åº“æˆåŠŸå¯¼å…¥ {imported_count} æ¡è®°å½•")
            return imported_count
            
        except Exception as e:
            self.logger.error(f"å¯¼å…¥scrapyæ•°æ®æ—¶å‡ºé”™: {e}")
            return 0

    def export_to_mysql(self) -> bool:
        """å¯¼å‡ºæ•°æ®åˆ°MySQLæ•°æ®åº“ï¼ˆä¸ETLæµç¨‹å…¼å®¹ï¼‰"""
        try:
            from etl.load.import_data import main as import_main
            
            # è°ƒç”¨ç°æœ‰çš„å¯¼å…¥è„šæœ¬
            self.logger.info("å¼€å§‹å¯¼å‡ºæ•°æ®åˆ°MySQL...")
            success = import_main()
            
            if success:
                self.logger.info("æ•°æ®å¯¼å‡ºåˆ°MySQLæˆåŠŸ")
            else:
                self.logger.error("æ•°æ®å¯¼å‡ºåˆ°MySQLå¤±è´¥")
                
            return success
            
        except Exception as e:
            self.logger.error(f"å¯¼å‡ºåˆ°MySQLæ—¶å‡ºé”™: {e}")
            return False

    def calculate_pagerank(self) -> bool:
        """è®¡ç®—PageRankåˆ†æ•°"""
        try:
            # ç›´æ¥å¯¼å…¥å¹¶è¿è¡ŒPageRankè®¡ç®—è„šæœ¬
            import sys
            pagerank_dir = Path(__file__).parent.parent / "pagerank"
            sys.path.append(str(pagerank_dir))
            
            from calculate_pagerank import main as pagerank_main
            
            self.logger.info("å¼€å§‹è®¡ç®—PageRankåˆ†æ•°...")
            pagerank_main()
            self.logger.info("PageRankè®¡ç®—å®Œæˆ")
            return True
            
        except Exception as e:
            self.logger.error(f"è®¡ç®—PageRankå¤±è´¥: {e}")
            return False

    def get_statistics(self) -> Dict[str, Any]:
        """è·å–çˆ¬è™«ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "platform": self.platform,
            "tag": self.tag,
            "start_urls_count": len(self.start_urls),
            "allowed_domains": list(self.allowed_domains),
            "max_depth": self.max_depth,
            "max_pages": self.max_pages,
            "visited_urls_count": len(self.visited_urls),
            "crawled_count": self.crawled_count,
            "counters": dict(self.counter),
            "data_dir": str(self.data_dir)
        }
        
        # ç»Ÿè®¡ä¿å­˜çš„æ–‡ä»¶æ•°é‡
        json_files = list(self.data_dir.glob("**/*.json"))
        md_files = list(self.data_dir.glob("**/*.md"))
        html_files = list(self.data_dir.glob("**/original.html"))
        
        stats.update({
            "saved_json_files": len(json_files),
            "saved_md_files": len(md_files),
            "saved_html_snapshots": len(html_files)
        })
        
        return stats



if __name__ == "__main__":
    async def main():
        """å¼‚æ­¥ä¸»å‡½æ•°"""
        import argparse
        
        parser = argparse.ArgumentParser(description="ç½‘é¡µçˆ¬è™«å·¥å…·")
        parser.add_argument("--mode", choices=["crawl", "import", "export", "pagerank", "stats"], 
                          default="crawl", help="è¿è¡Œæ¨¡å¼")
        parser.add_argument("--debug", action="store_true", help="è°ƒè¯•æ¨¡å¼")
        parser.add_argument("--headless", action="store_true", default=True, help="æ— å¤´æ¨¡å¼")
        parser.add_argument("--proxy", action="store_true", help="ä½¿ç”¨ä»£ç†")
        parser.add_argument("--max-depth", type=int, default=-1, help="æœ€å¤§çˆ¬å–æ·±åº¦ï¼Œ-1è¡¨ç¤ºæ— é™åˆ¶")
        parser.add_argument("--max-pages", type=int, default=110000, help="æœ€å¤§çˆ¬å–é¡µé¢æ•°")
        parser.add_argument("--start-urls", type=int, default=-1, help="ä½¿ç”¨çš„èµ·å§‹URLæ•°é‡ï¼Œ-1è¡¨ç¤ºä½¿ç”¨å…¨éƒ¨")
        parser.add_argument("--concurrent", type=int, default=-1, help="å¹¶å‘è¯·æ±‚æ•°é‡ï¼Œ-1è¡¨ç¤ºæ— é™åˆ¶")
        parser.add_argument("--batch-size", type=int, default=1000, help="æ¯æ‰¹å¤„ç†çš„URLæ•°é‡")
        parser.add_argument("--fresh-start", action="store_true",default=True, help="å…¨æ–°å¼€å§‹ï¼Œå¿½ç•¥å†å²è®°å½•")
        
        args = parser.parse_args()
        
        # å¤„ç†èµ·å§‹URLæ•°é‡å‚æ•°
        if args.start_urls == -1:
            start_urls = nankai_start_urls  # ä½¿ç”¨å…¨éƒ¨URL
        else:
            start_urls = nankai_start_urls[:args.start_urls]  # ä½¿ç”¨æŒ‡å®šæ•°é‡
            
        # å¤„ç†å¹¶å‘æ•°é‡å‚æ•°
        concurrent_limit = None if args.concurrent == -1 else args.concurrent
            
        crawler = WebpageCrawler(
            start_urls=start_urls,
            tag="nku",
            debug=args.debug,
            headless=args.headless,
            use_proxy=args.proxy,
            max_depth=args.max_depth,
            max_pages=args.max_pages
        )
        
        try:
            if args.mode in ["crawl", "export", "stats"]:
                await crawler.async_init()
            
            if args.mode == "crawl":
                print("ğŸš€ å¼€å§‹ç½‘é¡µçˆ¬å–...")
                concurrent_display = "æ— é™åˆ¶" if args.concurrent == -1 else str(args.concurrent)
                print(f"âš™ï¸  é…ç½®: å¹¶å‘={concurrent_display}, æ‰¹æ¬¡å¤§å°={args.batch_size}, æœ€å¤§é¡µé¢={args.max_pages}")
                print(f"ğŸ”— èµ·å§‹URL: {len(start_urls)} ä¸ª (æ€»å…±å¯ç”¨: {len(nankai_start_urls)} ä¸ª)")
                if args.fresh_start:
                    print("ğŸ†• å…¨æ–°å¼€å§‹æ¨¡å¼ï¼Œå°†å¿½ç•¥å†å²çˆ¬å–è®°å½•")
                await crawler.crawl(
                    concurrent_limit=concurrent_limit,
                    batch_size=args.batch_size,
                    fresh_start=args.fresh_start
                )
                
                # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
                stats = crawler.get_statistics()
                print("\nğŸ“Š çˆ¬å–ç»Ÿè®¡:")
                for key, value in stats.items():
                    if key not in ["allowed_domains", "data_dir"]:
                        print(f"  {key}: {value}")
                        
            elif args.mode == "import":
                print("ğŸ“¥ ä»scrapyæ•°æ®åº“å¯¼å…¥æ•°æ®...")
                count = crawler.import_from_scrapy_db()
                print(f"âœ… æˆåŠŸå¯¼å…¥ {count} æ¡è®°å½•")
                
            elif args.mode == "export":
                print("ğŸ“¤ å¯¼å‡ºæ•°æ®åˆ°MySQL...")
                success = crawler.export_to_mysql()
                if success:
                    print("âœ… æ•°æ®å¯¼å‡ºæˆåŠŸ")
                else:
                    print("âŒ æ•°æ®å¯¼å‡ºå¤±è´¥")
                    
            elif args.mode == "pagerank":
                print("ğŸ”— è®¡ç®—PageRankåˆ†æ•°...")
                success = crawler.calculate_pagerank()
                if success:
                    print("âœ… PageRankè®¡ç®—æˆåŠŸ")
                else:
                    print("âŒ PageRankè®¡ç®—å¤±è´¥")
                    
            elif args.mode == "stats":
                stats = crawler.get_statistics()
                print("\nğŸ“Š è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯:")
                import json
                print(json.dumps(stats, indent=2, ensure_ascii=False))
                
        except KeyboardInterrupt:
            print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        except Exception as e:
            print(f"âŒ è¿è¡Œå‡ºé”™: {e}")
        finally:
            if hasattr(crawler, 'browser') and crawler.browser:
                await crawler.close()

    # è¿è¡Œå¼‚æ­¥ä¸»å‡½æ•°
    asyncio.run(main()) 