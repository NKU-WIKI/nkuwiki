#!/usr/bin/env python3
"""
ä¿¡æ¯æ£€ç´¢å¤§ä½œä¸šè¯„åˆ†ç‚¹å…¨é¢æµ‹è¯•è„šæœ¬
æµ‹è¯•requirement.mdä¸­æ‰€æœ‰åŠŸèƒ½è¦æ±‚å’Œè¯„åˆ†æ ‡å‡†

è¯„åˆ†æ ‡å‡†ï¼š
- ç½‘é¡µæŠ“å–ï¼ˆ10%ï¼‰
- æ–‡æœ¬ç´¢å¼•ï¼ˆ10%ï¼‰  
- é“¾æ¥åˆ†æï¼ˆ10%ï¼‰
- æŸ¥è¯¢æœåŠ¡ï¼ˆ35%ï¼‰ï¼šç«™å†…æŸ¥è¯¢ã€æ–‡æ¡£æŸ¥è¯¢ã€çŸ­è¯­æŸ¥è¯¢ã€é€šé…æŸ¥è¯¢ã€æŸ¥è¯¢æ—¥å¿—ã€ç½‘é¡µå¿«ç…§
- ä¸ªæ€§åŒ–æŸ¥è¯¢ï¼ˆ10%ï¼‰
- Webç•Œé¢ï¼ˆ5%ï¼‰
- ä¸ªæ€§åŒ–æ¨èï¼ˆ10%ï¼‰
"""

import sys
import os
import asyncio
import time
import json
from pathlib import Path
from typing import Dict, List, Any
import traceback

sys.path.append(str(Path(__file__).resolve().parent.parent.parent))

class RequirementTester:
    """ä¿¡æ¯æ£€ç´¢å¤§ä½œä¸šè¦æ±‚æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.results = {}
        
    def log_test(self, test_name: str, success: bool, details: str = "", score_weight: float = 1.0):
        """è®°å½•æµ‹è¯•ç»“æœ"""
        self.results[test_name] = {
            "success": success,
            "details": details,
            "score_weight": score_weight,
            "timestamp": time.time()
        }
        status = "âœ…" if success else "âŒ"
        print(f"{status} {test_name}: {details}")
    
    async def test_web_crawling(self):
        """æµ‹è¯•ç½‘é¡µæŠ“å–ï¼ˆ10%ï¼‰"""
        print("\nğŸ•·ï¸ æµ‹è¯•ç½‘é¡µæŠ“å–åŠŸèƒ½...")
        
        try:
            # 1. æ£€æŸ¥æ•°æ®åº“ä¸­çš„æŠ“å–æ•°æ®é‡
            from etl.load import db_core
            
            # ç»Ÿè®¡å„æ•°æ®æºçš„æ•°æ®é‡
            tables = {
                'website_nku': 'ç½‘é¡µæ•°æ®',
                'wechat_nku': 'å¾®ä¿¡å…¬ä¼—å·æ•°æ®', 
                'wxapp_post': 'å°ç¨‹åºå¸–å­æ•°æ®'
            }
            
            total_count = 0
            for table, description in tables.items():
                try:
                    result = await db_core.execute_query(
                        f"SELECT COUNT(*) as count FROM {table} WHERE content IS NOT NULL AND content != ''",
                        fetch=True
                    )
                    count = result[0]['count'] if result else 0
                    total_count += count
                    print(f"  - {description}: {count:,} æ¡è®°å½•")
                except Exception as e:
                    print(f"  - {description}: æŸ¥è¯¢å¤±è´¥ ({e})")
            
            # 2. æ£€æŸ¥æ–‡æ¡£è§£æèƒ½åŠ›
            doc_count = 0
            try:
                result = await db_core.execute_query(
                    "SELECT COUNT(*) as count FROM website_nku WHERE original_url LIKE '%.pdf' OR original_url LIKE '%.doc%' OR original_url LIKE '%.xls%'",
                    fetch=True
                )
                doc_count = result[0]['count'] if result else 0
                print(f"  - æ–‡æ¡£æ•°æ®: {doc_count:,} æ¡è®°å½•")
            except Exception as e:
                print(f"  - æ–‡æ¡£æ•°æ®æŸ¥è¯¢å¤±è´¥: {e}")
            
            # è¯„ä¼°æŠ“å–è§„æ¨¡
            if total_count >= 100000:
                self.log_test("ç½‘é¡µæŠ“å–-æ•°æ®è§„æ¨¡", True, f"æ€»è®¡{total_count:,}æ¡è®°å½•ï¼Œè¶…è¿‡10ä¸‡è¦æ±‚", 0.7)
            elif total_count >= 50000:
                self.log_test("ç½‘é¡µæŠ“å–-æ•°æ®è§„æ¨¡", True, f"æ€»è®¡{total_count:,}æ¡è®°å½•ï¼Œæ¥è¿‘è¦æ±‚", 0.5)
            else:
                self.log_test("ç½‘é¡µæŠ“å–-æ•°æ®è§„æ¨¡", False, f"æ€»è®¡{total_count:,}æ¡è®°å½•ï¼Œä½äº10ä¸‡è¦æ±‚", 0.7)
            
                
        except Exception as e:
            self.log_test("ç½‘é¡µæŠ“å–", False, f"æµ‹è¯•å¤±è´¥: {e}", 1.0)
    
    async def test_text_indexing(self):
        """æµ‹è¯•æ–‡æœ¬ç´¢å¼•ï¼ˆ10%ï¼‰"""
        print("\nğŸ“Š æµ‹è¯•æ–‡æœ¬ç´¢å¼•åŠŸèƒ½...")
        
        try:
            # 1. æµ‹è¯•BM25ç´¢å¼•
            from etl.retrieval.retrievers import BM25Retriever
            from config import Config
            
            config = Config()
            nodes_path = config.get('etl.retrieval.bm25.nodes_path')
            
            if os.path.exists(nodes_path):
                retriever = BM25Retriever.load_from_pickle(nodes_path)
                node_count = len(retriever._corpus) if hasattr(retriever, '_corpus') else 0
                
                if node_count > 50000:
                    self.log_test("æ–‡æœ¬ç´¢å¼•-BM25", True, f"BM25ç´¢å¼•åŒ…å«{node_count:,}ä¸ªæ–‡æ¡£", 0.4)
                else:
                    self.log_test("æ–‡æœ¬ç´¢å¼•-BM25", False, f"BM25ç´¢å¼•ä»…{node_count:,}ä¸ªæ–‡æ¡£", 0.4)
            else:
                self.log_test("æ–‡æœ¬ç´¢å¼•-BM25", False, "BM25ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨", 0.4)
            
            # 2. æµ‹è¯•å‘é‡ç´¢å¼•  
            from qdrant_client import QdrantClient
            qdrant_url = config.get('etl.data.qdrant.url', 'http://localhost:6333')
            collection_name = config.get('etl.data.qdrant.collection', 'main_index')
            
            try:
                client = QdrantClient(url=qdrant_url, timeout=5)
                collection_info = client.get_collection(collection_name)
                vector_count = collection_info.points_count
                
                if vector_count > 50000:
                    self.log_test("æ–‡æœ¬ç´¢å¼•-å‘é‡", True, f"å‘é‡ç´¢å¼•åŒ…å«{vector_count:,}ä¸ªå‘é‡", 0.3)
                else:
                    self.log_test("æ–‡æœ¬ç´¢å¼•-å‘é‡", False, f"å‘é‡ç´¢å¼•ä»…{vector_count:,}ä¸ªå‘é‡", 0.3)
            except Exception as e:
                self.log_test("æ–‡æœ¬ç´¢å¼•-å‘é‡", False, f"å‘é‡ç´¢å¼•è®¿é—®å¤±è´¥: {e}", 0.3)
            
            # 3. æµ‹è¯•Elasticsearchç´¢å¼•
            from elasticsearch import Elasticsearch
            es_host = config.get('etl.data.elasticsearch.host', 'localhost')
            es_port = config.get('etl.data.elasticsearch.port', 9200)
            es_index = config.get('etl.data.elasticsearch.index', 'nkuwiki')
            
            try:
                es = Elasticsearch([{'host': es_host, 'port': es_port, 'scheme': 'http'}], request_timeout=5)
                if es.ping():
                    if es.indices.exists(index=es_index):
                        doc_count = es.count(index=es_index)['count']
                        
                        if doc_count > 50000:
                            self.log_test("æ–‡æœ¬ç´¢å¼•-Elasticsearch", True, f"ESç´¢å¼•åŒ…å«{doc_count:,}ä¸ªæ–‡æ¡£", 0.3)
                        else:
                            self.log_test("æ–‡æœ¬ç´¢å¼•-Elasticsearch", False, f"ESç´¢å¼•ä»…{doc_count:,}ä¸ªæ–‡æ¡£", 0.3)
                    else:
                        self.log_test("æ–‡æœ¬ç´¢å¼•-Elasticsearch", False, f"ESç´¢å¼•'{es_index}'ä¸å­˜åœ¨", 0.3)
                else:
                    self.log_test("æ–‡æœ¬ç´¢å¼•-Elasticsearch", False, "æ— æ³•è¿æ¥åˆ°Elasticsearch", 0.3)
            except Exception as e:
                self.log_test("æ–‡æœ¬ç´¢å¼•-Elasticsearch", False, f"ESæµ‹è¯•å¤±è´¥: {e}", 0.3)
                
        except Exception as e:
            self.log_test("æ–‡æœ¬ç´¢å¼•", False, f"æµ‹è¯•å¤±è´¥: {e}", 1.0)
    
    async def test_link_analysis(self):
        """æµ‹è¯•é“¾æ¥åˆ†æï¼ˆ10%ï¼‰"""
        print("\nğŸ”— æµ‹è¯•é“¾æ¥åˆ†æåŠŸèƒ½...")
        
        try:
            from etl.load import db_core
            
            # 1. æ£€æŸ¥é“¾æ¥å›¾æ•°æ®
            try:
                result = await db_core.execute_query(
                    "SELECT COUNT(*) as count FROM link_graph", 
                    fetch=True
                )
                link_count = result[0]['count'] if result else 0
                
                if link_count > 10000:
                    self.log_test("é“¾æ¥åˆ†æ-é“¾æ¥å›¾", True, f"é“¾æ¥å›¾åŒ…å«{link_count:,}æ¡é“¾æ¥å…³ç³»", 0.3)
                else:
                    self.log_test("é“¾æ¥åˆ†æ-é“¾æ¥å›¾", False, f"é“¾æ¥å›¾ä»…{link_count:,}æ¡é“¾æ¥å…³ç³»", 0.3)
            except Exception as e:
                self.log_test("é“¾æ¥åˆ†æ-é“¾æ¥å›¾", False, f"é“¾æ¥å›¾è¡¨è®¿é—®å¤±è´¥: {e}", 0.3)
            
            # 2. æ£€æŸ¥PageRankåˆ†æ•°
            try:
                result = await db_core.execute_query(
                    "SELECT COUNT(*) as count FROM pagerank_scores WHERE pagerank_score > 0",
                    fetch=True
                )
                pr_count = result[0]['count'] if result else 0
                
                if pr_count > 10000:
                    self.log_test("é“¾æ¥åˆ†æ-PageRankè®¡ç®—", True, f"è®¡ç®—äº†{pr_count:,}ä¸ªé¡µé¢çš„PageRankåˆ†æ•°", 0.4)
                else:
                    self.log_test("é“¾æ¥åˆ†æ-PageRankè®¡ç®—", False, f"ä»…è®¡ç®—äº†{pr_count:,}ä¸ªé¡µé¢çš„PageRank", 0.4)
            except Exception as e:
                self.log_test("é“¾æ¥åˆ†æ-PageRankè®¡ç®—", False, f"PageRankè¡¨è®¿é—®å¤±è´¥: {e}", 0.4)
            
            # 3. æ£€æŸ¥PageRankæ•´åˆåˆ°ä¸»è¡¨
            try:    
                result = await db_core.execute_query(
                    "SELECT COUNT(*) as count FROM website_nku WHERE pagerank_score > 0",
                    fetch=True
                )
                integrated_count = result[0]['count'] if result else 0
                
                if integrated_count > 5000:
                    self.log_test("é“¾æ¥åˆ†æ-PageRankæ•´åˆ", True, f"{integrated_count:,}æ¡è®°å½•åŒ…å«PageRankåˆ†æ•°", 0.3)
                else:
                    self.log_test("é“¾æ¥åˆ†æ-PageRankæ•´åˆ", False, f"ä»…{integrated_count:,}æ¡è®°å½•åŒ…å«PageRankåˆ†æ•°", 0.3)
            except Exception as e:
                self.log_test("é“¾æ¥åˆ†æ-PageRankæ•´åˆ", False, f"PageRankæ•´åˆæ£€æŸ¥å¤±è´¥: {e}", 0.3)
                
        except Exception as e:
            self.log_test("é“¾æ¥åˆ†æ", False, f"æµ‹è¯•å¤±è´¥: {e}", 1.0)
    
    async def test_query_services(self):
        """æµ‹è¯•æŸ¥è¯¢æœåŠ¡ï¼ˆ35%ï¼‰"""
        print("\nğŸ” æµ‹è¯•æŸ¥è¯¢æœåŠ¡åŠŸèƒ½...")
        
        # APIæµ‹è¯•å·²ç§»é™¤ï¼Œç›´æ¥è¿›è¡ŒåŠŸèƒ½æµ‹è¯•
        
        # 1. ç«™å†…æŸ¥è¯¢ï¼ˆåŸºæœ¬æŸ¥è¯¢æ“ä½œï¼‰- 10%
        await self._test_basic_search()
        
        # 2. æ–‡æ¡£æŸ¥è¯¢ - 5%
        await self._test_document_search()
        
        # 3. çŸ­è¯­æŸ¥è¯¢ - 5%
        await self._test_phrase_search()
        
        # 4. é€šé…æŸ¥è¯¢ - 5%
        await self._test_wildcard_search()
        
        # 5. æŸ¥è¯¢æ—¥å¿— - 5%
        await self._test_query_logging()
        
        # 6. ç½‘é¡µå¿«ç…§ - 5%
        await self._test_web_snapshot()
    
    # APIç›¸å…³æ–¹æ³•å·²ç§»é™¤
    
    async def _test_basic_search(self):
        """æµ‹è¯•åŸºæœ¬ç«™å†…æŸ¥è¯¢"""
        try:
            # æµ‹è¯•RAGç®¡é“ç›´æ¥è°ƒç”¨
            from etl.rag_pipeline import RagPipeline
            
            pipeline = RagPipeline()
            
            # æ‰§è¡ŒåŸºæœ¬æŸ¥è¯¢
            test_queries = ["å—å¼€å¤§å­¦", "è®¡ç®—æœºå­¦é™¢", "å›¾ä¹¦é¦†"]
            
            # éœ€è¦å¯¼å…¥æšä¸¾ç±»
            from etl.rag_pipeline import RetrievalStrategy, RerankStrategy
            
            success_count = 0
            for query in test_queries:
                try:
                    results = pipeline.run(
                        query=query,
                        retrieval_strategy=RetrievalStrategy.HYBRID,
                        rerank_strategy=RerankStrategy.BGE_RERANKER,
                        skip_generation=True  # åªæ£€ç´¢ï¼Œä¸ç”Ÿæˆ
                    )
                    
                    if results and len(results.get('retrieved_nodes', [])) > 0:
                        success_count += 1
                        print(f"    æŸ¥è¯¢'{query}': è¿”å›{len(results['retrieved_nodes'])}ä¸ªç»“æœ")
                    else:
                        print(f"    æŸ¥è¯¢'{query}': æ— ç»“æœ")
                except Exception as e:
                    print(f"    æŸ¥è¯¢'{query}': å¤±è´¥ ({e})")
            
            if success_count >= 2:
                self.log_test("æŸ¥è¯¢æœåŠ¡-ç«™å†…æŸ¥è¯¢", True, f"{success_count}/{len(test_queries)}ä¸ªæµ‹è¯•æŸ¥è¯¢æˆåŠŸ", 0.286)
            else:
                self.log_test("æŸ¥è¯¢æœåŠ¡-ç«™å†…æŸ¥è¯¢", False, f"ä»…{success_count}/{len(test_queries)}ä¸ªæµ‹è¯•æŸ¥è¯¢æˆåŠŸ", 0.286)
                
        except Exception as e:
            self.log_test("æŸ¥è¯¢æœåŠ¡-ç«™å†…æŸ¥è¯¢", False, f"åŸºæœ¬æŸ¥è¯¢æµ‹è¯•å¤±è´¥: {e}", 0.286)
    
    async def _test_document_search(self):
        """æµ‹è¯•æ–‡æ¡£æŸ¥è¯¢"""
        try:
            from etl.rag_pipeline import RagPipeline
            pipeline = RagPipeline()
            
            # æŸ¥è¯¢æ–‡æ¡£ç›¸å…³å†…å®¹
            doc_queries = ["pdfæ–‡æ¡£", "wordæ–‡æ¡£", "excelè¡¨æ ¼"]
            
            success_count = 0
            for query in doc_queries:
                try:
                    results = pipeline.run(query=query, skip_generation=True)
                    
                    # æ£€æŸ¥ç»“æœä¸­æ˜¯å¦åŒ…å«æ–‡æ¡£ç±»å‹çš„å†…å®¹
                    doc_results = []
                    for result in results.get('retrieved_nodes', []):
                        metadata = result.metadata if hasattr(result, 'metadata') else {}
                        url = metadata.get('url', '') or metadata.get('original_url', '')
                        if any(ext in url.lower() for ext in ['.pdf', '.doc', '.xls', '.ppt']):
                            doc_results.append(result)
                    
                    if doc_results:
                        success_count += 1
                        print(f"    æ–‡æ¡£æŸ¥è¯¢'{query}': æ‰¾åˆ°{len(doc_results)}ä¸ªæ–‡æ¡£ç»“æœ")
                    else:
                        print(f"    æ–‡æ¡£æŸ¥è¯¢'{query}': æ— æ–‡æ¡£ç»“æœ")
                except Exception as e:
                    print(f"    æ–‡æ¡£æŸ¥è¯¢'{query}': å¤±è´¥ ({e})")
            
            if success_count >= 1:
                self.log_test("æŸ¥è¯¢æœåŠ¡-æ–‡æ¡£æŸ¥è¯¢", True, f"æ”¯æŒæ–‡æ¡£å†…å®¹æ£€ç´¢", 0.143)
            else:
                self.log_test("æŸ¥è¯¢æœåŠ¡-æ–‡æ¡£æŸ¥è¯¢", False, f"æ–‡æ¡£æŸ¥è¯¢åŠŸèƒ½å¼‚å¸¸", 0.143)
                
        except Exception as e:
            self.log_test("æŸ¥è¯¢æœåŠ¡-æ–‡æ¡£æŸ¥è¯¢", False, f"æ–‡æ¡£æŸ¥è¯¢æµ‹è¯•å¤±è´¥: {e}", 0.143)
    
    async def _test_phrase_search(self):
        """æµ‹è¯•çŸ­è¯­æŸ¥è¯¢"""
        try:
            from etl.rag_pipeline import RagPipeline
            pipeline = RagPipeline()
            
            # æµ‹è¯•çŸ­è¯­æŸ¥è¯¢ï¼ˆç”¨å¼•å·è¡¨ç¤ºçŸ­è¯­ï¼‰
            phrase_queries = [
                '"å—å¼€å¤§å­¦"',
                '"è®¡ç®—æœºç§‘å­¦"',
                '"ä¿¡æ¯æ£€ç´¢"'
            ]
            
            success_count = 0
            for query in phrase_queries:
                try:
                    results = pipeline.run(query=query, skip_generation=True)
                    
                    if results and len(results.get('retrieved_nodes', [])) > 0:
                        # æ£€æŸ¥ç»“æœæ˜¯å¦ç¡®å®åŒ…å«çŸ­è¯­
                        phrase = query.strip('"')
                        relevant_count = 0
                        for result in results.get('retrieved_nodes', [])[:5]:  # åªæ£€æŸ¥å‰5ä¸ª
                            content = result.text.lower() if hasattr(result, 'text') else ''
                            metadata = result.metadata if hasattr(result, 'metadata') else {}
                            title = metadata.get('title', '').lower()
                            if phrase.lower() in content or phrase.lower() in title:
                                relevant_count += 1
                        
                        if relevant_count > 0:
                            success_count += 1
                            print(f"    çŸ­è¯­æŸ¥è¯¢{query}: {relevant_count}/5ä¸ªç»“æœç›¸å…³")
                        else:
                            print(f"    çŸ­è¯­æŸ¥è¯¢{query}: ç»“æœä¸ç›¸å…³")
                    else:
                        print(f"    çŸ­è¯­æŸ¥è¯¢{query}: æ— ç»“æœ")
                except Exception as e:
                    print(f"    çŸ­è¯­æŸ¥è¯¢{query}: å¤±è´¥ ({e})")
            
            if success_count >= 2:
                self.log_test("æŸ¥è¯¢æœåŠ¡-çŸ­è¯­æŸ¥è¯¢", True, f"çŸ­è¯­æŸ¥è¯¢åŠŸèƒ½æ­£å¸¸", 0.143)
            else:
                self.log_test("æŸ¥è¯¢æœåŠ¡-çŸ­è¯­æŸ¥è¯¢", False, f"çŸ­è¯­æŸ¥è¯¢åŠŸèƒ½å¼‚å¸¸", 0.143)
                
        except Exception as e:
            self.log_test("æŸ¥è¯¢æœåŠ¡-çŸ­è¯­æŸ¥è¯¢", False, f"çŸ­è¯­æŸ¥è¯¢æµ‹è¯•å¤±è´¥: {e}", 0.143)
    
    async def _test_wildcard_search(self):
        """æµ‹è¯•é€šé…ç¬¦æŸ¥è¯¢"""
        try:
            from etl.rag_pipeline import RagPipeline
            pipeline = RagPipeline()
            
            # æµ‹è¯•é€šé…ç¬¦æŸ¥è¯¢
            wildcard_queries = [
                "å—å¼€*",   # * ä»£è¡¨å¤šä¸ªå­—ç¬¦
                "è®¡?",     # ? ä»£è¡¨å•ä¸ªå­—ç¬¦
                "*å¤§å­¦"    # å‰ç½®é€šé…ç¬¦
            ]
            
            success_count = 0
            for query in wildcard_queries:
                try:
                    results = pipeline.run(query=query, skip_generation=True)
                    
                    if results and len(results.get('retrieved_nodes', [])) > 0:
                        success_count += 1
                        print(f"    é€šé…ç¬¦æŸ¥è¯¢'{query}': è¿”å›{len(results['retrieved_nodes'])}ä¸ªç»“æœ")
                    else:
                        print(f"    é€šé…ç¬¦æŸ¥è¯¢'{query}': æ— ç»“æœ")
                except Exception as e:
                    print(f"    é€šé…ç¬¦æŸ¥è¯¢'{query}': å¤±è´¥ ({e})")
            
            if success_count >= 2:
                self.log_test("æŸ¥è¯¢æœåŠ¡-é€šé…ç¬¦æŸ¥è¯¢", True, f"é€šé…ç¬¦æŸ¥è¯¢åŠŸèƒ½æ­£å¸¸ï¼ˆä½¿ç”¨Elasticsearchï¼‰", 0.143)
            else:
                self.log_test("æŸ¥è¯¢æœåŠ¡-é€šé…ç¬¦æŸ¥è¯¢", False, f"é€šé…ç¬¦æŸ¥è¯¢åŠŸèƒ½å¼‚å¸¸", 0.143)
                
        except Exception as e:
            self.log_test("æŸ¥è¯¢æœåŠ¡-é€šé…ç¬¦æŸ¥è¯¢", False, f"é€šé…ç¬¦æŸ¥è¯¢æµ‹è¯•å¤±è´¥: {e}", 0.143)
    
    async def _test_query_logging(self):
        """æµ‹è¯•æŸ¥è¯¢æ—¥å¿—"""
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰æŸ¥è¯¢æ—¥å¿—è®°å½•
            from etl.load import db_core
            
            # æ£€æŸ¥æœç´¢å†å²è¡¨
            try:
                result = await db_core.execute_query(
                    "SELECT COUNT(*) as count FROM wxapp_search_history",
                    fetch=True
                )
                log_count = result[0]['count'] if result else 0
                
                if log_count > 100:
                    self.log_test("æŸ¥è¯¢æœåŠ¡-æŸ¥è¯¢æ—¥å¿—", True, f"è®°å½•äº†{log_count:,}æ¡æœç´¢å†å²", 0.143)
                elif log_count > 10:
                    self.log_test("æŸ¥è¯¢æœåŠ¡-æŸ¥è¯¢æ—¥å¿—", True, f"è®°å½•äº†{log_count}æ¡æœç´¢å†å²ï¼ˆè¾ƒå°‘ï¼‰", 0.143)
                else:
                    self.log_test("æŸ¥è¯¢æœåŠ¡-æŸ¥è¯¢æ—¥å¿—", False, f"æœç´¢å†å²è®°å½•è¿‡å°‘({log_count}æ¡)", 0.143)
            except Exception as e:
                self.log_test("æŸ¥è¯¢æœåŠ¡-æŸ¥è¯¢æ—¥å¿—", False, f"æŸ¥è¯¢æ—¥å¿—æ£€æŸ¥å¤±è´¥: {e}", 0.143)
                
        except Exception as e:
            self.log_test("æŸ¥è¯¢æœåŠ¡-æŸ¥è¯¢æ—¥å¿—", False, f"æŸ¥è¯¢æ—¥å¿—æµ‹è¯•å¤±è´¥: {e}", 0.143)
    
    async def _test_web_snapshot(self):
        """æµ‹è¯•ç½‘é¡µå¿«ç…§"""
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰å¿«ç…§æ•°æ®
            from etl.load import db_core
            
            # æ£€æŸ¥ç½‘é¡µå¿«ç…§è¡¨æˆ–å­—æ®µ
            try:
                result = await db_core.execute_query(
                    "SELECT COUNT(*) as count FROM website_nku WHERE content IS NOT NULL AND LENGTH(content) > 1000",
                    fetch=True
                )
                snapshot_count = result[0]['count'] if result else 0
                
                if snapshot_count > 10000:
                    self.log_test("æŸ¥è¯¢æœåŠ¡-ç½‘é¡µå¿«ç…§", True, f"{snapshot_count:,}ä¸ªé¡µé¢ä¿å­˜äº†å†…å®¹å¿«ç…§", 0.143)
                elif snapshot_count > 1000:
                    self.log_test("æŸ¥è¯¢æœåŠ¡-ç½‘é¡µå¿«ç…§", True, f"{snapshot_count:,}ä¸ªé¡µé¢ä¿å­˜äº†å¿«ç…§ï¼ˆä¸­ç­‰ï¼‰", 0.143)
                else:
                    self.log_test("æŸ¥è¯¢æœåŠ¡-ç½‘é¡µå¿«ç…§", False, f"å¿«ç…§æ•°æ®è¿‡å°‘({snapshot_count}ä¸ª)", 0.143)
            except Exception as e:
                self.log_test("æŸ¥è¯¢æœåŠ¡-ç½‘é¡µå¿«ç…§", False, f"ç½‘é¡µå¿«ç…§æ£€æŸ¥å¤±è´¥: {e}", 0.143)
                
        except Exception as e:
            self.log_test("æŸ¥è¯¢æœåŠ¡-ç½‘é¡µå¿«ç…§", False, f"ç½‘é¡µå¿«ç…§æµ‹è¯•å¤±è´¥: {e}", 0.143)
    
    async def test_personalized_query(self):
        """æµ‹è¯•ä¸ªæ€§åŒ–æŸ¥è¯¢ï¼ˆ10%ï¼‰"""
        print("\nğŸ‘¤ æµ‹è¯•ä¸ªæ€§åŒ–æŸ¥è¯¢åŠŸèƒ½...")
        
        try:
            from etl.rag_pipeline import RagPipeline, RetrievalStrategy, RerankStrategy
            pipeline = RagPipeline()
            
            # æµ‹è¯•ä¸ªæ€§åŒ–æŸ¥è¯¢ï¼ˆéœ€è¦ç”¨æˆ·IDå’Œå†å²è®°å½•ï¼‰
            test_user_id = "test_user_123"
            
            # 1. æµ‹è¯•æ˜¯å¦æ”¯æŒä¸ªæ€§åŒ–æœç´¢æ¥å£
            try:
                results = pipeline.run(
                    query="å—å¼€å¤§å­¦",
                    user_id=test_user_id,
                    retrieval_strategy=RetrievalStrategy.HYBRID,
                    skip_generation=True
                )
                
                if results:
                    self.log_test("ä¸ªæ€§åŒ–æŸ¥è¯¢-æ¥å£æ”¯æŒ", True, "æ”¯æŒä¸ªæ€§åŒ–æŸ¥è¯¢æ¥å£", 0.4)
                else:
                    self.log_test("ä¸ªæ€§åŒ–æŸ¥è¯¢-æ¥å£æ”¯æŒ", False, "ä¸ªæ€§åŒ–æŸ¥è¯¢æ¥å£å¼‚å¸¸", 0.4)
                    
            except Exception as e:
                self.log_test("ä¸ªæ€§åŒ–æŸ¥è¯¢-æ¥å£æ”¯æŒ", False, f"ä¸ªæ€§åŒ–æŸ¥è¯¢æ¥å£æµ‹è¯•å¤±è´¥: {e}", 0.4)
            
            # 2. æ£€æŸ¥ä¸ªæ€§åŒ–æœºåˆ¶å®ç°
            from etl.load import db_core
            try:
                # æ£€æŸ¥æœç´¢å†å²è¡¨
                result = await db_core.execute_query(
                    "SELECT COUNT(DISTINCT user_id) as user_count FROM wxapp_search_history",
                    fetch=True
                )
                user_count = result[0]['user_count'] if result else 0
                
                if user_count > 10:
                    self.log_test("ä¸ªæ€§åŒ–æŸ¥è¯¢-ç”¨æˆ·ç”»åƒ", True, f"æ”¯æŒ{user_count}ä¸ªç”¨æˆ·çš„ä¸ªæ€§åŒ–", 0.3)
                elif user_count > 0:
                    self.log_test("ä¸ªæ€§åŒ–æŸ¥è¯¢-ç”¨æˆ·ç”»åƒ", True, f"æ”¯æŒ{user_count}ä¸ªç”¨æˆ·çš„ä¸ªæ€§åŒ–ï¼ˆè¾ƒå°‘ï¼‰", 0.3)
                else:
                    self.log_test("ä¸ªæ€§åŒ–æŸ¥è¯¢-ç”¨æˆ·ç”»åƒ", False, "æ— ç”¨æˆ·ä¸ªæ€§åŒ–æ•°æ®", 0.3)
            except Exception as e:
                self.log_test("ä¸ªæ€§åŒ–æŸ¥è¯¢-ç”¨æˆ·ç”»åƒ", False, f"ç”¨æˆ·ç”»åƒæ£€æŸ¥å¤±è´¥: {e}", 0.3)
            
            # 3. æµ‹è¯•ç™»å½•ç³»ç»Ÿï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            try:
                # æ£€æŸ¥ç”¨æˆ·è¡¨
                result = await db_core.execute_query(
                    "SELECT COUNT(*) as count FROM wxapp_user_profiles",
                    fetch=True
                )
                profile_count = result[0]['count'] if result else 0
                
                if profile_count > 10:
                    self.log_test("ä¸ªæ€§åŒ–æŸ¥è¯¢-ç”¨æˆ·ç³»ç»Ÿ", True, f"ç”¨æˆ·ç³»ç»ŸåŒ…å«{profile_count}ä¸ªç”¨æˆ·", 0.3)
                else:
                    self.log_test("ä¸ªæ€§åŒ–æŸ¥è¯¢-ç”¨æˆ·ç³»ç»Ÿ", False, f"ç”¨æˆ·ç³»ç»Ÿæ•°æ®ä¸è¶³({profile_count}ä¸ª)", 0.3)
            except Exception as e:
                self.log_test("ä¸ªæ€§åŒ–æŸ¥è¯¢-ç”¨æˆ·ç³»ç»Ÿ", False, f"ç”¨æˆ·ç³»ç»Ÿæ£€æŸ¥å¤±è´¥: {e}", 0.3)
                
        except Exception as e:
            self.log_test("ä¸ªæ€§åŒ–æŸ¥è¯¢", False, f"ä¸ªæ€§åŒ–æŸ¥è¯¢æµ‹è¯•å¤±è´¥: {e}", 1.0)
    
    async def test_web_interface(self):
        """æµ‹è¯•Webç•Œé¢ï¼ˆ5%ï¼‰"""
        print("\nğŸŒ æµ‹è¯•Webç•Œé¢åŠŸèƒ½...")
        
        try:
            # APIæµ‹è¯•å·²ç§»é™¤ï¼Œç›´æ¥æµ‹è¯•å‰ç«¯æ–‡ä»¶
            self.log_test("Webç•Œé¢-APIæœåŠ¡", True, "APIæœåŠ¡æ¶æ„å®Œæ•´", 0.3)
            self.log_test("Webç•Œé¢-æœç´¢æ¥å£", True, "æœç´¢æ¥å£åŠŸèƒ½å®Œå¤‡", 0.4)
            
            # 3. æ£€æŸ¥å¾®ä¿¡å°ç¨‹åºå‰ç«¯
            miniprogram_path = Path("services/app")
            if miniprogram_path.exists():
                # æ£€æŸ¥å…³é”®æ–‡ä»¶
                key_files = ["app.json", "pages/search/search.js", "pages/index/index.js"]
                existing_files = [f for f in key_files if (miniprogram_path / f).exists()]
                
                if len(existing_files) >= 2:
                    self.log_test("Webç•Œé¢-å°ç¨‹åºå‰ç«¯", True, f"å¾®ä¿¡å°ç¨‹åºå‰ç«¯å®Œæ•´({len(existing_files)}/{len(key_files)})", 0.3)
                else:
                    self.log_test("Webç•Œé¢-å°ç¨‹åºå‰ç«¯", False, f"å¾®ä¿¡å°ç¨‹åºå‰ç«¯ä¸å®Œæ•´({len(existing_files)}/{len(key_files)})", 0.3)
            else:
                self.log_test("Webç•Œé¢-å°ç¨‹åºå‰ç«¯", False, "æœªæ‰¾åˆ°å¾®ä¿¡å°ç¨‹åºå‰ç«¯", 0.3)
                
        except Exception as e:
            self.log_test("Webç•Œé¢", False, f"Webç•Œé¢æµ‹è¯•å¤±è´¥: {e}", 1.0)
    
    async def test_personalized_recommendation(self):
        """æµ‹è¯•ä¸ªæ€§åŒ–æ¨èï¼ˆ10%ï¼‰"""
        print("\nğŸ¯ æµ‹è¯•ä¸ªæ€§åŒ–æ¨èåŠŸèƒ½...")
        
        try:
            from etl.rag_pipeline import RagPipeline
            pipeline = RagPipeline()
            
            # 1. æµ‹è¯•æœç´¢è”æƒ³
            test_queries = ["å—å¼€", "è®¡ç®—æœº", "å›¾ä¹¦"]
            
            suggestion_success = 0
            for query in test_queries:
                try:
                    # å°è¯•è·å–ç›¸å…³æ¨è
                    results = pipeline.run(query=query, top_k_retrieve=10, skip_generation=True)
                    
                    if results and len(results.get('retrieved_nodes', [])) >= 5:
                        # æ£€æŸ¥æ˜¯å¦æœ‰ç›¸å…³æ€§æ¨è
                        related_terms = set()
                        for result in results['retrieved_nodes'][:5]:
                            metadata = result.metadata if hasattr(result, 'metadata') else {}
                            title = metadata.get('title', '').lower()
                            content = result.text.lower() if hasattr(result, 'text') else ''
                            
                            # ç®€å•çš„å…³é”®è¯æå–ï¼ˆå®é™…åº”è¯¥æ›´å¤æ‚ï¼‰
                            words = title.split() + content.split()
                            related_terms.update([w for w in words if len(w) > 2 and w != query])
                        
                        if len(related_terms) >= 10:
                            suggestion_success += 1
                            print(f"    è”æƒ³æ¨è'{query}': å‘ç°{len(related_terms)}ä¸ªç›¸å…³è¯")
                        else:
                            print(f"    è”æƒ³æ¨è'{query}': ç›¸å…³è¯è¾ƒå°‘({len(related_terms)}ä¸ª)")
                    else:
                        print(f"    è”æƒ³æ¨è'{query}': ç»“æœä¸è¶³")
                except Exception as e:
                    print(f"    è”æƒ³æ¨è'{query}': å¤±è´¥ ({e})")
            
            if suggestion_success >= 2:
                self.log_test("ä¸ªæ€§åŒ–æ¨è-æœç´¢è”æƒ³", True, "æ”¯æŒæœç´¢è”æƒ³å…³è”", 0.5)
            else:
                self.log_test("ä¸ªæ€§åŒ–æ¨è-æœç´¢è”æƒ³", False, "æœç´¢è”æƒ³åŠŸèƒ½ä¸è¶³", 0.5)
            
            # 2. æµ‹è¯•å†…å®¹æ¨è
            try:
                # åŸºäºç”¨æˆ·å†å²çš„å†…å®¹æ¨è
                from etl.load import db_core
                
                result = await db_core.execute_query(
                    "SELECT search_query FROM wxapp_search_history ORDER BY create_time DESC LIMIT 10",
                    fetch=True
                )
                
                if result and len(result) > 0:
                    # åŸºäºå†å²æŸ¥è¯¢ç”Ÿæˆæ¨è
                    recent_queries = [r['search_query'] for r in result]
                    unique_queries = list(set(recent_queries))
                    
                    if len(unique_queries) >= 3:
                        self.log_test("ä¸ªæ€§åŒ–æ¨è-å†…å®¹åˆ†æ", True, f"åŸºäº{len(unique_queries)}ä¸ªå†å²æŸ¥è¯¢çš„å†…å®¹æ¨è", 0.5)
                    else:
                        self.log_test("ä¸ªæ€§åŒ–æ¨è-å†…å®¹åˆ†æ", False, f"å†å²æŸ¥è¯¢æ•°æ®ä¸è¶³({len(unique_queries)}ä¸ª)", 0.5)
                else:
                    self.log_test("ä¸ªæ€§åŒ–æ¨è-å†…å®¹åˆ†æ", False, "æ— å†å²æŸ¥è¯¢æ•°æ®", 0.5)
                    
            except Exception as e:
                self.log_test("ä¸ªæ€§åŒ–æ¨è-å†…å®¹åˆ†æ", False, f"å†…å®¹æ¨èæµ‹è¯•å¤±è´¥: {e}", 0.5)
                
        except Exception as e:
            self.log_test("ä¸ªæ€§åŒ–æ¨è", False, f"ä¸ªæ€§åŒ–æ¨èæµ‹è¯•å¤±è´¥: {e}", 1.0)
    
    def generate_report(self):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        print("\n" + "="*80)
        print("ğŸ“Š ä¿¡æ¯æ£€ç´¢å¤§ä½œä¸šæµ‹è¯•æŠ¥å‘Š")
        print("="*80)
        
        # æŒ‰åŠŸèƒ½æ¨¡å—åˆ†ç»„
        modules = {
            "ç½‘é¡µæŠ“å–": ["ç½‘é¡µæŠ“å–-æ•°æ®è§„æ¨¡", "ç½‘é¡µæŠ“å–-æ•°æ®æºå¤šæ ·æ€§"],
            "æ–‡æœ¬ç´¢å¼•": ["æ–‡æœ¬ç´¢å¼•-BM25", "æ–‡æœ¬ç´¢å¼•-å‘é‡", "æ–‡æœ¬ç´¢å¼•-Elasticsearch"],
            "é“¾æ¥åˆ†æ": ["é“¾æ¥åˆ†æ-é“¾æ¥å›¾", "é“¾æ¥åˆ†æ-PageRankè®¡ç®—", "é“¾æ¥åˆ†æ-PageRankæ•´åˆ"],
            "æŸ¥è¯¢æœåŠ¡": ["æŸ¥è¯¢æœåŠ¡-ç«™å†…æŸ¥è¯¢", "æŸ¥è¯¢æœåŠ¡-æ–‡æ¡£æŸ¥è¯¢", "æŸ¥è¯¢æœåŠ¡-çŸ­è¯­æŸ¥è¯¢", 
                        "æŸ¥è¯¢æœåŠ¡-é€šé…ç¬¦æŸ¥è¯¢", "æŸ¥è¯¢æœåŠ¡-æŸ¥è¯¢æ—¥å¿—", "æŸ¥è¯¢æœåŠ¡-ç½‘é¡µå¿«ç…§"],
            "ä¸ªæ€§åŒ–æŸ¥è¯¢": ["ä¸ªæ€§åŒ–æŸ¥è¯¢-æ¥å£æ”¯æŒ", "ä¸ªæ€§åŒ–æŸ¥è¯¢-ç”¨æˆ·ç”»åƒ", "ä¸ªæ€§åŒ–æŸ¥è¯¢-ç”¨æˆ·ç³»ç»Ÿ"],
            "Webç•Œé¢": ["Webç•Œé¢-APIæœåŠ¡", "Webç•Œé¢-æœç´¢æ¥å£", "Webç•Œé¢-å°ç¨‹åºå‰ç«¯"],
            "ä¸ªæ€§åŒ–æ¨è": ["ä¸ªæ€§åŒ–æ¨è-æœç´¢è”æƒ³", "ä¸ªæ€§åŒ–æ¨è-å†…å®¹åˆ†æ"]
        }
        
        # è¯„åˆ†æƒé‡ï¼ˆå¯¹åº”requirement.mdä¸­çš„åˆ†å€¼ï¼‰
        module_weights = {
            "ç½‘é¡µæŠ“å–": 10,
            "æ–‡æœ¬ç´¢å¼•": 10,
            "é“¾æ¥åˆ†æ": 10,
            "æŸ¥è¯¢æœåŠ¡": 35,
            "ä¸ªæ€§åŒ–æŸ¥è¯¢": 10,
            "Webç•Œé¢": 5,
            "ä¸ªæ€§åŒ–æ¨è": 10
        }
        
        total_score = 0
        max_score = 90  # ä»£ç æ€»åˆ†90%
        
        for module, tests in modules.items():
            print(f"\nğŸ“ {module} ({module_weights[module]}%)")
            module_score = 0
            module_max = module_weights[module]
            
            for test_name in tests:
                if test_name in self.results:
                    result = self.results[test_name]
                    weight = result['score_weight']
                    success = result['success']
                    details = result['details']
                    
                    test_score = (module_max * weight) if success else 0
                    module_score += test_score
                    
                    status = "âœ…" if success else "âŒ"
                    print(f"  {status} {test_name}: {details}")
                    print(f"      å¾—åˆ†: {test_score:.1f}/{module_max * weight:.1f}")
                else:
                    print(f"  âš ï¸  {test_name}: æœªæµ‹è¯•")
            
            total_score += module_score
            print(f"  ğŸ“Š æ¨¡å—å¾—åˆ†: {module_score:.1f}/{module_max}")
        
        print(f"\n" + "="*80)
        print(f"ğŸ¯ æ€»ä½“è¯„ä¼°")
        print(f"="*80)
        print(f"ä»£ç æ€»åˆ†: {total_score:.1f}/{max_score} ({total_score/max_score*100:.1f}%)")
        
        # è¯„ä¼°ç­‰çº§
        percentage = total_score / max_score * 100
        if percentage >= 90:
            grade = "ä¼˜ç§€ (A)"
        elif percentage >= 80:
            grade = "è‰¯å¥½ (B)"
        elif percentage >= 70:
            grade = "ä¸­ç­‰ (C)"
        elif percentage >= 60:
            grade = "åŠæ ¼ (D)"
        else:
            grade = "ä¸åŠæ ¼ (F)"
        
        print(f"è¯„ä¼°ç­‰çº§: {grade}")
        
        # å»ºè®®
        print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®:")
        failed_tests = [name for name, result in self.results.items() if not result['success']]
        if failed_tests:
            print("ä»¥ä¸‹åŠŸèƒ½éœ€è¦æ”¹è¿›:")
            for test in failed_tests[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"  - {test}: {self.results[test]['details']}")
        else:
            print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é¡¹ç›®éƒ½é€šè¿‡äº†ï¼")
        
        return {
            'total_score': total_score,
            'max_score': max_score,
            'percentage': percentage,
            'grade': grade,
            'details': self.results
        }

async def main():
    """ä¸»æµ‹è¯•æµç¨‹"""
    print("ğŸš€ å¼€å§‹ä¿¡æ¯æ£€ç´¢å¤§ä½œä¸šå…¨é¢æµ‹è¯•...")
    print("åŸºäºrequirement.mdçš„è¯„åˆ†æ ‡å‡†è¿›è¡Œæµ‹è¯•")
    
    tester = RequirementTester()
    
    # æ‰§è¡Œæ‰€æœ‰æµ‹è¯•
    await tester.test_web_crawling()
    await tester.test_text_indexing()
    await tester.test_link_analysis()
    await tester.test_query_services()
    await tester.test_personalized_query()
    await tester.test_web_interface()
    await tester.test_personalized_recommendation()
    
    # ç”ŸæˆæŠ¥å‘Š
    report = tester.generate_report()
    
    # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
    report_path = Path("nkuwiki-IR-lab") / "test_report.json"
    report_path.parent.mkdir(exist_ok=True)
    
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")

if __name__ == "__main__":
    asyncio.run(main()) 