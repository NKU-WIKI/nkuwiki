---
description: 
globs: 
alwaysApply: true
---
# NKUWiki ETL 开发规范

本文档为 `nkuwiki` 项目的ETL（数据提取、转换、加载）系统开发提供指导规范，旨在确保代码的一致性、模块化和可维护性。

## 1. ETL 核心设计理念

项目ETL流程遵循**阶段式解耦**的设计原则，主要分为三个独立阶段：

1.  **数据采集 (Crawling)**: 从不同数据源（网站、公众号、校园集市等）抓取原始信息，并以标准化的JSON格式存入统一的数据湖。
2.  **数据处理与索引 (Processing & Indexing)**: 对原始数据进行转换、分块、向量化，并加载到Qdrant向量数据库和Elasticsearch全文索引中。
3.  **洞察生成与应用 (Insight & Application)**: 基于新增数据，利用大语言模型生成结构化的分析洞察，并存入MySQL数据库，为上层应用提供数据支持。

`etl/daily_pipeline.py` 是驱动阶段2和3的核心任务编排器。

---

## 2. 阶段一：数据采集 (`etl/crawler/`)

### 任务与职责
- 从指定的数据源抓取原始数据。
- 将抓取到的数据处理成**标准JSON格式**。
- 将JSON文件存储到统一的数据湖 `/data/raw/`。

### 开发新爬虫的步骤

1.  **创建爬虫脚本**:
    - 在 `etl/crawler/` 下为新数据源创建一个独立的`py`文件，例如 `etl/crawler/bilibili_spider.py`。
    - 爬虫逻辑应包含错误处理、重试机制，并记录详细日志。

2.  **标准化输出**:
    - 爬虫的最终输出**必须**是一个或多个 `.json` 文件。
    - 每个JSON文件应包含单个数据单元（如一篇文章、一个帖子）的完整信息。
    - JSON文件必须包含以下核心字段，以确保下游处理流程可以正确识别：
      ```json
      {
        "id": "数据唯一标识，建议使用URL的MD5哈希",
        "title": "标题",
        "content": "正文内容",
        "url": "原始链接",
        "platform": "平台标识 (例如: 'website', 'wechat', 'market')",
        "source": "具体来源 (例如: 'nkunews', 'nkuyouth', 'market_sell')",
        "publish_time": "发布时间 (ISO 8601格式, e.g., '2023-10-27T10:00:00+08:00')"
      }
      ```

3.  **统一存储**:
    - 所有生成的 `.json` 文件必须存放到 `/data/raw/` 目录。
    - 存储路径应遵循规范：`/data/raw/{platform}/{source}/{year}{month}/{article_id}.json`。其中 `article_id` 通常是文件内容的md5。

---

## 3. 阶段二 & 三：任务编排 (`etl/daily_pipeline.py`)

`daily_pipeline.py` 是整个ETL流程的"指挥官"，它通过灵活的命令行接口，驱动数据的处理、索引和洞察生成。

### `daily_pipeline.py` 的核心流程

该脚本的工作流由 `--steps` 参数控制，主要包含三个可组合的步骤：`scan`, `index`, `insight`。

1.  **文件扫描 (`scan`)**:
    - **触发**: 总是执行（除非只单独运行 `index` 或 `insight` 且不带 `scan`）。
    - **动作**: 调用 `find_new_files_in_timespan` 函数，根据 `--start_time`, `--end_time`, `--hours` 等参数确定时间窗口，在 `/data/raw` 中高效查找此时间段内发布的 `.json` 文件。
    - **输出**: 一个待处理的文件路径列表。

2.  **建立索引 (`index`)**:
    - **触发**: 当 `--steps` 包含 `index` 时执行。
    - **依赖**: `scan` 步骤的输出（文件路径列表）。
    - **动作**:
        - **节点化**: 调用 `process_files_to_nodes`，将文件内容异步地读取、处理，并使用 `ChunkCacheManager` 分块，转换为 `llama_index` 的 `TextNode` 对象。
        - **索引**: 调用 `build_qdrant_indexes`，将 `TextNode` 列表批量送入Qdrant建立向量索引。

3.  **生成洞察 (`insight`)**:
    - **触发**: 当 `--steps` 包含 `insight` 时执行。
    - **依赖**: `scan` 步骤的输出（文件路径列表）。
    - **动作**:
        - **分类**: 将文件按来源分为三类：**官方** (`website`平台、官方公众号)、**社区** (社团及民间公众号)、**集市** (`market`平台)。
        - **提示词构建**: 为每个非空分类构建一个详细的Prompt。
        - **AI生成**: 调用 `core.agent.text_generator.generate_structured_json`，请求大模型返回结构化的JSON洞察报告。
        - **入库**: 将生成的多条洞察存入MySQL的 `insights` 表。

### 如何使用 `daily_pipeline.py`

```bash
# 示例1: 执行完整流程（扫描、索引、洞察），处理过去24小时的数据
python etl/daily_pipeline.py --steps scan,index,insight

# 示例2: 只执行扫描和索引，不生成洞察
python etl/daily_pipeline.py --steps scan,index

# 示例3: 处理指定时间范围内的数据，且只生成洞察
python etl/daily_pipeline.py --steps scan,insight --start_time "2023-10-26" --end_time "2023-10-27"

# 示例4: 只处理过去2小时内'wechat'平台的数据，并建立索引
python etl/daily_pipeline.py --hours 2 --platform wechat --steps scan,index

# 示例5: 使用'all'关键字代表所有步骤
python etl/daily_pipeline.py --steps all # 等同于 scan,index,insight
```

## 4. 开发最佳实践

- **异步优先**: 对于IO密集型操作（如读写文件、数据库访问），应优先使用 `async/await` 范式。
- **配置驱动**: 
  - 所有配置项（数据库凭据、路径、模型名称等）均通过根目录的 `config.py` 和 `config.json` 进行管理。
  - **ETL模块的常量配置**：为了方便管理和复用，所有与ETL流程相关的配置都在 `etl/__init__.py` 中被加载、处理，并定义为模块级常量（如 `DB_HOST`, `RAW_PATH`, `EMBEDDING_MODEL_PATH` 等）。
  - **最佳实践**: 在ETL模块内部，应直接从 `etl` 包导入这些已定义好的常量，而不是重复调用 `Config` 对象。这保证了配置的统一和代码的简洁。
- **日志记录**: 在关键步骤和异常处理中添加清晰的日志记录，使用 `core/utils/logger.py` 中的 `register_logger`。
- **依赖注入**: 核心组件（如 `QdrantIndexer`, `ChunkCacheManager`）应在需要时才实例化，而不是作为全局变量。
- **环境隔离**: 确保本地开发环境与生产环境的数据目录、配置等相互隔离。
