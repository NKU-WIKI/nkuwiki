---
description: 
globs: 
alwaysApply: false
---
# ETL 开发规范

## 爬虫架构概述

### 双架构并存设计
项目采用两种爬虫架构并存的设计：

1. **BaseCrawler架构** - 基于Playwright的现代爬虫框架
2. **Scrapy架构** - 传统的分布式爬虫框架

### 基础架构原则
- 所有爬虫必须实现反反爬策略
- 支持代理池和随机请求头
- 统一日志记录和错误处理
- 数据保存到统一格式

## BaseCrawler爬虫开发规范

### 1. 基础爬虫类继承
```python
from etl.crawler import crawler_logger, config, RAW_PATH
from etl.crawler.base_crawler import BaseCrawler
from etl.utils.date import parse_date
from etl.utils.file import clean_filename

class MyCrawler(BaseCrawler):
    """自定义爬虫类"""
    
    def __init__(self, tag: str = "default", debug: bool = False, 
                 headless: bool = True, use_proxy: bool = False):
        # 必须设置的属性
        self.platform = "myplatform"  # 平台名称
        self.tag = tag
        self.content_type = "article"  # 内容类型：article/notice/event
        self.base_url = "https://example.com/"
        
        # 调用父类初始化
        super().__init__(debug, headless, use_proxy)
        
        # 设置cookie初始化URL
        self.cookie_init_url = "https://example.com/"
        
    async def login_for_cookies(self) -> Dict[str, str]:
        """登录获取cookies（如果需要登录）"""
        try:
            await self.page.goto(self.base_url)
            # 实现登录逻辑
            # 返回cookies字典
            return {cookie['name']: cookie['value'] for cookie in await self.context.cookies()}
        except Exception as e:
            self.logger.error(f"登录失败: {e}")
            raise
    
    async def scrape(self, max_count: int = 100, **kwargs):
        """主要爬取方法"""
        try:
            # 初始化异步组件
            await self.async_init()
            
            # 获取已爬取URL集合
            scraped_urls = self.get_scraped_original_urls()
            
            # 实现具体爬取逻辑
            await self.scrape_data(scraped_urls, max_count)
            
        except Exception as e:
            self.logger.error(f"爬取失败: {e}")
        finally:
            await self.close()
    
    async def download(self, **kwargs):
        """下载详细内容"""
        try:
            await self.async_init()
            # 实现下载逻辑
            await self.download_content()
        except Exception as e:
            self.logger.error(f"下载失败: {e}")
        finally:
            await self.close()
```

### 2. 反反爬策略实现
```python
class AntiDetectionMixin:
    """反反爬检测混入类"""
    
    async def inject_anti_detection_script(self):
        """注入反检测脚本"""
        script_path = Path(__file__).parent / 'init_script.js'
        if script_path.exists():
            script_content = script_path.read_text(encoding='utf-8')
            await self.page.add_init_script(script_content)
    
    async def random_sleep(self, min_delay: float = 1.0, max_delay: float = 3.0):
        """智能延时策略"""
        if self.debug:
            return
        
        # 模拟人类行为
        await self.simulate_human_behavior()
        
        # 随机延时
        delay = random.uniform(min_delay, max_delay)
        await asyncio.sleep(delay)
    
    async def simulate_human_behavior(self):
        """模拟人类浏览行为"""
        # 随机滚动
        scroll_steps = [
            (random.randint(100, 300), 500),
            (random.randint(400, 600), 800),
            (random.randint(700, 900), 1000)
        ]
        
        for pos, delay in scroll_steps:
            await self.page.evaluate(f"window.scrollTo(0, {pos})")
            await asyncio.sleep(delay / 1000.0)
        
        # 随机鼠标移动
        await self.page.mouse.move(
            random.randint(0, 500), 
            random.randint(0, 300),
            steps=random.randint(5, 10)
        )
    
    async def rotate_proxy(self):
        """代理轮换"""
        if self.use_proxy and self.proxy_pool:
            self.current_proxy = random.choice(self.proxy_pool)
            self.logger.info(f"切换代理: {self.current_proxy}")
```

## Scrapy爬虫开发规范

### 1. Scrapy项目结构
```python
# spiders/wiki.py
import scrapy
from ..items import ContentItem, LinkItem
from ..parse_different_college import url_maps

class WikiSpider(scrapy.Spider):
    name = 'wiki_spider'
    allowed_domains = ['nankai.edu.cn']
    start_urls = list(url_maps.values())
    
    custom_settings = {
        'ITEM_PIPELINES': {
            'counselor.pipelines.LinkGraphPipeline': 300,
            'counselor.pipelines.WikiPipeline': 800
        },
        'LOG_FILE': f'./log/{date.today().strftime("%Y-%m-%d")}.txt'
    }
    
    def parse(self, response):
        """处理起始URL"""
        yield scrapy.Request(response.url, callback=self.parse_category)
    
    def parse_category(self, response):
        """处理分类页面"""
        if response.status not in range(200, 300):
            self.logger.error(f'访问失败: {response.url}, 状态码: {response.status}')
            return
        
        # 提取链接
        links = self.extract_valid_links(response)
        
        for link in links:
            # 生成链接关系项
            link_item = LinkItem()
            link_item['source_url'] = response.url
            link_item['target_url'] = link
            yield link_item
            
            # 判断是否为内容页面
            if self.is_content_page(link):
                yield scrapy.Request(link, callback=self.parse_content)
            else:
                yield scrapy.Request(link, callback=self.parse_category)
    
    def parse_content(self, response):
        """处理内容页面"""
        item = ContentItem()
        
        # 检测文档类型
        if self.is_document_url(response.url):
            item['title'] = os.path.basename(response.url)
            item['file_url'] = response.url
            item['content'] = ''  # 文档内容后续解析
        else:
            item['title'] = self.extract_title(response)
            item['content'] = self.extract_content(response)
            item['html_content'] = response.body  # 保存网页快照
        
        # 通用字段
        item['url'] = response.url
        item['push_time'] = self.extract_publish_time(response)
        item['source'] = self.determine_source(response.url)
        
        yield item
```

### 2. Items定义规范
```python
# items.py
import scrapy

class ContentItem(scrapy.Item):
    """内容项目"""
    title = scrapy.Field()          # 标题
    push_time = scrapy.Field()      # 发布时间
    source = scrapy.Field()         # 来源
    url = scrapy.Field()            # URL地址
    content = scrapy.Field()        # 内容
    file_url = scrapy.Field()       # 文档URL（如PDF）
    html_content = scrapy.Field()   # HTML内容（用于快照）

class LinkItem(scrapy.Item):
    """链接关系项目（用于PageRank）"""
    source_url = scrapy.Field()     # 源URL
    target_url = scrapy.Field()     # 目标URL
```

### 3. Pipeline数据处理
```python
# pipelines.py
import sqlite3
import hashlib
from pathlib import Path

class WikiPipeline:
    """内容处理管道"""
    
    def __init__(self):
        self.db_path = Path(__file__).parent / 'nk_database.db'
        self.conn = sqlite3.connect(self.db_path, check_same_thread=False)
        self.cursor = self.conn.cursor()
        self.lock = threading.Lock()
        
        # 快照存储路径
        self.snapshots_path = Path('data/raw/website/snapshots')
        self.snapshots_path.mkdir(parents=True, exist_ok=True)
    
    def process_item(self, item, spider):
        """处理单个项目"""
        if isinstance(item, ContentItem):
            return self.process_content_item(item, spider)
        elif isinstance(item, LinkItem):
            return self.process_link_item(item, spider)
        return item
    
    def process_content_item(self, item, spider):
        """处理内容项目"""
        # 保存网页快照
        if 'html_content' in item and item['html_content']:
            self.save_snapshot(item['url'], item['html_content'], spider)
            del item['html_content']  # 移除，避免存入数据库
        
        # 保存到数据库
        self.save_to_database(item, spider)
        return item
    
    def save_snapshot(self, url: str, html_content: bytes, spider):
        """保存网页快照"""
        try:
            url_hash = hashlib.sha256(url.encode('utf-8')).hexdigest()
            snapshot_file = self.snapshots_path / f"{url_hash}.html"
            
            with open(snapshot_file, 'wb') as f:
                f.write(html_content)
            
            spider.logger.info(f"网页快照已保存: {snapshot_file}")
        except Exception as e:
            spider.logger.error(f"保存网页快照失败: {e}")

class LinkGraphPipeline:
    """链接图管道"""
    
    def process_item(self, item, spider):
        """处理链接项目"""
        if isinstance(item, LinkItem):
            self.save_link_relation(item, spider)
        return item
    
    def save_link_relation(self, item, spider):
        """保存链接关系"""
        try:
            self.cursor.execute('''
                INSERT OR IGNORE INTO link_graph (source_url, target_url)
                VALUES (?, ?)
            ''', (item['source_url'], item['target_url']))
            self.conn.commit()
        except Exception as e:
            spider.logger.error(f"保存链接关系失败: {e}")
```

## 数据格式规范

### 1. 统一数据格式
```json
{
    "platform": "wechat/website/market/rednote/tiktok",
    "original_url": "带签名认证的原始链接",
    "title": "UTF-8编码标题",
    "author": "公众号/发布者名称",
    "publish_time": "发布时间 YYYY-mm-dd 或 YYYY-mm-dd HH:mm:ss",
    "scrape_time": "抓取时间 YYYY-mm-dd HH:mm:ss",
    "content_type": "article/notice/event",
    "content": "文章正文内容",
    "file_url": "PDF等文档的URL",
    "abstract": "AI生成的300字摘要",
    "keywords": "实体词抽取结果"
}
```

### 2. 文档类型检测
```python
def is_document_url(self, url: str) -> bool:
    """检测是否为文档链接"""
    doc_extensions = ['.pdf', '.docx', '.doc', '.xlsx', '.xls', '.ppt', '.pptx']
    return any(url.lower().endswith(ext) for ext in doc_extensions)

def is_content_page(self, url: str) -> bool:
    """判断是否为内容页面"""
    # 文档链接
    if self.is_document_url(url):
        return True
    
    # 特殊页面模式
    special_patterns = [
        'news/news-detail',
        '/info/',
        'mp.weixin.qq.com',
        '/post/',
        '/article/'
    ]
    
    for pattern in special_patterns:
        if pattern in url:
            return True
    
    # URL结尾模式检查
    import re
    content_pattern = r'\d+\.(html|shtml|chtml|htm)$|/post/\d+$|/article/\d+$'
    return bool(re.search(content_pattern, url))

def extract_valid_links(self, response) -> List[str]:
    """提取有效链接"""
    from urllib.parse import urlparse, urljoin
    
    base_url = f"{urlparse(response.url).scheme}://{urlparse(response.url).netloc}"
    links = []
    
    # 提取所有a标签的href属性
    href_list = response.css('a::attr(href)').getall()
    
    for href in href_list:
        if not href or href == '#':
            continue
        
        # 过滤无效链接
        if any(invalid in href for invalid in ['javascript:', 'void(0)', 'JavaScript:;']):
            continue
        
        if any(char in href for char in ['_', '<span', '@', '*']):
            continue
        
        # 转换为绝对URL
        if not href.startswith('http'):
            href = urljoin(base_url, href.replace('..', ''))
        
        # URL有效性检查
        if self.is_valid_url(href.strip()):
            links.append(href.strip())
    
    return list(set(links))  # 去重

def is_valid_url(self, url: str) -> bool:
    """URL有效性检查"""
    from urllib.parse import urlparse
    
    try:
        parsed = urlparse(url)
        return all([
            parsed.scheme in ['http', 'https'],
            parsed.netloc,
            parsed.netloc in self.allowed_domains if hasattr(self, 'allowed_domains') else True
        ])
    except Exception:
        return False
```

### 3. 内容提取工具函数
```python
def extract_title(self, response) -> str:
    """提取页面标题"""
    # 多种选择器优先级
    title_selectors = [
        'h1::text',
        '.article-title::text',
        '.title::text',
        'title::text'
    ]
    
    for selector in title_selectors:
        title = response.css(selector).get()
        if title and title.strip():
            return title.strip()
    
    # 从URL提取标题作为后备
    from urllib.parse import urlparse
    path = urlparse(response.url).path
    return path.split('/')[-1] or 'Unknown'

def extract_content(self, response) -> str:
    """提取页面内容"""
    # 获取完整HTML内容用于快照
    html_content = response.body
    
    # 内容区域选择器
    content_selectors = [
        '.article-content',
        '.content',
        '.main-content',
        'main',
        '#content',
        '.post-content'
    ]
    
    for selector in content_selectors:
        content_elem = response.css(selector)
        if content_elem:
            # 提取纯文本
            text_parts = content_elem.css('::text').getall()
            content = ' '.join(part.strip() for part in text_parts if part.strip())
            
            if len(content) > 50:  # 最小内容长度检查
                return self.clean_text(content)
    
    # 备用方案：提取body中所有文本
    body_text = response.css('body::text').getall()
    return self.clean_text(' '.join(body_text))

def extract_publish_time(self, response) -> str:
    """提取发布时间"""
    time_selectors = [
        '.publish-time::text',
        '.date::text',
        '.time::text',
        '[class*="time"]::text',
        '[class*="date"]::text'
    ]
    
    for selector in time_selectors:
        time_text = response.css(selector).get()
        if time_text:
            # 解析时间格式
            parsed_time = self.parse_time_string(time_text.strip())
            if parsed_time:
                return parsed_time
    
    # 从URL中提取时间
    return self.extract_time_from_url(response.url)

def determine_source(self, url: str) -> str:
    """确定内容来源"""
    from urllib.parse import urlparse
    from ..parse_different_college import url_maps
    
    domain = urlparse(url).netloc
    
    # 从URL映射中查找对应的学院
    for college, college_url in url_maps.items():
        if domain in college_url:
            return college
    
    return domain  # 默认返回域名
```

## 文本处理规范

### 1. 文本清理工具
```python
import re
import jieba
from typing import List

def clean_text(text: str) -> str:
    """通用文本清理函数"""
    if not text:
        return ""
    
    # 移除HTML标签
    text = re.sub(r'<[^>]+>', '', text)
    
    # 移除URL链接
    text = re.sub(r'http[s]?://\S+', '', text)
    
    # 移除邮箱地址
    text = re.sub(r'\S+@\S+', '', text)
    
    # 标准化空白字符
    text = re.sub(r'\s+', ' ', text)
    
    # 移除控制字符
    text = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f-\x9f]', '', text)
    
    # 移除过长的重复字符
    text = re.sub(r'(.)\1{10,}', r'\1\1\1', text)
    
    # 保留中文、英文、数字和基本标点
    text = re.sub(r'[^\u4e00-\u9fa5\u3040-\u309F\u30A0-\u30FFa-zA-Z0-9\s.,;:!?()（）。，；：！？【】《》""'']', '', text)
    
    return text.strip()

def parse_time_string(time_str: str) -> str:
    """解析时间字符串"""
    from datetime import datetime
    
    # 常见时间格式
    time_formats = [
        '%Y-%m-%d %H:%M:%S',
        '%Y/%m/%d %H:%M:%S',
        '%Y年%m月%d日 %H:%M',
        '%Y年%m月%d日',
        '%Y-%m-%d',
        '%Y/%m/%d'
    ]
    
    for fmt in time_formats:
        try:
            dt = datetime.strptime(time_str, fmt)
            return dt.strftime('%Y-%m-%d')
        except ValueError:
            continue
    
    # 处理相对时间描述
    relative_patterns = {
        r'(\d+)小时前': lambda m: (datetime.now() - timedelta(hours=int(m.group(1)))).strftime('%Y-%m-%d'),
        r'(\d+)天前': lambda m: (datetime.now() - timedelta(days=int(m.group(1)))).strftime('%Y-%m-%d'),
        r'昨天': lambda m: (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d'),
        r'今天': lambda m: datetime.now().strftime('%Y-%m-%d')
    }
    
    for pattern, handler in relative_patterns.items():
        match = re.search(pattern, time_str)
        if match:
            return handler(match)
    
    return ""

def extract_time_from_url(url: str) -> str:
    """从URL中提取时间信息"""
    import re
    from datetime import datetime
    
    # URL中的日期模式
    date_patterns = [
        r'/(\d{4})/(\d{2})/(\d{2})/',
        r'/(\d{4})-(\d{2})-(\d{2})/',
        r'/(\d{4})(\d{2})(\d{2})/',
        r'_(\d{4})(\d{2})(\d{2})_',
        r'-(\d{4})(\d{2})(\d{2})-'
    ]
    
    for pattern in date_patterns:
        match = re.search(pattern, url)
        if match:
            year, month, day = match.groups()
            try:
                date_obj = datetime(int(year), int(month), int(day))
                return date_obj.strftime('%Y-%m-%d')
            except ValueError:
                continue
    
    return ""

class TextProcessor:
    """文本处理工具类"""
    
    def __init__(self):
        # 加载停用词
        self.stopwords = self.load_stopwords()
        
        # 初始化jieba
        self.init_jieba()
    
    def init_jieba(self):
        """初始化jieba分词器"""
        try:
            # 加载自定义词典
            user_dict_path = '/data/models/user_dict.txt'
            if Path(user_dict_path).exists():
                jieba.load_userdict(user_dict_path)
        except Exception as e:
            self.logger.warning(f"加载用户词典失败: {e}")
    
    def segment_text(self, text: str) -> List[str]:
        """中文分词"""
        if not text:
            return []
        
        # 使用jieba分词
        words = jieba.lcut(text)
        
        # 过滤停用词、单字符词和纯数字词
        filtered_words = [
            word.strip() for word in words 
            if (len(word.strip()) > 1 
                and word.strip() not in self.stopwords 
                and not word.strip().isdigit()
                and not re.match(r'^[^\u4e00-\u9fa5a-zA-Z]+$', word.strip()))
        ]
        
        return filtered_words
    
    def load_stopwords(self) -> set:
        """加载停用词表"""
        default_stopwords = {
            '的', '是', '在', '了', '和', '有', '我', '你', '他', '她', '它',
            '这', '那', '与', '及', '以', '为', '等', '中', '上', '下', '来', '去'
        }
        
        try:
            stopwords_path = '/data/nltk/hit_stopwords.txt'
            if Path(stopwords_path).exists():
                with open(stopwords_path, 'r', encoding='utf-8') as f:
                    file_stopwords = set(line.strip() for line in f if line.strip())
                return default_stopwords.union(file_stopwords)
        except Exception as e:
            print(f"加载停用词文件失败: {e}")
        
        return default_stopwords
```

## 数据存储规范

### 1. 文件存储结构
```
data/raw/{platform}/
├── {tag}/
│   ├── articles/           # 文章数据
│   │   ├── article_001.json
│   │   └── article_002.json
│   ├── snapshots/          # 网页快照
│   │   ├── {url_hash}.html
│   │   └── {url_hash}.html
│   ├── scraped_original_urls.json  # 已爬取URL记录
│   ├── counter.txt         # 统计信息
│   └── update.txt          # 更新日志
```

### 2. 数据库集成
```python
def export_to_mysql(data_source: str = "scrapy") -> bool:
    """导出数据到MySQL数据库"""
    try:
        if data_source == "scrapy":
            # 从Scrapy sqlite数据库导入
            from etl.crawler.webpage_spider.from_sqlite_to_mysql import export_web_to_mysql
            export_web_to_mysql(logger)
        else:
            # 从JSON文件导入
            from etl.load.import_data import main as import_main
            import_main()
        
        return True
    except Exception as e:
        logger.error(f"导出到MySQL失败: {e}")
        return False

def save_article(article_data: Dict) -> bool:
    """保存文章数据到JSON文件"""
    try:
        # 生成文件路径
        url_hash = hashlib.md5(article_data['original_url'].encode()).hexdigest()
        file_path = self.data_dir / "articles" / f"{url_hash}.json"
        file_path.parent.mkdir(parents=True, exist_ok=True)
        
        # 添加元数据
        article_data.update({
            'scrape_time': datetime.now().isoformat(),
            'platform': self.platform,
            'content_type': getattr(self, 'content_type', 'article')
        })
        
        # 保存文件
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(article_data, f, ensure_ascii=False, indent=2)
        
        self.logger.debug(f"文章保存成功: {file_path}")
        return True
        
    except Exception as e:
        self.logger.error(f"保存文章失败: {e}")
        return False
```

## 运行和部署规范

### 1. 爬虫启动方式
```python
# 方式1：直接运行爬虫模块
if __name__ == "__main__":
    # BaseCrawler爬虫
    wechat = Wechat(
        authors=["club_official_accounts"], 
        debug=False, 
        headless=True, 
        use_proxy=False
    )
    
    # 先爬取元数据
    asyncio.run(wechat.scrape(max_article_num=5, total_max_article_num=100))
    
    # 再下载详细内容
    asyncio.run(wechat.download())

# 方式2：模块式运行
# python -m etl.crawler.wechat

# 方式3：Scrapy爬虫运行
# cd etl/crawler/webpage_spider
# scrapy crawl wiki_spider
```

### 2. 配置管理
```python
# 爬虫配置通过config.json管理
{
    "etl": {
        "crawler": {
            "proxy_pool": "http://127.0.0.1:7897",
            "market_token": "",
            "max_retry_count": 3,
            "default_delay": 1.0
        }
    }
}

# 使用配置
from etl.crawler import config, proxy_pool, market_token

proxy = config.get("etl.crawler.proxy_pool", "")
max_retries = config.get("etl.crawler.max_retry_count", 3)
```

### 3. 错误处理和重试机制
```python
async def robust_request(self, url: str, max_retries: int = 3) -> Dict:
    """带重试机制的请求"""
    for attempt in range(max_retries):
        try:
            await self.page.goto(url, wait_until='domcontentloaded', timeout=30000)
            await self.random_sleep()
            
            # 检查页面是否正常加载
            if await self.page.query_selector('body'):
                return {'success': True, 'url': url}
            else:
                raise Exception("页面加载异常")
                
        except Exception as e:
            self.logger.warning(f"请求失败 (尝试 {attempt + 1}/{max_retries}): {url}, 错误: {e}")
            
            if attempt < max_retries - 1:
                # 轮换代理
                if self.use_proxy:
                    await self.rotate_proxy()
                
                # 增加延时
                await asyncio.sleep(2 ** attempt)
            else:
                self.logger.error(f"请求最终失败: {url}")
                return {'success': False, 'url': url, 'error': str(e)}

def safe_file_operation(self, operation: Callable, *args, **kwargs) -> bool:
    """安全的文件操作"""
    try:
        result = operation(*args, **kwargs)
        return True
    except PermissionError:
        self.logger.error("文件权限错误")
        return False
    except OSError as e:
        self.logger.error(f"文件系统错误: {e}")
        return False
    except Exception as e:
        self.logger.error(f"文件操作失败: {e}")
        return False
```

### 4. 性能监控和统计
```python
class CrawlerMonitor:
    """爬虫监控器"""
    
    def __init__(self):
        self.stats = {
            'start_time': None,
            'requests_count': 0,
            'success_count': 0,
            'error_count': 0,
            'total_bytes': 0,
            'errors': []
        }
    
    def start_monitoring(self):
        """开始监控"""
        self.stats['start_time'] = time.time()
    
    def record_request(self, success: bool = True, size: int = 0, error: str = None):
        """记录请求"""
        self.stats['requests_count'] += 1
        
        if success:
            self.stats['success_count'] += 1
            self.stats['total_bytes'] += size
        else:
            self.stats['error_count'] += 1
            if error:
                self.stats['errors'].append(error)
    
    def get_report(self) -> Dict:
        """获取监控报告"""
        duration = time.time() - self.stats['start_time']
        
        return {
            'duration': duration,
            'requests_per_second': self.stats['requests_count'] / duration,
            'success_rate': self.stats['success_count'] / self.stats['requests_count'] * 100,
            'total_mb': self.stats['total_bytes'] / 1024 / 1024,
            'error_rate': self.stats['error_count'] / self.stats['requests_count'] * 100
        }
```

### 5. 数据流集成
```python
def run_complete_etl_pipeline():
    """运行完整的ETL流程"""
    try:
        # 1. 运行Scrapy爬虫
        logger.info("开始网页爬虫...")
        scrapy_result = run_scrapy_spider()
        
        # 2. 运行其他平台爬虫
        logger.info("开始其他平台爬虫...")
        platform_results = run_platform_crawlers()
        
        # 3. 数据导入MySQL
        logger.info("开始数据导入...")
        from etl.crawler.webpage_spider.from_sqlite_to_mysql import export_web_to_mysql
        export_web_to_mysql(logger)
        
        # 4. 计算PageRank
        logger.info("开始计算PageRank...")
        from etl.pagerank.calculate_pagerank import main as pagerank_main
        pagerank_main()
        
        # 5. 构建索引
        logger.info("开始构建索引...")
        from etl.embedding.run_embedding_pipeline import main as embedding_main
        embedding_main()
        
        logger.info("ETL流程完成")
        return True
        
    except Exception as e:
        logger.error(f"ETL流程失败: {e}")
        return False

def run_scrapy_spider():
    """运行Scrapy爬虫"""
    import subprocess
    
    scrapy_dir = Path(__file__).parent / "webpage_spider"
    cmd = ["scrapy", "crawl", "wiki_spider"]
    
    result = subprocess.run(
        cmd, 
        cwd=scrapy_dir,
        capture_output=True,
        text=True
    )
    
    if result.returncode == 0:
        logger.info("Scrapy爬虫运行成功")
        return True
    else:
        logger.error(f"Scrapy爬虫运行失败: {result.stderr}")
        return False
```

## 数据库操作规范

### 1. 数据库连接管理
```python
from etl.load import db_core

async def batch_insert_documents(documents: List[Document]) -> bool:
    """批量插入文档"""
    try:
        connection = db_core.get_connection()
        
        # 准备批量插入数据
        insert_data = []
        for doc in documents:
            insert_data.append({
                'id': doc.id,
                'title': doc.title,
                'content': doc.content,
                'source': doc.source,
                'url': doc.url,
                'metadata': json.dumps(doc.metadata, ensure_ascii=False),
                'create_time': doc.create_time
            })
        
        # 批量插入
        success_count = await db_core.batch_insert(
            connection=connection,
            table_name='website_nku',
            data_list=insert_data,
            batch_size=1000
        )
        
        logger.info("批量插入完成，成功数量: %d", success_count)
        return success_count == len(documents)
        
    except Exception as e:
        logger.error("批量插入失败: %s", str(e))
        return False
    finally:
        if connection:
            connection.close()
```

### 2. PageRank分数计算
```python
import networkx as nx
from etl.load import db_core

class PageRankCalculator:
    """PageRank分数计算器"""
    
    def __init__(self, config: Config):
        self.config = config
        self.logger = register_logger(__name__)
    
    async def calculate_pagerank(self) -> bool:
        """计算PageRank分数"""
        try:
            # 构建链接图
            graph = await self.build_link_graph()
            
            # 计算PageRank
            pagerank_scores = nx.pagerank(
                graph,
                alpha=0.85,
                max_iter=100,
                tol=1e-6
            )
            
            # 保存分数
            await self.save_pagerank_scores(pagerank_scores)
            
            self.logger.info("PageRank计算完成，节点数量: %d", len(pagerank_scores))
            return True
            
        except Exception as e:
            self.logger.error("PageRank计算失败: %s", str(e))
            return False
    
    async def build_link_graph(self) -> nx.DiGraph:
        """构建链接图"""
        connection = db_core.get_connection()
        
        # 查询链接数据
        query = "SELECT source_url, target_url FROM link_graph"
        links = db_core.query_all(connection, query)
        
        # 构建有向图
        graph = nx.DiGraph()
        
        for link in links:
            graph.add_edge(link['source_url'], link['target_url'])
        
        connection.close()
        return graph
    
    async def save_pagerank_scores(self, scores: Dict[str, float]):
        """保存PageRank分数"""
        connection = db_core.get_connection()
        
        # 批量更新分数
        update_data = [
            {"url": url, "pagerank_score": score}
            for url, score in scores.items()
        ]
        
        await db_core.batch_update(
            connection=connection,
            table_name='pagerank_scores',
            data_list=update_data,
            key_field='url'
        )
        
        connection.close()
```

## 任务调度规范

### 1. ETL任务编排
```python
import asyncio
from typing import Callable

class ETLPipeline:
    """ETL管道编排器"""
    
    def __init__(self, config: Config):
        self.config = config
        self.logger = register_logger(__name__)
        self.tasks = []
    
    def add_task(self, name: str, func: Callable, **kwargs):
        """添加任务"""
        self.tasks.append({
            'name': name,
            'func': func,
            'kwargs': kwargs
        })
    
    async def run_pipeline(self) -> bool:
        """运行管道"""
        self.logger.info("开始运行ETL管道，任务数量: %d", len(self.tasks))
        
        for i, task in enumerate(self.tasks):
            try:
                self.logger.info("开始任务 %d/%d: %s", i+1, len(self.tasks), task['name'])
                
                start_time = time.time()
                success = await task['func'](mdc:**task['kwargs'])
                duration = time.time() - start_time
                
                if success:
                    self.logger.info("任务完成: %s (%.2fs)", task['name'], duration)
                else:
                    self.logger.error("任务失败: %s", task['name'])
                    return False
                    
            except Exception as e:
                self.logger.error("任务异常: %s, 错误: %s", task['name'], str(e))
                return False
        
        self.logger.info("ETL管道运行完成")
        return True

# 使用示例
async def run_full_etl():
    """运行完整ETL流程"""
    pipeline = ETLPipeline(config)
    
    # 添加任务
    pipeline.add_task("数据爬取", run_crawlers)
    pipeline.add_task("数据处理", process_documents)
    pipeline.add_task("构建向量索引", build_vector_index)
    pipeline.add_task("构建BM25索引", build_bm25_index)
    pipeline.add_task("计算PageRank", calculate_pagerank)
    
    # 运行管道
    success = await pipeline.run_pipeline()
    return success
```

## 监控和日志规范

### 1. 进度监控
```python
from tqdm import tqdm
import json

class ETLMonitor:
    """ETL监控器"""
    
    def __init__(self):
        self.stats = {
            'start_time': None,
            'end_time': None,
            'total_documents': 0,
            'processed_documents': 0,
            'failed_documents': 0,
            'errors': []
        }
        self.logger = register_logger(__name__)
    
    def start_monitoring(self, total_docs: int):
        """开始监控"""
        self.stats['start_time'] = time.time()
        self.stats['total_documents'] = total_docs
        self.progress_bar = tqdm(total=total_docs, desc="处理文档")
    
    def update_progress(self, success: bool = True, error: str = None):
        """更新进度"""
        if success:
            self.stats['processed_documents'] += 1
        else:
            self.stats['failed_documents'] += 1
            if error:
                self.stats['errors'].append(error)
        
        self.progress_bar.update(1)
    
    def finish_monitoring(self):
        """结束监控"""
        self.stats['end_time'] = time.time()
        self.progress_bar.close()
        
        # 输出统计信息
        duration = self.stats['end_time'] - self.stats['start_time']
        success_rate = self.stats['processed_documents'] / self.stats['total_documents'] * 100
        
        self.logger.info(
            "ETL任务完成 - 总数: %d, 成功: %d, 失败: %d, 成功率: %.2f%%, 耗时: %.2fs",
            self.stats['total_documents'],
            self.stats['processed_documents'],
            self.stats['failed_documents'],
            success_rate,
            duration
        )
        
        # 保存统计报告
        self.save_report()
    
    def save_report(self):
        """保存统计报告"""
        report_path = f"/data/reports/etl_report_{int(self.stats['start_time'])}.json"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(self.stats, f, ensure_ascii=False, indent=2)
        
        self.logger.info("ETL报告保存: %s", report_path)

## 最佳实践总结

### 1. 爬虫开发指南

#### 选择合适的爬虫架构
- **简单静态网页**: 使用Scrapy框架，效率高、稳定性好
- **复杂动态网页**: 使用BaseCrawler+Playwright，支持JavaScript渲染
- **需要登录的平台**: 使用BaseCrawler，更好的会话管理

#### 数据质量保证
- **内容验证**: 检查标题、内容长度、时间格式
- **去重策略**: URL级别去重，避免重复抓取
- **异常处理**: 网络异常、解析异常、文件操作异常

#### 反反爬策略
- **请求头轮换**: 随机User-Agent、Accept等
- **IP代理池**: 支持代理轮换和健康检查
- **访问频率控制**: 智能延时、模拟人类行为
- **浏览器指纹**: 注入反检测脚本

### 2. 常见问题解决

#### Scrapy爬虫问题
```python
# 问题1: 内存占用过高
# 解决: 限制并发数和延时
custom_settings = {
    'CONCURRENT_REQUESTS': 8,
    'DOWNLOAD_DELAY': 1.0,
    'RANDOMIZE_DOWNLOAD_DELAY': 0.5
}

# 问题2: 重复爬取
# 解决: 使用去重过滤器
def check_if_in_sqlite(self, url):
    return url in self.url_set

# 问题3: 链接图数据过大
# 解决: 使用INSERT OR IGNORE批量插入
cursor.execute('''
    INSERT OR IGNORE INTO link_graph (source_url, target_url)
    VALUES (?, ?)
''', (source_url, target_url))
```

#### BaseCrawler问题
```python
# 问题1: 浏览器进程泄漏
# 解决: 确保正确关闭资源
try:
    await self.crawl_data()
finally:
    await self.close()  # 必须调用

# 问题2: 验证码或反爬检测
# 解决: 降低速度、增加随机性
async def handle_verification(self):
    if await self.page.query_selector('.captcha'):
        self.logger.warning("检测到验证码，等待人工处理")
        await asyncio.sleep(60)

# 问题3: 内存泄漏
# 解决: 定期清理和重启浏览器上下文
if self.processed_count % 100 == 0:
    await self.restart_browser_context()
```

### 3. 性能优化建议

#### 爬虫性能
- **并发控制**: Scrapy设置合理并发数，BaseCrawler使用信号量
- **资源复用**: 连接池、浏览器上下文复用
- **内存管理**: 及时释放不用的对象，定期重启浏览器
- **网络优化**: 压缩传输、keep-alive连接

#### 数据处理性能  
- **批量操作**: 数据库批量插入，文件批量处理
- **异步处理**: IO密集型操作使用异步编程
- **缓存策略**: 已处理URL缓存，避免重复处理
- **流式处理**: 大文件流式读取，避免内存溢出

### 4. 部署和运维

#### 环境配置
```bash
# 1. 安装依赖
pip install -r requirements.txt

# 2. 配置数据库
# 修改config.json中的数据库连接信息

# 3. 初始化目录
mkdir -p /data/raw/{platform}/{tag}
mkdir -p /data/snapshots
mkdir -p /data/logs

# 4. 启动服务
python -m etl.crawler.wechat
```

#### 监控告警
```python
# 监控指标
- 爬取速度 (requests/second)
- 成功率 (success_rate)
- 错误率 (error_rate)  
- 内存使用 (memory_usage)
- 磁盘空间 (disk_usage)

# 告警条件
- 成功率 < 90%
- 错误率 > 10%
- 内存使用 > 80%
- 磁盘空间 > 90%
```

#### 数据备份
- **定期备份**: 每日备份爬取数据和数据库
- **增量备份**: 只备份新增和变更的数据
- **异地备份**: 重要数据异地存储
- **恢复测试**: 定期测试备份恢复流程

### 5. 开发工具推荐

#### 调试工具
- **Scrapy Shell**: `scrapy shell "http://example.com"`
- **Playwright Inspector**: `DEBUG=1 python crawler.py`
- **网络代理工具**: Charles、Burp Suite
- **浏览器开发者工具**: 元素选择器、网络监控

#### 测试工具
- **单元测试**: pytest框架
- **集成测试**: 端到端测试
- **性能测试**: locust压力测试
- **数据验证**: Great Expectations

#### 数据分析
- **日志分析**: ELK Stack (Elasticsearch + Logstash + Kibana)
- **监控面板**: Grafana + Prometheus
- **数据质量**: Pandas数据探索
- **可视化**: Matplotlib、Plotly

---

## 注意事项

1. **合规性**: 遵守robots.txt和网站服务条款
2. **资源控制**: 控制爬取频率，避免对目标服务器造成压力
3. **数据隐私**: 不爬取个人隐私信息，遵守数据保护法规
4. **稳定性**: 做好异常处理，保证爬虫稳定运行
5. **可维护性**: 代码模块化，便于扩展和维护

---

**更新时间**: 2024年12月
**维护者**: ETL开发团队  
**版本**: v2.1.0
