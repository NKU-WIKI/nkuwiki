# RAG (检索增强生成) 实施指南

本文档旨在简明扼要地介绍 `nkuwiki` 项目中RAG（Retrieval-Augmented Generation）的实际工作流程，帮助开发者快速理解从数据处理到答案生成的全过程。

关于RAG的深入技术选型、理论和历史方案，请参阅技术论文归档：**[RAG策略技术报告](./rag_strategy_paper.md)**。

## RAG 核心流程概览

nkuwiki的RAG流程分为两个主要阶段：

1.  **离线数据处理与索引 (Offline Data Processing & Indexing)**:
    - **核心脚本**: `etl/daily_pipeline.py`
    - **任务**: 定期执行，负责将原始数据（通过爬虫采集）处理成可供检索的知识库。

2.  **在线检索与生成 (Online Retrieval & Generation)**:
    - **核心API**: `/api/agent/rag`
    - **任务**: 当用户发起查询时实时触发，负责从知识库中检索相关信息，并结合大语言模型生成最终答案。

---

## 1. 离线数据处理与索引

这是RAG的数据准备阶段，所有操作均由 `etl/daily_pipeline.py` 驱动。详细的执行步骤和自定义方法，请参阅 **[ETL流程开发规范](./etl_pipeline_guide.md)**。

其核心步骤包括：

1.  **扫描新数据**: 监控 `/data/raw/` 目录，发现新增的JSON数据文件。
2.  **文本分块**: 将JSON文件中的长文本内容，使用 `ChunkCacheManager` 切割成大小合适、有重叠的文本块（Nodes）。
3.  **建立索引**:
    - **向量索引**: 使用`BAAI/bge-large-zh-v1.5`等嵌入模型将文本块向量化，并存入 **Qdrant** 向量数据库。
    - **全文索引 (可选)**: 同时可能会使用 **BM25** 或 **Elasticsearch** 建立关键词索引。

此阶段完成后，所有知识都已准备就绪，等待被实时查询。

---

## 2. 在线检索与生成

当用户通过应用端发起一个问答请求时，后端的RAG流程被激活。

### a. 查询预处理 (Query Pre-processing)

-   **查询改写/扩展**: 用户的原始问题（如："查询课表？"）可能会被送入一个轻量级的大模型进行优化，扩展成包含更多关键词、意图更明确的查询语句，以提升后续的检索召回率。

### b. 多路检索与重排 (Multi-path Retrieval & Reranking)

系统会并行的从多个知识源中检索信息：

1.  **向量检索 (Vector Search)**:
    - 将优化后的查询语句进行向量化。
    - 在 **Qdrant** 中进行相似度搜索，召回最相关的 Top-K 个文本块。

2.  **关键词检索 (Keyword Search)**:
    - 使用 **BM25** 或 **Elasticsearch**，根据查询中的关键词进行检索，召回另外 Top-N 个文本块。

3.  **融合与重排 (Fusion & Reranking)**:
    - 将多路检索召回的文本块通过 **倒数排名融合 (RRF)** 等算法合并去重。
    - 使用 **Reranker模型** (如 `BAAI/bge-reranker-base`) 对合并后的文本块列表进行重新排序，筛选出与用户问题最精准匹配的少数几个（例如 Top 3-5）文本块。
    - **PageRank集成**: 在重排阶段，会结合文章的PageRank权威性分数，对来自官网等重要信源的文档进行提权。
    - **个性化排序**: 同时，系统会参考用户的搜索历史，对匹配历史关键词的文档给予一定的分数加成，实现个性化推荐。
    - 最终，综合 **重排分数、PageRank分数、个性化加成**，得到最终的上下文列表。

### c. 上下文构建与答案生成 (Context Building & Generation)

1.  **构建提示词 (Prompt Construction)**:
    - 将经过重排筛选出的最优文本块，按照预设的模板拼接成一段完整的上下文信息。
    - 将上下文信息和用户的原始问题组合成一个最终的提示词（Prompt）。

    ```
    上下文信息如下:
    [文档1: ...内容...]
    [文档2: ...内容...]

    请你基于以上上下文信息而不是自己的知识, 回答以下问题:
    {用户的原始问题}
    ```

2.  **调用大模型生成答案**:
    - 将构建好的最终提示词发送给核心的大语言模型（如 Coze, Deepseek, Zhipu等）。
    - LLM根据提供的上下文生成答案，并将结果返回给用户。

这个两阶段的设计将耗时的数据处理工作放在离线完成，保证了在线问答服务的高效和快速响应。 