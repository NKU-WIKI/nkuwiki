import{_ as i}from"./plugin-vue_export-helper-DlAUqK2U.js";import{e as r,h as e,f as a,o as s}from"./app-DWPKiBdg.js";const l={};function t(o,n){return s(),r("div",null,n[0]||(n[0]=[e(`<h1 id="rag方案" tabindex="-1"><a class="header-anchor" href="#rag方案"><span>rag方案</span></a></h1><h2 id="摘要" tabindex="-1"><a class="header-anchor" href="#摘要"><span>摘要</span></a></h2><p>本文提出了EasyRAG,一个面向AIOps网络运维的简洁､轻量､高效的检索增强问答框架｡我们主要的贡献为:(1) 问答准确,我们设计了一套简洁的基于两路稀疏检索粗排—LLM Reranker重排—LLM答案生成与优化的RAG方案以及配套的数据处理流程, 部署简单,我们的方法主要由bm25检索和bge-reranker重排组成,无需微调任何模型,占用显存少,部署容易,可扩展性强;我们提供了一套灵活的代码库,内置了多种搜索和生成策略,方便自定义流程实现｡(2) 推理高效,我们设计了一套粗排-重排-生成全流程的高效推理加速方案,能够大幅降低RAG的推理延迟且较好地维持了准确度水平;每个加速方案都可以即插即用到任意的RAG流程的相关组件中,一致地提升RAG系统的效率｡</p><h2 id="_1-easyrag框架介绍" tabindex="-1"><a class="header-anchor" href="#_1-easyrag框架介绍"><span>1 EasyRAG框架介绍</span></a></h2><p>我们的复赛方案可以用图1概括｡包含数据处理流程(1.1) 和RAG流程(1.2)｡</p><h3 id="_1-1-数据处理流程" tabindex="-1"><a class="header-anchor" href="#_1-1-数据处理流程"><span>1.1 数据处理流程</span></a></h3><h3 id="_1-1-1-文本分块" tabindex="-1"><a class="header-anchor" href="#_1-1-1-文本分块"><span>1.1.1 文本分块</span></a></h3><p>分块设置 我们使用了SentenceSplitter对文档进行分块,即先利用中文分隔符分割为句子,再按照设置的文本块大小合并,使用的块大小(chunk-size)为1024,块重叠大小(chunkoverlap)为200｡<br> 消除分块中的路径影响 在实践中,我们发现llama-idnex的原实现中含有对路径信息的简单但不稳定的使用方式,即将文本长度减去文件路径长度得到实际使用的文本长度,这样会让不同的数据路径产生不同的分块结果,在初赛中我们发现同样的chunk-size和chunkoverlap,更换路径最多可以为最终评测结果带来3个点的波动,这显然在实践中无法接受｡针对此问题,我们实现了自定义的分块类,将路径长度的利用给消除,从而保证可稳定复现｡</p><h3 id="_1-2-rag流程" tabindex="-1"><a class="header-anchor" href="#_1-2-rag流程"><span>1.2 RAG流程</span></a></h3><h3 id="_1-2-1-查询改写" tabindex="-1"><a class="header-anchor" href="#_1-2-1-查询改写"><span>1.2.1 查询改写</span></a></h3><p>用户所给的查询都非常简短，并且我们发现部分查询语句存在语义不通顺或者关键词不清晰的问题，例如：“查询课表？”。我们在将问题query输入到RAG Pipeline前，通过LLM（doubao）进行查询改写（扩展）</p><ul><li><strong>查询扩展</strong>：我们对用户日志的查询问题进行了总结，其中查询语句存在以下特点：<br> • 查询中的关键词非常重要；<br> • 查询语句长度短、信息量方差大。<br> 在这种情况下，我们尝试了根据初始查询语句和设计的提示词，利用LLM模型总结问题中的关键词或者是可能涉及到的一些关键词，即利用LLM模型的知识，进行运维、通信领域的关键词联想和查询关键词的总结，我们称之为关键词扩展。<br> 人工标注若干条数据中的关键词和可能联想的关键词后，随即利用LLM即进行few - shots的关键词总结、扩展。我们参考（Wang et al., 2023），将扩展得到的关键词基于原始查询通过直接拼接、再次利用大语言模型总结两种方式生成新的查询。<br> 其中，c表示的大语言模型LLM，q、p分别代表的是初始查询和提示词，(p_{\\exp})表示扩展查询的提示词，其中包括人工标注的几条数据，(p_{sum})表示的是利用大模型进行语句和扩展关键词的总结拼接的提示词。</li></ul><h4 id="_1-2-2-两路稀疏检索粗排" tabindex="-1"><a class="header-anchor" href="#_1-2-2-两路稀疏检索粗排"><span>1.2.2 两路稀疏检索粗排</span></a></h4><p>在稀疏检索部分，我们采用BM25算法构建检索器，BM25的核心思想是基于词频（TF）和逆文档频率（IDF）来，同时还引入了文档的长度信息来计算文档和查询q之间的相关性。具体实现上，BM25检索器主要由中文分词器、停用词表构成，我们逐一进行介绍。</p><ul><li><strong>中文分词器</strong>：对于中文分词器，我们采用的是常见的jieba中文分词器，jieba分词器的优点在于轻量级，可以开启多线程模式加速分词和词性分析，并且可以根据自己的需要自定义词频或是自定义词典调整分词偏好。对于分词器我们也尝试做了词表的自定义，当前场景是南开大学校园问答，由于缺乏数据词表，因此最终我们仍然使用了原始的jieba词表。</li><li><strong>停用词表</strong>：对于中文的停用词表，我们采用了哈尔滨工业大学搜集的中文常见停用词表作为中文分词过程中过滤无意义词汇的参考词表，通过过滤无意义的词汇和特殊符号以提高有效关键词的命中率，提高正确文档的召回率。</li><li><strong>两路检索</strong>：BM25两路检索粗排为文本块检索和路径检索。 <ol><li><strong>文本块检索</strong>：使用BM25对分割好的文本块进行搜索，粗排召回得分大于0的前192个文本块。</li></ol></li><li><strong>检索流程</strong>：BM25检索器对于一个给定的查询q，具体的文档检索流程如下： <ol><li>文档预处理：先对所有文档（文本块或路径）进行停用词过滤，再利用中文分词器进行中文分词，并预先计算文档的的IDF分数。</li><li>查询处理：对查询q进行停用词过滤和中文分词。</li><li>相似度召回：统计查询q的关键词和计算各个文档的TF值，根据TF、IDF值计算查询q和各个文档中的相关分数，根据分数召回相关文档。</li></ol></li></ul><h4 id="_1-2-3-密集检索粗排" tabindex="-1"><a class="header-anchor" href="#_1-2-3-密集检索粗排"><span>1.2.3 密集检索粗排</span></a></h4><p>密集检索部分我们采用了BAAI/bge-large-zh-v1.5（引用待补充），此模型在（待补充）上达到了先进效果。</p><ul><li><strong>检索流程</strong>：密集检索器对于一个给定的查询q，具体的文档检索流程如下： <ol><li>文档编码：将所有文本块输入模型进行编码得到表征，存入qdrant向量数据库。</li><li>查询编码：利用查询提示模板将q转换为bge的查询输入，利用模型进行编码。</li><li>相似度召回：在检索时，使用余弦相似度进行匹配，召回前288个文本块。</li></ol></li></ul><h4 id="_1-2-4-llm-reranker重排" tabindex="-1"><a class="header-anchor" href="#_1-2-4-llm-reranker重排"><span>1.2.4 LLM Reranker重排</span></a></h4><p>我们采用了BAAI/bge-reranker-base（待补充），一个基于（待补充）在混合的多个多语言排序数据集上训练的LLM Reranker，此模型在中英文上具有先进的排序效果，且含有配套的工具代码，可以很方便地根据具体场景进行进一步微调。</p><ul><li><strong>重排流程</strong>：LLM - Reranker对于一个给定的查询q和(k&#39;)个粗排得到的文本块，具体的文档排序流程如下： <ol><li>文档扩展：将知识路径和每个文本块拼接起来作为扩展文档用于检索。</li><li>文本处理：将q和(k&#39;)文本块分别组合成(k&#39;)个查询 - 文档对，输入分词器得到LLM的输入数据。</li><li>相似度排序：将输入数据输入LLM得到查询和每个文本块的重排分数，并根据此分数进行排序，取最高的k（一般为6）个文本块返回。</li></ol></li></ul><h4 id="_1-2-5-多路排序融合" tabindex="-1"><a class="header-anchor" href="#_1-2-5-多路排序融合"><span>1.2.5 多路排序融合</span></a></h4><ul><li><strong>融合算法</strong>：由于我们设计了多路检索粗排，那么也需要设计相应的排序融合策略，我们主要使用了简单合并与倒数排序融合（Reciprocal Rank Fusion RRF）两种策略。简单合并策略直接将多路得到的文本块进行去重与合并。倒数排序融合则将同一个文档在多个路径的搜索排序的倒数进行求和作为融合的分数进行再次排序。</li><li><strong>粗排融合</strong>：最直接的排序融合的使用方式是，得到多路粗排结果后直接使用融合算法将多路粗排得到的文本块合并为一个文本块集合，再交给Reranker重排。复赛中我们使用了简单合并将两路稀疏检索粗排结果合并。</li><li><strong>重排融合</strong>：我们还可以在每一路完成粗排 - 重排后，再进行融合。初赛中我们融合了文本块稀疏检索和文本块密集检索两路，针对这两路检索，我们设计了三种重排融合方法。 <ol><li>使用RRF将多路的粗排 - 精排后的结果合并。</li><li>将多路重排后的文本块输入LLM分别得到相应的答案，取答案更长的作为最终答案。</li><li>将多路重排后的文本块输入LLM分别得到相应的答案，将多路的答案直接拼接。</li></ol></li></ul><h4 id="_1-2-6-llm回答" tabindex="-1"><a class="header-anchor" href="#_1-2-6-llm回答"><span>1.2.6 LLM回答</span></a></h4><p>此部分我们先将重排得到的top6文本块的内容用以下模板拼接得到上下文字符串：</p><div class="language-markdown line-numbers-mode" data-highlighter="shiki" data-ext="markdown" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#E45649;--shiki-dark:#E06C75;">### 文档0: {chunk_i} ### 文档5: {chunk_i}</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>之后我们将上下文字符串和问题用如下的问答模板组合成提示词，输入LLM（deepseek-V3）获得答案。</p><div class="language-markdown line-numbers-mode" data-highlighter="shiki" data-ext="markdown" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">上下文信息如下:  </span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">{context_str}  </span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">请你基于上下文信息而不是自己的知识,回答以下问题,可以分点作答,如果上下文信息没有相关知识,可以回答不确定,不要复述上下文信息:  </span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">{query_str}  </span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">回答:</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>除此之外，我们还设计了其他格式的问答模板，参考思维链（Wei et al., 2022）设计了思维链问答模板（附录A.2），参考COSTAR（Teo, 2023）设计了markdown格式问答模板（附录A.1），为了让LLM更重视top1文档设计了侧重问答模板（附录A.3）</p><h4 id="_1-2-7-llm答案优化" tabindex="-1"><a class="header-anchor" href="#_1-2-7-llm答案优化"><span>1.2.7 LLM答案优化</span></a></h4><p>由于我们发现LLM对于每个文本块都会给予一定的注意力，可能会导致top1文本块的有效信息没有得到充分利用，对于这种情况，我们设计了答案整合提示词（附录B），将根据6个文本块得到的答案利用top1文本块进行补充整合，得到最终的答案。</p><h3 id="_2-可扩展性" tabindex="-1"><a class="header-anchor" href="#_2-可扩展性"><span>2 可扩展性</span></a></h3><p>文档可扩展性 我们的方案主要基于bm25检索与Reranker重排,只需要对最新的文档进行处理,之后重新进行分块和idf值计算,整个过程时间开销较小,可以在5分钟内完成全部的处理流程｡<br> 用户可扩展性 我们的方案显存占用较小,且在各个环节都设计了相应的推理加速方法,可以根据用户的具体规模决定使用相应的优化策略｡即使是使用完全无加速的方案,一张80G的显卡也可以支撑至少6个RAG进程,在半分钟内返回答案给用户｡</p><h3 id="_3-结论" tabindex="-1"><a class="header-anchor" href="#_3-结论"><span>3 结论</span></a></h3><p>本文提出了EasyRAG,一个面向AIOps网络运维的准确､轻量､高效､灵活且可扩展的检索增强问答框架｡</p><h3 id="参考文献" tabindex="-1"><a class="header-anchor" href="#参考文献"><span>参考文献</span></a></h3><p>[1] Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. 2024. Bge m3-embedding: Multi-lingual, multifunctionality, multi-granularity text embeddings through self-knowledge distillation.<br> [2] Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. 2022. Precise zero-shot dense retrieval without relevance labels. arXiv preprint arXiv:2212.10496.<br> [3] Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang, Jing Zhang, Juanzi Li, Lei Zhao, Lindong Wu, Lucen Zhong, Mingdao Liu, Minlie Huang, Peng Zhang, Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shudan Zhang, Shulin Cao, Shuxun Yang, Weng Lam Tam, Wenyi Zhao, Xiao Liu, Xiao Xia, Xiaohan Zhang, Xiaotao Gu, Xin Lv, Xinghan Liu, Xinyi Liu, Xinyue Yang, Xixuan Song, Xunkai Zhang, Yifan An, Yifan Xu, Yilin Niu, Yuantao Yang, Yueyan Li, Yushi Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang, Zhen Yang, Zhengxiao Du, Zhenyu Hou, and Zihan Wang. 2024. Chatglm: A family of large language models from glm-130b to glm-4 all tools.<br> [4] Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2023a. LLMLingua: Compressing prompts for accelerated inference of large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 13358–13376, Singapore. Association for Computational Linguistics.<br> [5] Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2023b. Longllmlingua: Accelerating and enhancing llms in long context scenarios via prompt compression. arXiv preprint arXiv:2310.06839.<br> [6] Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang. 2023. Towards general text embeddings with multistage contrastive learning. arXiv preprint arXiv:2308.03281.<br> [7] Weijie Liu, Peng Zhou, Zhiruo Wang, Zhe Zhao, Haotang Deng, and Qi Ju. 2020. FastBERT: a self-distilling BERT with adaptive inference time. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6035–6044, Online. Association for Computational Linguistics.<br> [8] Xing Han Lù. 2024. Bm25s: Orders of magnitude faster lexical search via eager sparse scoring.<br> [9] Sheila Teo. 2023. How i won singapore’s gpt-4 prompt engineering competition.<br> [10] Liang Wang, Nan Yang, and Furu Wei. 2023. Query2doc: Query expansion with large language models. arXiv preprint arXiv:2303.07678.<br> [11] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824–24837.<br> [12] Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and Jimmy Lin. 2020. DeeBERT: Dynamic early exiting for accelerating BERT inference. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2246–2251, Online. Association for Computational Linguistics.</p><h3 id="附录a-问答提示词模板" tabindex="-1"><a class="header-anchor" href="#附录a-问答提示词模板"><span>附录A 问答提示词模板</span></a></h3><h4 id="a-1-markdown格式问答模板" tabindex="-1"><a class="header-anchor" href="#a-1-markdown格式问答模板"><span>A.1 Markdown格式问答模板</span></a></h4><h2 id="目标" tabindex="-1"><a class="header-anchor" href="#目标"><span>目标</span></a></h2><p>请你结合上下文中k个5G运维私域文档的信息,回答给定的问题</p><h2 id="要求" tabindex="-1"><a class="header-anchor" href="#要求"><span>要求</span></a></h2><ol><li>可以分点作答,尽量详细且具体</li><li>不要简单复述上下文信息</li><li>不要使用自己的知识,只能根据上下文文档中的内容作答</li></ol><h2 id="上下文" tabindex="-1"><a class="header-anchor" href="#上下文"><span>上下文</span></a></h2>`,44),a("p",{context_str:""},null,-1),a("h2",{id:"问题",tabindex:"-1"},[a("a",{class:"header-anchor",href:"#问题"},[a("span",null,"问题")])],-1),a("p",{query_str:""},null,-1),e('<h2 id="回答" tabindex="-1"><a class="header-anchor" href="#回答"><span>回答</span></a></h2><h4 id="a-2-思维链问答模板" tabindex="-1"><a class="header-anchor" href="#a-2-思维链问答模板"><span>A.2 思维链问答模板</span></a></h4><p>上下文信息如下:<br> {context_str}<br> 请你基于上下文信息而不是自己的知识,回答以下问题,请一步步思考,先给出分析过程,再生成答案:<br> {query_str}<br> 回答:</p><h4 id="a-3-侧重问答模板" tabindex="-1"><a class="header-anchor" href="#a-3-侧重问答模板"><span>A.3 侧重问答模板</span></a></h4><p>上下文信息如下:<br> {context_str}<br> 请你基于上下文信息而不是自己的知识,回答以下问题,可以分点作答,0号文档的内容比较重要,可以重点参考,如果上下文信息没有相关知识,可以回答不确定,不要复述上下文信息:<br> {query_str}<br> 回答:</p><h3 id="附录b-答案整合模板" tabindex="-1"><a class="header-anchor" href="#附录b-答案整合模板"><span>附录B 答案整合模板</span></a></h3><p>上下文:<br> {top1_content_str}<br> 你将看到一个问题,和这个问题对应的参考答案<br> 请基于上下文知识而不是自己的知识补充参考答案,让其更完整地回答问题<br> 请注意,严格保留参考答案的每个字符,并将补充的内容和参考答案合理地合并,输出更长更完整的包含更多术语和分点的新答案<br> 问题:<br> {query_str}<br> 参考答案:<br> {answer_str}<br> 新答案:</p>',7)]))}const p=i(l,[["render",t],["__file","rag.html.vue"]]),d=JSON.parse('{"path":"/docs/rag.html","title":"rag方案","lang":"zh-CN","frontmatter":{"description":"rag方案 摘要 本文提出了EasyRAG,一个面向AIOps网络运维的简洁､轻量､高效的检索增强问答框架｡我们主要的贡献为:(1) 问答准确,我们设计了一套简洁的基于两路稀疏检索粗排—LLM Reranker重排—LLM答案生成与优化的RAG方案以及配套的数据处理流程, 部署简单,我们的方法主要由bm25检索和bge-reranker重排组成,无需微...","head":[["meta",{"property":"og:url","content":"https://vuepress-theme-hope-docs-demo.netlify.app/docs/rag.html"}],["meta",{"property":"og:site_name","content":"南开WIKI"}],["meta",{"property":"og:title","content":"rag方案"}],["meta",{"property":"og:description","content":"rag方案 摘要 本文提出了EasyRAG,一个面向AIOps网络运维的简洁､轻量､高效的检索增强问答框架｡我们主要的贡献为:(1) 问答准确,我们设计了一套简洁的基于两路稀疏检索粗排—LLM Reranker重排—LLM答案生成与优化的RAG方案以及配套的数据处理流程, 部署简单,我们的方法主要由bm25检索和bge-reranker重排组成,无需微..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"rag方案\\",\\"image\\":[\\"\\"],\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"NKU-WIKI\\",\\"url\\":\\"https://github.com/NKU-WIKI/nkuwiki\\"}]}"]]},"git":{},"readingTime":{"minutes":12.42,"words":3727},"filePathRelative":"docs/rag.md","autoDesc":true}');export{p as comp,d as data};
